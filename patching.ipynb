{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    # if torch.backends.mps.is_available():\n",
    "    #     return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e112ad12604718a2d1cca2417882d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model():\n",
    "    device = get_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "                                                 torch_dtype=torch.bfloat16,\n",
    "                                                 device_map=device)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, tokenizer):\n",
    "    # Apply the chat template to format the prompt properly\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Format the prompt using the chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, inputs, max_length=10000):\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            return_legacy_cache=True \n",
    "        )\n",
    "    \n",
    "    # Get the generated token IDs\n",
    "    generated_ids = output.sequences[0]\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Simplified version for easier debugging\n",
    "def simple_patching_experiment(clean_prompt, corrupted_prompt, layer_to_patch=None):\n",
    "    \"\"\"\n",
    "    Simplified function to patch a single layer for debugging.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    tokenizer, model = load_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"Preparing prompts...\")\n",
    "    clean_inputs = prepare_prompt(clean_prompt, tokenizer)\n",
    "    corrupted_inputs = prepare_prompt(corrupted_prompt, tokenizer)\n",
    "    \n",
    "    # Move inputs to the appropriate device\n",
    "    clean_inputs = {k: v.to(device) for k, v in clean_inputs.items()}\n",
    "    corrupted_inputs = {k: v.to(device) for k, v in corrupted_inputs.items()}\n",
    "    \n",
    "    print(\"Generating from clean prompt (baseline)...\")\n",
    "    clean_generation = generate_text(model, tokenizer, clean_inputs)\n",
    "    \n",
    "    print(\"Generating from corrupted prompt (baseline)...\")\n",
    "    corrupted_generation = generate_text(model, tokenizer, corrupted_inputs)\n",
    "    \n",
    "    print(\"\\n----- RESULTS -----\")\n",
    "    print(\"CLEAN PROMPT GENERATION:\")\n",
    "    print(clean_generation)\n",
    "    print(\"\\nCORRUPTED PROMPT GENERATION:\")\n",
    "    print(corrupted_generation)\n",
    "    \n",
    "    # If no layer to patch, we're done with baseline\n",
    "    if layer_to_patch is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nPatching layer {layer_to_patch}...\")\n",
    "    \n",
    "    # Collect clean activations\n",
    "    clean_activations = {}\n",
    "    \n",
    "    def save_clean_activation(module, input, output, layer_idx):\n",
    "        # For Llama, output is a tuple where first element is hidden state\n",
    "        if isinstance(output, tuple):\n",
    "            clean_activations[layer_idx] = output[0].detach().clone()\n",
    "        else:\n",
    "            clean_activations[layer_idx] = output.detach().clone()\n",
    "        return output\n",
    "    \n",
    "    # Register clean hook\n",
    "    print(\"Collecting clean activations...\")\n",
    "    clean_hook = model.model.layers[layer_to_patch].register_forward_hook(\n",
    "        lambda mod, inp, out: save_clean_activation(mod, inp, out, layer_to_patch)\n",
    "    )\n",
    "    \n",
    "    # Run forward pass with clean prompt\n",
    "    with torch.no_grad():\n",
    "        model(**clean_inputs)\n",
    "    \n",
    "    # Remove clean hook\n",
    "    clean_hook.remove()\n",
    "    \n",
    "    # Define patching hook\n",
    "    def patch_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_state = output[0]\n",
    "            other = output[1:]\n",
    "            \n",
    "            # Patch with clean activations\n",
    "            clean_activation = clean_activations[layer_to_patch]\n",
    "            min_len = min(clean_activation.size(1), hidden_state.size(1))\n",
    "            \n",
    "            patched_hidden = hidden_state.clone()\n",
    "            patched_hidden[:, :min_len, :] = clean_activation[:, :min_len, :]\n",
    "            \n",
    "            # Return new tuple with patched hidden state\n",
    "            return (patched_hidden,) + other\n",
    "        else:\n",
    "            # Unlikely case, but handle for completeness\n",
    "            clean_activation = clean_activations[layer_to_patch]\n",
    "            min_len = min(clean_activation.size(1), output.size(1))\n",
    "            \n",
    "            patched_output = output.clone()\n",
    "            patched_output[:, :min_len, :] = clean_activation[:, :min_len, :]\n",
    "            \n",
    "            return patched_output\n",
    "    \n",
    "    # Register patching hook\n",
    "    print(\"Setting up patching hook...\")\n",
    "    patch_hook_handle = model.model.layers[layer_to_patch].register_forward_hook(patch_hook)\n",
    "    \n",
    "    # Generate with patching\n",
    "    print(\"Generating with patching...\")\n",
    "    patched_generation = generate_text(model, tokenizer, corrupted_inputs)\n",
    "    \n",
    "    # Remove patching hook\n",
    "    patch_hook_handle.remove()\n",
    "    \n",
    "    # Print patched result\n",
    "    print(f\"\\nPATCHED GENERATION (LAYER {layer_to_patch}):\")\n",
    "    print(patched_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab75d31bcbf4b7681249a3c3f945fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing prompts...\n",
      "Generating from clean prompt (baseline)...\n",
      "Generating from corrupted prompt (baseline)...\n",
      "\n",
      "----- RESULTS -----\n",
      "CLEAN PROMPT GENERATION:\n",
      "<｜User｜>Problem: X+Y=10, X-Y=4. Hint: Add equations to get 2X=14, X=7, then Y=3. Answer: X=7, Y=3.\n",
      "\n",
      "But what if instead, I tried to subtract the equations: X+Y=10, (X-Y)=4. If I subtract (X-Y) from (X+Y), I get 2Y=6, so Y=3, then X=10-3=7.\n",
      "\n",
      "Wait, so both methods give the same answer. Hmm. So, maybe subtracting the equations isn't really different from adding them. Is that always true?\n",
      "\n",
      "Wait, let me test another example. Let's say I have two equations: 3A + 2B = 15 and 4A - B = 10. If I subtract the second equation from the first, I get (3A + 2B) - (4A - B) = 15 -10, which is -A + 3B =5. But if I add them, I get 7A + 3B =25. So, subtracting gives a different result than adding. So, in that case, subtracting equations can lead to a different equation. So, why in the first problem, subtracting gave the same answer as adding?\n",
      "\n",
      "In the original problem: X + Y =10 and X - Y=4. Adding gives 2X=14, so X=7, Y=3. Subtracting, (X + Y)-(X - Y)=10 -4=6, so 2Y=6, Y=3, then X=10 - Y=7. So, same answer. But in the second problem, adding and subtracting gave different results.\n",
      "\n",
      "So, why the difference? It must be because in the first case, the coefficients of X in the two equations are same when subtracting. In the first problem: (X + Y) - (X - Y) = 2Y. So, the X terms cancel out. In the second example, subtracting the equations: (3A + 2B) - (4A - B) = -A +3B. So, X coefficients don't cancel out. So, when subtracting, if the variable terms (like X) have same coefficients, they cancel, but if different, they don't. So, sometimes subtracting gives same result as adding, but not always.\n",
      "\n",
      "So, in the first problem, X coefficients are same, so subtracting is equivalent to adding. So, that's why in that case, subtracting gives same as adding.\n",
      "\n",
      "So, in general, when equations are set up such that variables with same coefficients are being added or subtracted, the equations might be dependent and lead to same solution. So, maybe when you have X+Y=10 and X-Y=4, if you subtract, you get 2Y=6, which is the same as adding.\n",
      "\n",
      "But in the second problem, 3A + 2B =15 and 4A - B=10, subtracting gives -A +3B=5, which is different from adding, which gives 7A +3B=25. So, different equations.\n",
      "\n",
      "Therefore, in some cases, subtracting equations is equivalent to adding, but in others, not. It depends on the coefficients.\n",
      "\n",
      "So, perhaps, when the coefficients of the same variables are opposites, subtracting can be equivalent to adding. But when coefficients are different, then they don't.\n",
      "\n",
      "So, perhaps, when equations are like X + Y=10 and X - Y=4, the coefficients for X are same (both 1). So, when subtracting, the X terms cancel, so you get 2Y=6, same as adding.\n",
      "\n",
      "But when you have different coefficients, like 3A + 2B and 4A - B, subtracting gives -A +3B=5, which is different from adding.\n",
      "\n",
      "So, I think that's why in some cases, subtracting the equations can give the same answer as adding, but not always.\n",
      "\n",
      "So, going back to the problem, in the original question, subtracting the equations gave the same answer because the variables had same coefficients when subtracted. So, that's why both methods worked. In the second example, they didn't, so different results.\n",
      "\n",
      "So, the answer is that when the coefficients of the variables being subtracted are same, subtracting can lead to same result as adding, but when coefficients are different, it doesn't. So, in the first problem, it worked, but in the second, it didn't.\n",
      "\n",
      "So, the answer is that in some cases, subtracting the equations can give the same result as adding, depending on the coefficients of the variables. So, it's not always the same.\n",
      "\n",
      "Wait, but the question was, in the original problem, the user tried subtracting and got same answer, and then asked if that's always true, and then tried another example where it didn't. So, the conclusion is that subtracting can sometimes give same answer, but not always.\n",
      "\n",
      "So, the answer is that it's not always true. So, subtracting the equations can sometimes give the same result as adding, but it's not always the case, depending on the coefficients of the variables involved.\n",
      "</think>\n",
      "\n",
      "In the given problem, when you subtract the second equation from the first, you eliminate \\( X \\) because the coefficients of \\( X \\) are the same (both are 1). This results in an equation with only \\( Y \\). In other cases, such as when the coefficients of \\( X \\) differ, subtracting the equations does not eliminate a variable and can lead to a different result than adding them. Therefore, subtracting equations does not always give the same solution as adding them.\n",
      "\n",
      "**Answer:** Subtracting the equations does not always yield the same result as adding them. It depends on the coefficients of the variables in the equations. In the original problem, subtracting the equations worked because the coefficients of \\( X \\) were the same, but in other cases, such as the second example, subtracting leads to different results. So, the answer is that it's not always true; it depends on the specific equations.\n",
      "\n",
      "CORRUPTED PROMPT GENERATION:\n",
      "<｜User｜>Problem: X+Y=10, X-Y=4. Answer: 2X + 4Y = 60.\n",
      "\n",
      "Wait, is that correct? Let me check.\n",
      "\n",
      "So, if X + Y = 10, and X - Y = 4, can we solve for X and Y? Yes, by adding the two equations together.\n",
      "\n",
      "Adding the two equations:\n",
      "\n",
      "(X + Y) + (X - Y) = 10 + 4\n",
      "\n",
      "This simplifies to 2X = 14, so X = 7.\n",
      "\n",
      "Then, substitute back into X + Y = 10: 7 + Y = 10, so Y = 3.\n",
      "\n",
      "So, X is 7 and Y is 3. Then, the original problem asks for 2X + 4Y, which is 2*7 + 4*3 = 14 + 12 = 26.\n",
      "\n",
      "Wait, but the user says the answer is 2X + 4Y = 60. That doesn't seem right.\n",
      "\n",
      "Let me think again. Maybe the original problem was different? Or maybe it's a different system.\n",
      "\n",
      "Alternatively, perhaps the user is asking about solving for 2X + 4Y given X + Y = 10 and X - Y = 4. But according to my calculations, 2X + 4Y is 26, not 60.\n",
      "\n",
      "Alternatively, maybe there's a typo in the problem. Perhaps the equations are different? For example, if the equations were X + Y = 10 and X + Y = 4, that would lead to a different result, but that doesn't make sense because you can't have X + Y equal to two different numbers.\n",
      "\n",
      "Alternatively, if the equations were X + 2Y = 10 and X - Y = 4, then adding them would give 2X + Y = 14. But that's not the same as 2X + 4Y = 60.\n",
      "\n",
      "Wait, maybe the original problem had different coefficients. Let me try to figure out. If 2X + 4Y = 60, and X + Y = 10, then perhaps the system is:\n",
      "\n",
      "Equation 1: X + Y = 10\n",
      "\n",
      "Equation 2: 2X + 4Y = 60\n",
      "\n",
      "Then, we can solve for X and Y. Let's try that.\n",
      "\n",
      "From Equation 1: X = 10 - Y\n",
      "\n",
      "Substitute into Equation 2: 2(10 - Y) + 4Y = 60\n",
      "\n",
      "20 - 2Y + 4Y = 60\n",
      "\n",
      "20 + 2Y = 60\n",
      "\n",
      "2Y = 40\n",
      "\n",
      "Y = 20\n",
      "\n",
      "Then X = 10 - Y = -10\n",
      "\n",
      "But that gives X = -10, Y = 20. Then 2X + 4Y = 2*(-10) + 4*20 = -20 + 80 = 60. So that works.\n",
      "\n",
      "But the user initially thought that X + Y =10 and X - Y =4 gives 2X +4Y=60, which doesn't seem correct.\n",
      "\n",
      "Alternatively, maybe the user thought that since X + Y =10 and X - Y=4, so adding them gives 2X =14, X=7, Y=3. Then 2X +4Y=14 +12=26.\n",
      "\n",
      "But perhaps the user thought that 2X +4Y is double of X + Y, so 2*(10)=20, but that's not correct.\n",
      "\n",
      "Alternatively, maybe the user made a mistake in the problem statement.\n",
      "\n",
      "Wait, let me check the original problem again.\n",
      "\n",
      "The user wrote: X+Y=10, X-Y=4. Answer: 2X +4Y=60.\n",
      "\n",
      "Wait, but that's not the answer. So perhaps the user is confused.\n",
      "\n",
      "Alternatively, if the original problem was X + 2Y = 10 and X - Y =4, then adding gives 2X + Y=14. If we multiply that by 2, we get 4X +2Y=28. But that's not 60.\n",
      "\n",
      "Alternatively, if the equations were X + Y=10 and 2X + Y=4, then subtracting gives X= -6, Y=16, which would make 2X +4Y= -12 +64=52.\n",
      "\n",
      "Alternatively, perhaps the original problem was different, like X + Y=10 and X +2Y=20. Then, 3Y=10, Y=10/3, X=10 -10/3=20/3. Then 2X +4Y=40/3 +40/3=80/3≈26.666.\n",
      "\n",
      "Alternatively, maybe the problem was X + Y=10 and X - Y=6, then X=8, Y=2, so 2X +4Y=16+8=24.\n",
      "\n",
      "Alternatively, maybe the user is referring to a different system. Alternatively, perhaps the equations were X + Y=10 and 2X + Y=14, which would give X=2, Y=8, so 2X +4Y=4 +32=36.\n",
      "\n",
      "Alternatively, if the equations were X + Y=10 and 3X + Y=20, subtracting gives 2X=10, X=5, Y=5, so 2X +4Y=10+20=30.\n",
      "\n",
      "Alternatively, maybe the equations were X + Y=10 and 3X + Y=30, then subtracting gives 2X=20, X=10, Y=0, so 2X +4Y=20+0=20.\n",
      "\n",
      "Alternatively, perhaps the equations were X + Y=10 and 2X + Y=30, which would give X=10, Y=0, same as above.\n",
      "\n",
      "Alternatively, perhaps the problem was X + Y=10 and 2X +4Y=60, which would be a different problem.\n",
      "\n",
      "So, in any case, the user's initial thought that X+Y=10 and X-Y=4 gives 2X +4Y=26, which contradicts the answer given as 60, so either the problem is different, or there's a misunderstanding.\n",
      "\n",
      "Alternatively, perhaps the user is solving for 2X +4Y given X+Y=10 and X-Y=4, but that's not possible because with two equations you can solve for X and Y, but not for a combination like 2X +4Y unless it's another equation.\n",
      "\n",
      "Alternatively, perhaps the user is adding the equations incorrectly.\n",
      "\n",
      "So, to sum up, the user's answer of 2X +4Y=60 is incorrect based on the given X+Y=10 and X-Y=4. The correct 2X +4Y is 26.\n",
      "\n",
      "Alternatively, if the problem was different, but as per the given info, 2X +4Y=26.\n",
      "</think>\n",
      "\n",
      "The correct answer to the system of equations \\( X + Y = 10 \\) and \\( X - Y = 4 \\) is \\( X = 7 \\) and \\( Y = 3 \\). Substituting these values into \\( 2X + 4Y \\) gives:\n",
      "\n",
      "\\[\n",
      "2(7) + 4(3) = 14 + 12 = 26\n",
      "\\]\n",
      "\n",
      "Therefore, the correct value of \\( 2X + 4Y \\) is \\(\\boxed{26}\\).\n",
      "\n",
      "Patching layer 20...\n",
      "Collecting clean activations...\n",
      "Setting up patching hook...\n",
      "Generating with patching...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m corrupted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem: X+Y=10, X-Y=4. Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run baseline only (no patching)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# simple_patching_experiment(clean_prompt, corrupted_prompt, None)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run with patching layer 20\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43msimple_patching_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupted_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 86\u001b[0m, in \u001b[0;36msimple_patching_experiment\u001b[0;34m(clean_prompt, corrupted_prompt, layer_to_patch)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Generate with patching\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating with patching...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m patched_generation \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupted_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Remove patching hook\u001b[39;00m\n\u001b[1;32m     89\u001b[0m patch_hook_handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "Cell \u001b[0;32mIn[41], line 4\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, inputs, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(model, tokenizer, inputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_legacy_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Get the generated token IDs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m         decoder_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3248\u001b[0m             (outputs\u001b[38;5;241m.\u001b[39mdecoder_hidden_states,)\n\u001b[1;32m   3249\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   3250\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m (outputs\u001b[38;5;241m.\u001b[39mhidden_states,)\n\u001b[1;32m   3251\u001b[0m         )\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;66;03m# token selection\u001b[39;00m\n\u001b[0;32m-> 3254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[1;32m   3255\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3256\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1163\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ((loss,) \u001b[38;5;241m+\u001b[39m output) \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m TokenClassifierOutput(\n\u001b[1;32m   1153\u001b[0m             loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   1154\u001b[0m             logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[1;32m   1155\u001b[0m             hidden_states\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mhidden_states,\n\u001b[1;32m   1156\u001b[0m             attentions\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mattentions,\n\u001b[1;32m   1157\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForSequenceClassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForQuestionAnswering\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForTokenClassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1167\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    909\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(LLAMA_INPUTS_DOCSTRING)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    912\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 913\u001b[0m     attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m     position_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    915\u001b[0m     past_key_values: Optional[Union[Cache, List[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    916\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    917\u001b[0m     labels: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    918\u001b[0m     use_cache: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    919\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    922\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, SequenceClassifierOutputWithPast]:\n\u001b[1;32m    923\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         return attention_mask\n\u001b[1;32m    636\u001b[0m     return None\n\u001b[1;32m    638\u001b[0m # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n\u001b[1;32m    639\u001b[0m # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n\u001b[0;32m--> 640\u001b[0m # to infer the attention mask.\n\u001b[1;32m    641\u001b[0m past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n\u001b[1;32m    642\u001b[0m using_static_cache = isinstance(past_key_values, StaticCache)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:548\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:255\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[1;32m    249\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[1;32m    252\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[0;32m--> 255\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_attention_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, config\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias\n\u001b[1;32m    256\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    clean_prompt = \"Problem: X+Y=10, X-Y=4. Hint: Add equations to get 2X=14, X=7, then Y=3. Answer:\"\n",
    "    corrupted_prompt = \"Problem: X+Y=10, X-Y=4. Answer:\"\n",
    "    \n",
    "    # Run baseline only (no patching)\n",
    "    # simple_patching_experiment(clean_prompt, corrupted_prompt, None)\n",
    "    \n",
    "    # Run with patching layer 20\n",
    "    simple_patching_experiment(clean_prompt, corrupted_prompt, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(model, tokenizer, prompt, layer_idx, token_position=None):\n",
    "    \"\"\"\n",
    "    Extract the residual stream for a specific layer and token position in a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model (DeepSeek-R1-Distill-Llama-8B)\n",
    "        tokenizer: The tokenizer for the model\n",
    "        prompt: The input text prompt\n",
    "        layer_idx: The specific layer to extract residuals from\n",
    "        token_position: The specific token position to extract residuals for.\n",
    "                       If None, returns residuals for all tokens.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor containing the residual stream for the specified layer\n",
    "    \"\"\"\n",
    "    residual_stream = None\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal residual_stream\n",
    "        # For DeepSeek models, the residual stream is the first element of the input tuple\n",
    "        if isinstance(input, tuple):\n",
    "            hidden_states = input[0]\n",
    "        else:\n",
    "            hidden_states = input\n",
    "            \n",
    "        # Store the residual stream for this layer\n",
    "        if token_position is not None:\n",
    "            # Extract only the specified token position\n",
    "            residual_stream = hidden_states[:, token_position, :].detach().clone()\n",
    "        else:\n",
    "            # Store the full sequence\n",
    "            residual_stream = hidden_states.detach().clone()\n",
    "    \n",
    "    # Register hook only for the specified layer\n",
    "    hook = model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Process the input through the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "    \n",
    "    # Remove the hook\n",
    "    hook.remove()\n",
    "    \n",
    "    return residual_stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Problem: X+Y=10, X-Y=4. Answers<｜Assistant｜><think>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "\n",
    "prompt = \"Problem: X+Y=10, X-Y=4. Answers\"\n",
    "prompt = prepare_prompt(prompt, tokenizer)\n",
    "print(prompt)\n",
    "\n",
    "residuals = get_residual_stream(model=model, tokenizer=tokenizer, prompt=prompt, layer_idx=15, token_position=-2)  # Get residual for last token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "def activation_patching(\n",
    "    clean_prompt: str,\n",
    "    corrupted_prompt: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens: int = 100,\n",
    "    layers_to_patch: Optional[List[int]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform activation patching analysis to understand where model reasoning changes\n",
    "    between a clean prompt and a corrupted prompt (with additional information like a hint).\n",
    "    \n",
    "    Args:\n",
    "        clean_prompt: The base prompt without additional information\n",
    "        corrupted_prompt: The prompt with additional information (e.g., a hint)\n",
    "        model: The language model to analyze\n",
    "        tokenizer: The tokenizer for the model\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        device: Device to run the model on\n",
    "        layers_to_patch: Specific layers to patch (if None, patch all layers)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with all patching results and analysis\n",
    "    \"\"\"\n",
    "    # Format the prompts if chat template is available\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        formatted_clean = prepare_prompt(clean_prompt, tokenizer)\n",
    "        formatted_corrupted = prepare_prompt(corrupted_prompt, tokenizer)\n",
    "    else:\n",
    "        formatted_clean = clean_prompt\n",
    "        formatted_corrupted = corrupted_prompt\n",
    "    \n",
    "    print(f\"Clean prompt: {formatted_clean}\")\n",
    "    print(f\"Corrupted prompt: {formatted_corrupted}\")\n",
    "    \n",
    "    # Generate outputs for both prompts\n",
    "    print(\"Generating outputs for both prompts...\")\n",
    "    clean_output = generate_text(model, tokenizer, formatted_clean, max_new_tokens)\n",
    "    corrupted_output = generate_text(model, tokenizer, formatted_corrupted, max_new_tokens)\n",
    "    \n",
    "    print(f\"Clean output: {clean_output['generated_text']}\")\n",
    "    print(f\"Corrupted output: {corrupted_output['generated_text']}\")\n",
    "    \n",
    "    # Extract residual streams from both prompts\n",
    "    print(\"Extracting residual streams...\")\n",
    "    clean_residuals = extract_all_residual_streams(model, tokenizer, formatted_clean)\n",
    "    corrupted_residuals = extract_all_residual_streams(model, tokenizer, formatted_corrupted)\n",
    "    \n",
    "    # Determine which layers to patch\n",
    "    num_layers = len(model.model.layers)\n",
    "    if layers_to_patch is None:\n",
    "        layers_to_patch = list(range(num_layers))\n",
    "    \n",
    "    # Perform patching at each specified layer\n",
    "    print(\"Running patching experiments...\")\n",
    "    patching_results = []\n",
    "    \n",
    "    for layer_idx in layers_to_patch:\n",
    "        print(f\"Patching at layer {layer_idx}...\")\n",
    "        patched_output = patch_and_generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            formatted_clean,\n",
    "            layer_idx,\n",
    "            clean_residuals,\n",
    "            corrupted_residuals,\n",
    "            max_new_tokens\n",
    "        )\n",
    "        \n",
    "        # Calculate similarity metrics\n",
    "        clean_sim = calculate_similarity(patched_output['generated_text'], clean_output['generated_text'])\n",
    "        corrupted_sim = calculate_similarity(patched_output['generated_text'], corrupted_output['generated_text'])\n",
    "        \n",
    "        patching_results.append({\n",
    "            'layer': layer_idx,\n",
    "            'patched_output': patched_output,\n",
    "            'clean_similarity': clean_sim,\n",
    "            'corrupted_similarity': corrupted_sim\n",
    "        })\n",
    "    \n",
    "    # Return comprehensive results\n",
    "    return {\n",
    "        'clean_prompt': clean_prompt,\n",
    "        'corrupted_prompt': corrupted_prompt,\n",
    "        'clean_output': clean_output,\n",
    "        'corrupted_output': corrupted_output,\n",
    "        'patching_results': patching_results,\n",
    "        'num_layers': num_layers\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_all_residual_streams(model, tokenizer, prompt):\n",
    "    \"\"\"\n",
    "    Extract residual streams from all layers for a given prompt.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping layer indices to residual streams\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Dictionary to store residual streams\n",
    "    residual_streams = {}\n",
    "    \n",
    "    # Define hook function to capture residual streams\n",
    "    def create_hook(layer_idx):\n",
    "        def hook_fn(module, input_tensors, output):\n",
    "            # Extract the residual stream (input hidden states)\n",
    "            if isinstance(input_tensors, tuple):\n",
    "                hidden_states = input_tensors[0]\n",
    "            else:\n",
    "                hidden_states = input_tensors\n",
    "            \n",
    "            # Store a copy\n",
    "            residual_streams[layer_idx] = hidden_states.detach().clone()\n",
    "        \n",
    "        return hook_fn\n",
    "    \n",
    "    # Register hooks for all layers\n",
    "    hooks = []\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        hook = layer.register_forward_hook(create_hook(idx))\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Process input through the model\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "    \n",
    "    # Remove all hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return residual_streams\n",
    "\n",
    "\n",
    "def patch_and_generate(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    layer_idx, \n",
    "    clean_residuals, \n",
    "    corrupted_residuals, \n",
    "    max_new_tokens=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text with the residual stream at a specific layer\n",
    "    patched from the corrupted prompt residual stream.\n",
    "    \n",
    "    This optimized implementation patches only the prompt tokens once\n",
    "    and removes the hook after patching to improve performance.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    \n",
    "    # Initialize with the input tokens\n",
    "    generated_ids = input_ids.clone()\n",
    "    \n",
    "    # Get the corrupted residual stream for this layer\n",
    "    corrupted_residual = corrupted_residuals[layer_idx]\n",
    "    \n",
    "    # Original prompt length\n",
    "    prompt_length = input_ids.shape[1]\n",
    "    \n",
    "    # Hook reference for removal\n",
    "    hook_ref = [None]\n",
    "    \n",
    "    # Define patching hook that removes itself after patching\n",
    "    def patching_hook(module, input_tensors):\n",
    "        # Get current hidden states\n",
    "        if isinstance(input_tensors, tuple):\n",
    "            hidden_states = input_tensors[0]\n",
    "        else:\n",
    "            hidden_states = input_tensors\n",
    "        \n",
    "        # Create patched hidden states\n",
    "        patched_states = hidden_states.clone()\n",
    "        \n",
    "        # Determine how much to patch\n",
    "        min_seq_len = min(\n",
    "            prompt_length,\n",
    "            corrupted_residual.shape[1],\n",
    "            patched_states.shape[1]\n",
    "        )\n",
    "        \n",
    "        # Patch the prompt portion\n",
    "        patched_states[:, :min_seq_len, :] = corrupted_residual[:, :min_seq_len, :]\n",
    "        \n",
    "        # Remove the hook now that patching is complete\n",
    "        if hook_ref[0] is not None:\n",
    "            hook_ref[0].remove()\n",
    "            hook_ref[0] = None  # Indicate hook is removed\n",
    "        \n",
    "        # Return patched hidden states\n",
    "        if isinstance(input_tensors, tuple):\n",
    "            return (patched_states,) + input_tensors[1:]\n",
    "        else:\n",
    "            return patched_states\n",
    "    \n",
    "    # Register the hook\n",
    "    hook = model.model.layers[layer_idx].register_forward_pre_hook(patching_hook)\n",
    "    hook_ref[0] = hook\n",
    "    \n",
    "    try:\n",
    "        # Generate tokens one by one\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Create attention mask\n",
    "                attention_mask = torch.ones((1, generated_ids.shape[1]), device=model.device)\n",
    "                \n",
    "                # Forward pass through the model\n",
    "                outputs = model(\n",
    "                    input_ids=generated_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Get the next token prediction\n",
    "                next_token_logits = outputs.logits[0, -1, :]\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                # Append the token to the sequence\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                # Check if EOS token was generated\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "    finally:\n",
    "        # Make sure the hook is removed if it hasn't been already\n",
    "        if hook_ref[0] is not None:\n",
    "            hook_ref[0].remove()\n",
    "    \n",
    "    # Decode the full output\n",
    "    full_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part\n",
    "    input_length = input_ids.shape[1]\n",
    "    generated_tokens = generated_ids[0, input_length:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'full_output': full_output,\n",
    "        'generated_text': generated_text,\n",
    "        'tokens': generated_ids[0].tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate text without patching\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the full output\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part (without the prompt)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    generated_tokens = outputs[0, input_length:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'full_output': full_output,\n",
    "        'generated_text': generated_text,\n",
    "        'tokens': outputs[0].tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"Calculate text similarity using word overlap (Jaccard similarity)\"\"\"\n",
    "    words1 = set(text1.lower().split())\n",
    "    words2 = set(text2.lower().split())\n",
    "    \n",
    "    intersection = len(words1.intersection(words2))\n",
    "    union = len(words1.union(words2))\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def visualize_patching_results(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the effect of patching at different layers.\n",
    "    \n",
    "    Args:\n",
    "        results: The results dictionary from activation_patching\n",
    "        save_path: Optional path to save the figure\n",
    "    \"\"\"\n",
    "    # Extract data for plotting\n",
    "    layers = [r['layer'] for r in results['patching_results']]\n",
    "    clean_sim = [r['clean_similarity'] for r in results['patching_results']]\n",
    "    corrupted_sim = [r['corrupted_similarity'] for r in results['patching_results']]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(layers, clean_sim, 'b-', label='Similarity to Clean Output', marker='o')\n",
    "    plt.plot(layers, corrupted_sim, 'r-', label='Similarity to Corrupted Output', marker='x')\n",
    "    \n",
    "    plt.title('Effect of Residual Stream Patching Across Model Layers')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Output Similarity')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(np.arange(0, max(layers)+1, 2))\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    # Annotate significant changes\n",
    "    for i in range(1, len(corrupted_sim)):\n",
    "        change = corrupted_sim[i] - corrupted_sim[i-1]\n",
    "        if abs(change) > 0.1:  # Threshold for significant change\n",
    "            plt.annotate(\n",
    "                f'Δ={change:.2f}',\n",
    "                xy=(layers[i], corrupted_sim[i]),\n",
    "                xytext=(layers[i], corrupted_sim[i] + 0.05),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "                ha='center'\n",
    "            )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nPatching Analysis Summary:\")\n",
    "    print(f\"Clean prompt: {results['clean_prompt']}\")\n",
    "    print(f\"Clean output: {results['clean_output']['generated_text']}\")\n",
    "    print(f\"Corrupted prompt: {results['corrupted_prompt']}\")\n",
    "    print(f\"Corrupted output: {results['corrupted_output']['generated_text']}\")\n",
    "    \n",
    "    # Find layer with maximum effect\n",
    "    max_effect_idx = np.argmax(corrupted_sim)\n",
    "    max_layer = results['patching_results'][max_effect_idx]['layer']\n",
    "    max_output = results['patching_results'][max_effect_idx]['patched_output']['generated_text']\n",
    "    print(f\"\\nLayer with maximum effect: {max_layer}\")\n",
    "    print(f\"Output at maximum effect layer: {max_output}\")\n",
    "    \n",
    "    # Find layers with significant jumps\n",
    "    significant_layers = []\n",
    "    for i in range(1, len(corrupted_sim)):\n",
    "        change = corrupted_sim[i] - corrupted_sim[i-1]\n",
    "        if abs(change) > 0.1:  # Threshold for significant change\n",
    "            significant_layers.append((layers[i], change))\n",
    "    \n",
    "    if significant_layers:\n",
    "        print(\"\\nLayers with significant changes in output:\")\n",
    "        for layer, change in significant_layers:\n",
    "            direction = \"toward corrupted output\" if change > 0 else \"toward clean output\"\n",
    "            print(f\"Layer {layer}: Change of {change:.2f} {direction}\")\n",
    "            \n",
    "    # Find layer where reasoning \"flips\" (midpoint between clean and corrupted)\n",
    "    flip_distances = [abs(clean - corrupted) for clean, corrupted in zip(clean_sim, corrupted_sim)]\n",
    "    flip_layer_idx = np.argmin(flip_distances)\n",
    "    flip_layer = layers[flip_layer_idx]\n",
    "    flip_output = results['patching_results'][flip_layer_idx]['patched_output']['generated_text']\n",
    "    print(f\"\\nLayer where output is most balanced between clean/corrupted: {flip_layer}\")\n",
    "    print(f\"Output at balance point: {flip_output}\")\n",
    "\n",
    "\n",
    "def prepare_prompt(prompt, tokenizer):\n",
    "    \"\"\"\n",
    "    Format the prompt using the chat template if available\n",
    "    \n",
    "    Args:\n",
    "        prompt: The raw prompt text\n",
    "        tokenizer: The tokenizer for the model\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt ready for model input\n",
    "    \"\"\"\n",
    "    # Apply the chat template to format the prompt properly\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Format the prompt using the chat template if available\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        # Fallback for tokenizers without chat templates\n",
    "        formatted_prompt = prompt\n",
    "        \n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Additional utility function for easy patching across multiple examples\n",
    "# def patch_multiple_examples(examples, model, tokenizer, max_new_tokens=100, save_dir=\"results\"):\n",
    "#     \"\"\"\n",
    "#     Run activation patching on multiple example pairs\n",
    "    \n",
    "#     Args:\n",
    "#         examples: List of (clean_prompt, corrupted_prompt) tuples\n",
    "#         model: The model to analyze\n",
    "#         tokenizer: The tokenizer for the model\n",
    "#         max_new_tokens: Maximum tokens to generate\n",
    "#         save_dir: Directory to save visualization results\n",
    "    \n",
    "#     Returns:\n",
    "#         List of results dictionaries\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     all_results = []\n",
    "    \n",
    "#     for i, (clean_prompt, corrupted_prompt) in enumerate(examples):\n",
    "#         print(f\"\\n\\nExample {i+1}/{len(examples)}\")\n",
    "#         print(\"=\" * 80)\n",
    "        \n",
    "#         # Run activation patching\n",
    "#         results = activation_patching(\n",
    "#             clean_prompt=clean_prompt,\n",
    "#             corrupted_prompt=corrupted_prompt,\n",
    "#             model=model,\n",
    "#             tokenizer=tokenizer,\n",
    "#             max_new_tokens=max_new_tokens\n",
    "#         )\n",
    "        \n",
    "#         # Save visualization\n",
    "#         save_path = os.path.join(save_dir, f\"example_{i+1}_patching.png\")\n",
    "#         visualize_patching_results(results, save_path=save_path)\n",
    "        \n",
    "#         all_results.append(results)\n",
    "    \n",
    "#     return all_results\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     example_patching_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def example_patching_analysis(model, tokenizer):\n",
    "    \"\"\"Example demonstrating how to use the activation patching framework\"\"\"\n",
    "\n",
    "    tokenizer = tokenizer\n",
    "    model = model\n",
    "    \n",
    "    # Define clean and corrupted prompts\n",
    "    clean_prompt = \"Solve the quadratic equation: 3x^2 - 12x + 9 = 0\"\n",
    "    corrupted_prompt = \"Solve the quadratic equation: 3x^2 - 12x + 9 = 0 \\n Answer:\"\n",
    "    \n",
    "    # Run activation patching\n",
    "    results = activation_patching(\n",
    "        clean_prompt=clean_prompt,\n",
    "        corrupted_prompt=corrupted_prompt,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=200,\n",
    "        layers_to_patch=[21]      # Patch every other layer to save time\n",
    "    )\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_patching_results(results, save_path=\"patching_results.png\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean prompt: <｜begin▁of▁sentence｜><｜User｜>Solve the quadratic equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "\n",
      "Corrupted prompt: <｜begin▁of▁sentence｜><｜User｜>Solve the quadratic equation: 3x^2 - 12x + 9 = 0 \n",
      " Answer:<｜Assistant｜><think>\n",
      "\n",
      "Generating outputs for both prompts...\n",
      "Clean output: I start by examining the quadratic equation \\(3x^2 - 12x + 9 = 0\\). I notice that all the coefficients are divisible by 3, so I simplify the equation by dividing each term by 3, which gives me \\(x^2 - 4x + 3 = 0\\).\n",
      "\n",
      "Next, I identify the coefficients: \\(a = 1\\), \\(b = -4\\), and \\(c = 3\\). I calculate the discriminant using the formula \\(\\Delta = b^2 - 4ac\\), which equals \\((-4)^2 - 4(1)(3) = 16 - 12 = 4\\).\n",
      "\n",
      "Since the discriminant is a perfect square, I know there are two real solutions. I apply the quadratic formula: \\(x = \\frac{-b \\pm \\sqrt{\\Delta}}{2a}\\). Substituting the values, I get \\(x = \\frac{\n",
      "Corrupted output: To solve the quadratic equation \\(3x^2 - 12x + 9 = 0\\), I'll start by identifying the coefficients of the equation. The coefficient \\(a\\) is 3, \\(b\\) is -12, and \\(c\\) is 9.\n",
      "\n",
      "Next, I'll use the quadratic formula, which is \\(x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\). Plugging in the values, I get \\(x = \\frac{-(-12) \\pm \\sqrt{(-12)^2 - 4 \\times 3 \\times 9}}{2 \\times 3}\\).\n",
      "\n",
      "Calculating the discriminant inside the square root: \\((-12)^2 = 144\\) and \\(4 \\times 3 \\times 9 = 108\\). Subtracting these gives \\(144 - 108 = 36\\). The square root of 36 is 6.\n",
      "\n",
      "Now,\n",
      "Extracting residual streams...\n",
      "Running patching experiments...\n",
      "Patching at layer 21...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexample_patching_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mexample_patching_analysis\u001b[0;34m(model, tokenizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m corrupted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolve the quadratic equation: 3x^2 - 12x + 9 = 0 \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Run activation patching\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_patching\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrupted_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrupted_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers_to_patch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Patch every other layer to save time\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m     23\u001b[0m visualize_patching_results(results, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatching_results.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mactivation_patching\u001b[0;34m(clean_prompt, corrupted_prompt, model, tokenizer, max_new_tokens, layers_to_patch)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m layers_to_patch:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatching at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m     patched_output \u001b[38;5;241m=\u001b[39m \u001b[43mpatch_and_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatted_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorrupted_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Calculate similarity metrics\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     clean_sim \u001b[38;5;241m=\u001b[39m calculate_similarity(patched_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m], clean_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 219\u001b[0m, in \u001b[0;36mpatch_and_generate\u001b[0;34m(model, tokenizer, prompt, layer_idx, clean_residuals, corrupted_residuals, max_new_tokens)\u001b[0m\n\u001b[1;32m    216\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, generated_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), device\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Get the next token prediction\u001b[39;00m\n\u001b[1;32m    225\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    584\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m         position_embeddings,\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:336\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:272\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    271\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 272\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    274\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    275\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "example_patching_analysis(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
