{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Print CUDA details\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "        print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")\n",
    "        return device\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    dtype = torch.bfloat16\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,  # Optimize memory usage during loading\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt: str, tokenizer) -> str:\n",
    "    \"\"\"Format the prompt using the chat template if available\"\"\"\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        return formatted_prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(prompt: str, tokenizer, device=None):\n",
    "    \"\"\"Tokenize a prompt and prepare model inputs\"\"\"\n",
    "    # Get appropriate device\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    # Format prompt\n",
    "    formatted_prompt = prepare_prompt(prompt, tokenizer)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_prompt = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device\n",
    "    tokenized_prompt = {k: v.to(device) for k, v in tokenized_prompt.items()}\n",
    "    \n",
    "    print(f\"Prompt: {formatted_prompt}\")\n",
    "    \n",
    "    return tokenized_prompt, formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(model, \n",
    "                        tokenizer, \n",
    "                        tokenized_prompt, \n",
    "                        max_new_tokens=20, \n",
    "                        layer_indices=None, \n",
    "                        save_path=None):\n",
    "    \"\"\"\n",
    "    Get and store the residual stream from the model for input tokens and generated tokens.\n",
    "    Optimized to write activations to disk incrementally to reduce memory usage.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        tokenized_prompt: Dictionary containing input_ids and attention_mask tensors\n",
    "        max_new_tokens: Maximum number of new tokens to generate (not including input tokens)\n",
    "        layer_indices: Optional list of specific layer indices to capture. If None, capture all layers.\n",
    "        save_path: Optional path to save the residual streams (as .pt file)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "            - all_residual_streams: Dict mapping from layer index to tensors of shape \n",
    "              [batch_size, seq_len, hidden_size] representing the residual stream at each layer\n",
    "            - generated_text: The text including the generated tokens\n",
    "            - token_ids: The token IDs including the generated tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = tokenized_prompt[\"input_ids\"].shape[0]\n",
    "    device = tokenized_prompt[\"input_ids\"].device\n",
    "    input_length = tokenized_prompt[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Create a temporary directory for storing activations\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"residual_stream_\")\n",
    "    print(f\"Saving activations to temporary directory: {temp_dir}\")\n",
    "    \n",
    "    # Track which tokens we've processed\n",
    "    current_token_pos = 0\n",
    "    \n",
    "    # Information about sequence length for each step\n",
    "    seq_length_info = []\n",
    "    \n",
    "    # Register hooks to capture residual streams\n",
    "    hooks = []\n",
    "    \n",
    "    # Function to determine if we should capture a specific layer\n",
    "    def should_capture_layer(idx):\n",
    "        if layer_indices is None:\n",
    "            return True\n",
    "        return idx in layer_indices\n",
    "    \n",
    "    # Define hook function to capture and save residual stream directly to disk\n",
    "    def get_activation(name, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal current_token_pos\n",
    "            \n",
    "            # For LLaMA models, the residual stream is the input to the layer\n",
    "            # Get the activation (residual stream)\n",
    "            activation = input[0].detach().cpu()\n",
    "            \n",
    "            # Save the current sequence length\n",
    "            if len(seq_length_info) <= current_token_pos:\n",
    "                seq_length_info.append(activation.shape[1])\n",
    "            \n",
    "            # Create filename for this activation\n",
    "            filename = f\"layer_{layer_idx}_{name}_token_{current_token_pos}.pt\"\n",
    "            file_path = os.path.join(temp_dir, filename)\n",
    "            \n",
    "            # Save activation to disk\n",
    "            torch.save(activation, file_path)\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks for each transformer layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        if should_capture_layer(i):\n",
    "            # Capture the input to the input_layernorm which is the residual stream\n",
    "            hook = layer.input_layernorm.register_forward_hook(get_activation(\"input_layernorm\", i))\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    # Also capture the final norm layer (after the last transformer layer)\n",
    "    final_layer_idx = len(model.model.layers)\n",
    "    final_hook = model.model.norm.register_forward_hook(\n",
    "        get_activation(\"final_norm\", final_layer_idx)\n",
    "    )\n",
    "    hooks.append(final_hook)\n",
    "    \n",
    "    # Clone the tokenized_prompt to avoid modifying the original\n",
    "    generation_inputs = {\n",
    "        \"input_ids\": tokenized_prompt[\"input_ids\"].clone(),\n",
    "        \"attention_mask\": tokenized_prompt[\"attention_mask\"].clone(),\n",
    "    }\n",
    "    \n",
    "    # Custom generation function to handle incrementing token position\n",
    "    generated_ids = None\n",
    "    \n",
    "    try:\n",
    "        # Save metadata about the generation\n",
    "        metadata = {\n",
    "            'input_length': input_length,\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'layer_indices': layer_indices or list(range(len(model.model.layers) + 1)),  # +1 for final norm\n",
    "        }\n",
    "        torch.save(metadata, os.path.join(temp_dir, \"metadata.pt\"))\n",
    "        \n",
    "        # Print information about captured layers\n",
    "        layer_indices_to_capture = metadata['layer_indices']\n",
    "        print(f\"Capturing {len(layer_indices_to_capture)} layers: {layer_indices_to_capture}\")\n",
    "        \n",
    "        # We'll use our own token-by-token generation to have more control\n",
    "        with torch.no_grad():\n",
    "            # First, process the input sequence\n",
    "            input_ids = generation_inputs[\"input_ids\"]\n",
    "            attention_mask = generation_inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Show progress info for initial processing\n",
    "            print(\"Processing input sequence...\")\n",
    "            # Initial forward pass with the input sequence\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Input sequence processed ({input_length} tokens)\")\n",
    "            \n",
    "            # Increment token position after processing the input sequence\n",
    "            current_token_pos += 1\n",
    "            \n",
    "            # Continue generating tokens one by one\n",
    "            generated_sequence = input_ids.clone()\n",
    "            \n",
    "            # Initialize progress bar for token generation\n",
    "            pbar = tqdm(total=max_new_tokens, desc=\"Generating tokens\")\n",
    "            \n",
    "            for i in range(max_new_tokens):\n",
    "                # Get next token prediction\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                # Append new token to sequence\n",
    "                generated_sequence = torch.cat([generated_sequence, next_token_id], dim=-1)\n",
    "                \n",
    "                # Update progress bar with token info\n",
    "                token_str = tokenizer.decode(next_token_id[0], skip_special_tokens=False)\n",
    "                pbar.set_postfix({\"token\": token_str})\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Check if we've generated an EOS token\n",
    "                if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # Forward pass with the updated sequence\n",
    "                # We only need to process the new token, using KV cache for efficiency\n",
    "                outputs = model(\n",
    "                    input_ids=next_token_id,\n",
    "                    attention_mask=torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=1),\n",
    "                    use_cache=True,\n",
    "                    past_key_values=outputs.past_key_values,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                # Update attention mask for next iteration\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=1)\n",
    "                \n",
    "                # Increment token position after processing a new token\n",
    "                current_token_pos += 1\n",
    "                \n",
    "            # Close the progress bar\n",
    "            pbar.close()\n",
    "            \n",
    "            # Store the final generated sequence\n",
    "            generated_ids = generated_sequence\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Save sequence length information\n",
    "    torch.save(seq_length_info, os.path.join(temp_dir, \"seq_length_info.pt\"))\n",
    "    \n",
    "    # Decode the generated text\n",
    "    try:\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        # Print a preview of the generated text\n",
    "        preview = generated_text.replace('\\n', '\\\\n')\n",
    "        if len(preview) > 100:\n",
    "            preview = preview[:97] + '...'\n",
    "        input_tokens = input_length\n",
    "        output_tokens = generated_ids.shape[1]\n",
    "        new_tokens = output_tokens - input_tokens\n",
    "        print(f\"Generated text: {preview}\")\n",
    "        print(f\"Input tokens: {input_tokens}, New tokens: {new_tokens}, Total tokens: {output_tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding generated tokens: {e}\")\n",
    "        generated_text = f\"[Error decoding tokens: {generated_ids[0]}]\"\n",
    "    \n",
    "    # Store token ids for reference\n",
    "    token_ids = generated_ids.detach().cpu()\n",
    "    torch.save(token_ids, os.path.join(temp_dir, \"token_ids.pt\"))\n",
    "    \n",
    "    # Load the individual files and combine them into the return format\n",
    "    all_residual_streams = {}\n",
    "    for layer_idx in metadata['layer_indices']:\n",
    "        all_residual_streams[layer_idx] = []\n",
    "        \n",
    "        # Load all token files for this layer\n",
    "        for token_pos in range(len(seq_length_info)):\n",
    "            # Try loading input_layernorm activation\n",
    "            try:\n",
    "                filename = f\"layer_{layer_idx}_input_layernorm_token_{token_pos}.pt\"\n",
    "                file_path = os.path.join(temp_dir, filename)\n",
    "                if os.path.exists(file_path):\n",
    "                    activation = torch.load(file_path)\n",
    "                    all_residual_streams[layer_idx].append(activation)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            # Try loading final_norm activation\n",
    "            try:\n",
    "                filename = f\"layer_{layer_idx}_final_norm_token_{token_pos}.pt\"\n",
    "                file_path = os.path.join(temp_dir, filename)\n",
    "                if os.path.exists(file_path):\n",
    "                    activation = torch.load(file_path)\n",
    "                    all_residual_streams[layer_idx].append(activation)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        # Concatenate activations for this layer\n",
    "        if all_residual_streams[layer_idx]:\n",
    "            all_residual_streams[layer_idx] = torch.cat(all_residual_streams[layer_idx], dim=1)\n",
    "        else:\n",
    "            print(f\"Warning: No activations found for layer {layer_idx}\")\n",
    "            all_residual_streams.pop(layer_idx)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_path is not None:\n",
    "        save_data = {\n",
    "            'residual_streams': all_residual_streams,\n",
    "            'generated_text': generated_text,\n",
    "            'token_ids': token_ids\n",
    "        }\n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(save_data, save_path)\n",
    "            print(f\"Residual stream data saved to {save_path} \\n-----\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving residual stream data: {e}\")\n",
    "    \n",
    "    # Clean up temporary files if requested (can be uncommented if you want to auto-delete temp files)\n",
    "    # import shutil\n",
    "    # shutil.rmtree(temp_dir)\n",
    "    # print(f\"Temporary files cleaned up from {temp_dir}\")\n",
    "    \n",
    "    return all_residual_streams, generated_text, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_residual_stream(load_path):\n",
    "    \"\"\"\n",
    "    Load previously saved residual stream data from a file.\n",
    "    \n",
    "    Args:\n",
    "        load_path: Path to the saved residual stream file (.pt)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "    \"\"\"\n",
    "    data = torch.load(load_path)\n",
    "    return data['residual_streams'], data['generated_text'], data['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_res_stream(model, \n",
    "                    tokenizer, \n",
    "                    clean_path, \n",
    "                    corrupt_path, \n",
    "                    patch_layers, \n",
    "                    num_tokens_to_patch=None, \n",
    "                    target_token=\"<think>\", \n",
    "                    save_path=None, \n",
    "                    device=None, \n",
    "                    max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Patch residual streams from corrupted prompt into the clean prompt at specified layers and tokens,\n",
    "    supporting autoregressive generation with improved progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        clean_path: Path to the saved clean prompt residual stream file (.pt)\n",
    "        corrupt_path: Path to the saved corrupted prompt residual stream file (.pt)\n",
    "        patch_layers: List of layer indices to patch (e.g., [5, 6, 7] or range(32))\n",
    "        num_tokens_to_patch: Number of tokens to patch after the target token.\n",
    "                           If None, will patch from the target token to the end.\n",
    "        target_token: Token after which to start patching (default: \"<think>\")\n",
    "        save_path: Optional path to save the patched results (.pt)\n",
    "        device: Device to perform computation on\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (patched_text, original_clean_text, original_corrupt_text, patched_token_ids)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Set device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the saved residual streams\n",
    "    print(f\"Loading residual streams from {clean_path} and {corrupt_path}...\")\n",
    "    clean_streams, clean_text, clean_ids = load_residual_stream(clean_path)\n",
    "    corrupt_streams, corrupt_text, corrupt_ids = load_residual_stream(corrupt_path)\n",
    "    \n",
    "    # Ensure both clean_ids and corrupt_ids are on CPU for initial processing\n",
    "    clean_ids = clean_ids.cpu()\n",
    "    corrupt_ids = corrupt_ids.cpu()\n",
    "    \n",
    "    print(f\"Clean text preview: {clean_text[:100]}...\")\n",
    "    print(f\"Corrupt text preview: {corrupt_text[:100]}...\")\n",
    "    \n",
    "    # Find the target token index in the clean prompt\n",
    "    if target_token:\n",
    "        # Check if target token is a string or a token ID\n",
    "        if isinstance(target_token, str):\n",
    "            # Find the token ID for the target token\n",
    "            target_token_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "            if len(target_token_ids) != 1:\n",
    "                print(f\"Warning: Target token '{target_token}' encoded to {len(target_token_ids)} tokens: {target_token_ids}\")\n",
    "            target_token_id = target_token_ids[0]\n",
    "        else:\n",
    "            target_token_id = target_token\n",
    "            \n",
    "        # Find the position of the target token in the clean_ids\n",
    "        target_positions = (clean_ids[0] == target_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(target_positions) == 0:\n",
    "            print(f\"Target token '{target_token}' not found in clean_ids. Will patch from the beginning.\")\n",
    "            patch_start_idx = 0\n",
    "        else:\n",
    "            # Take the first occurrence of the target token\n",
    "            patch_start_idx = target_positions[0].item()\n",
    "            target_token_text = tokenizer.decode(target_token_id)\n",
    "            print(f\"Found target token '{target_token_text}' (ID: {target_token_id}) at position {patch_start_idx}\")\n",
    "    else:\n",
    "        # If no target token specified, start patching from the beginning\n",
    "        patch_start_idx = 0\n",
    "    \n",
    "    # Set the patch range\n",
    "    if num_tokens_to_patch is None:\n",
    "        # From target token to the end\n",
    "        patch_end_idx = min(clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    else:\n",
    "        # Patch specified number of tokens after the target token\n",
    "        patch_end_idx = min(patch_start_idx + num_tokens_to_patch, clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    \n",
    "    patch_tokens_range = (patch_start_idx, patch_end_idx)\n",
    "    \n",
    "    print(f\"Will patch tokens from position {patch_start_idx} to {patch_end_idx} ({patch_end_idx - patch_start_idx} tokens)\")\n",
    "    print(f\"Will patch layers: {patch_layers}\")\n",
    "    \n",
    "    # Validate layers\n",
    "    max_layer = max(clean_streams.keys())\n",
    "    patch_layers = [layer for layer in patch_layers if layer in clean_streams and layer in corrupt_streams]\n",
    "    if not patch_layers:\n",
    "        raise ValueError(f\"No valid layers to patch. Layers must be in range 0-{max_layer}\")\n",
    "    \n",
    "    # Preload corrupted residual streams into device memory for faster access during generation\n",
    "    # Only preload the layers and tokens we need to patch\n",
    "    corrupted_cache = {}\n",
    "    \n",
    "    print(\"Preloading corrupted residual streams to device...\")\n",
    "    for layer_idx in tqdm(patch_layers, desc=\"Preloading layers\"):\n",
    "        # Get the corrupted streams for this layer, but only for tokens we'll patch\n",
    "        corrupted_cache[layer_idx] = corrupt_streams[layer_idx][0, patch_start_idx:patch_end_idx, :].to(device)\n",
    "    \n",
    "    # Create patched inputs based on clean prompt\n",
    "    input_ids = clean_ids[0, :patch_start_idx].unsqueeze(0).to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    \n",
    "    patched_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "    \n",
    "    # Set up patching state for autoregressive generation\n",
    "    # Position relative to the start of the patch range (0 = first token to patch)\n",
    "    patching_pos = 0\n",
    "    # Absolute position in the sequence\n",
    "    current_token_pos = patch_start_idx\n",
    "    \n",
    "    # Set up patching progress tracking\n",
    "    expected_tokens_to_patch = min(patch_end_idx - patch_start_idx, max_new_tokens)\n",
    "    progress_bar = tqdm(total=expected_tokens_to_patch, desc=\"Patching tokens\", ncols=100)\n",
    "    \n",
    "    # Create a hook to patch each token during autoregressive generation\n",
    "    hooks = []\n",
    "    \n",
    "    # Dictionary to track patches applied\n",
    "    patch_stats = {\n",
    "        \"total_tokens\": 0,\n",
    "        \"patched_tokens\": 0,\n",
    "        \"patched_per_layer\": {layer: 0 for layer in patch_layers}\n",
    "    }\n",
    "    \n",
    "    def patch_hook(layer_idx):\n",
    "        def hook(module, inputs):\n",
    "            nonlocal patching_pos, current_token_pos\n",
    "            \n",
    "            # Get residual stream\n",
    "            res_stream = inputs[0]\n",
    "            batch_size, seq_len, hidden_dim = res_stream.shape\n",
    "            \n",
    "            # During autoregressive generation, we'll get one token at a time\n",
    "            # For the initial pass with multiple tokens, patch only the target position and beyond\n",
    "            if seq_len > 1:\n",
    "                # This is the initial forward pass with all input tokens\n",
    "                # We don't patch anything in the input prefix (before target token)\n",
    "                return inputs\n",
    "            else:\n",
    "                # This is autoregressive generation with one token at a time\n",
    "                # Only update position tracking in the first layer to avoid duplicate counting\n",
    "                if layer_idx == patch_layers[0]:\n",
    "                    # Move to the next patching position\n",
    "                    patching_pos += 1\n",
    "                    current_token_pos += 1\n",
    "                    \n",
    "                    # Update progress bar only once per token (in first layer)\n",
    "                    if patching_pos <= expected_tokens_to_patch:\n",
    "                        progress_bar.update(1)\n",
    "                        patch_stats[\"total_tokens\"] += 1\n",
    "                \n",
    "                # Check if this position should be patched\n",
    "                if patching_pos > 0 and patching_pos <= (patch_end_idx - patch_start_idx):\n",
    "                    # Get the corresponding token from the preloaded corrupted cache\n",
    "                    # The position in the cache is (patching_pos - 1) because patching_pos starts at 1\n",
    "                    to_patch = corrupted_cache[layer_idx][patching_pos - 1, :]\n",
    "                    \n",
    "                    # Replace the residual stream for this token with the corrupted version\n",
    "                    res_stream[0, 0, :] = to_patch\n",
    "                    \n",
    "                    # Track stats\n",
    "                    if layer_idx == patch_layers[0]:\n",
    "                        patch_stats[\"patched_tokens\"] += 1\n",
    "                    patch_stats[\"patched_per_layer\"][layer_idx] += 1\n",
    "                    \n",
    "                    # Periodically log progress (less verbose)\n",
    "                    if patching_pos % 10 == 0 and layer_idx == patch_layers[0]:\n",
    "                        token_text = tokenizer.decode(input_ids[0, -1].item())\n",
    "                        current_token = token_text.replace('\\n', '\\\\n')\n",
    "                        progress_bar.set_postfix({\"current_token\": current_token, \"pos\": current_token_pos})\n",
    "            \n",
    "            # Return potentially modified inputs\n",
    "            return (res_stream,) + inputs[1:] if len(inputs) > 1 else (res_stream,)\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    # Register forward pre-hooks for each layer to patch\n",
    "    for layer_idx in patch_layers:\n",
    "        hook = model.model.layers[layer_idx].input_layernorm.register_forward_pre_hook(\n",
    "            patch_hook(layer_idx)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Generate text with patched residual streams\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Generate text with the patched residual stream\n",
    "            generation_output = model.generate(\n",
    "                **patched_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            patched_ids = generation_output.sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation with patched residual stream: {e}\")\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    finally:\n",
    "        # Remove all hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Close progress bar\n",
    "        progress_bar.close()\n",
    "    \n",
    "    # Decode the patched generation\n",
    "    patched_text = tokenizer.decode(patched_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- Patching Results ---\")\n",
    "    print(f\"Patched layers: {patch_layers}\")\n",
    "    print(f\"Patched token range: {patch_tokens_range}\")\n",
    "    print(f\"Total tokens processed: {patch_stats['total_tokens']}\")\n",
    "    print(f\"Total tokens patched: {patch_stats['patched_tokens']}\")\n",
    "    print(f\"Tokens patched per layer: {patch_stats['patched_per_layer']}\")\n",
    "    print(f\"Generation length: {patched_ids.shape[1] - input_ids.shape[1]} new tokens\")\n",
    "    \n",
    "    print(\"\\nOriginal clean output: \")\n",
    "    print(clean_text[:200] + \"...\" if len(clean_text) > 200 else clean_text)\n",
    "    print(\"\\nOriginal corrupt output: \")\n",
    "    print(corrupt_text[:200] + \"...\" if len(corrupt_text) > 200 else corrupt_text)\n",
    "    print(\"\\nPatched output: \")\n",
    "    print(patched_text[:200] + \"...\" if len(patched_text) > 200 else patched_text)\n",
    "    \n",
    "    # Save patched results if requested\n",
    "    if save_path:\n",
    "        patched_data = {\n",
    "            'patched_text': patched_text,\n",
    "            'clean_text': clean_text,\n",
    "            'corrupt_text': corrupt_text,\n",
    "            'patched_token_ids': patched_ids.cpu(),\n",
    "            'patch_stats': patch_stats,\n",
    "            'patch_info': {\n",
    "                'patch_layers': patch_layers,\n",
    "                'patch_tokens_range': patch_tokens_range,\n",
    "                'clean_path': clean_path,\n",
    "                'corrupt_path': corrupt_path\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(patched_data, save_path)\n",
    "            print(f\"Patched results saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving patched results: {e}\")\n",
    "    \n",
    "    return patched_text, clean_text, corrupt_text, patched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_prompt = \"What is the capital of Finland?\"\n",
    "clean_tokenized_prompt, clean_formatted_prompt = tokenize_prompt(clean_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "clean_residual_streams, clean_generated_text, clean_token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    clean_tokenized_prompt, \n",
    "                                                                    max_new_tokens=512, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/clean-prompt-1-test.pt\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(clean_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_prompt = \"What is the capital of Belgium?\"\n",
    "corrupted_tokenized_prompt, corrupted_formatted_prompt = tokenize_prompt(corrupted_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "corrupted_residual_streams, corrupted_generated_text, corrupted_token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    corrupted_tokenized_prompt, \n",
    "                                                                    max_new_tokens=512, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/corrupted-prompt-1-test.pt\")\n",
    "\n",
    "print(corrupted_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_text, clean_text, corrupt_text, patched_ids = patch_res_stream(model, \n",
    "                                                                        tokenizer, \n",
    "                                                                        clean_path = \"res-stream/clean-prompt-1-test.pt\", \n",
    "                                                                        corrupt_path = \"res-stream/corrupted-prompt-1-test.pt\", \n",
    "                                                                        patch_layers = [0], \n",
    "                                                                        num_tokens_to_patch=64, \n",
    "                                                                        target_token=\"<think>\", \n",
    "                                                                        save_path=\"res-stream/patched-prompt-1-test.pt\", \n",
    "                                                                        device=None, \n",
    "                                                                        max_new_tokens=512)\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(patched_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"corrupted text:\")\n",
    "print(corrupt_text)\n",
    "print(\"\\n---\")\n",
    "print(\"clean text:\")\n",
    "print(clean_text)\n",
    "print(\"\\n---\")\n",
    "print(\"patched text:\")\n",
    "print(patched_text)\n",
    "print(\"\\n---\")\n",
    "# print(patched_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_streams, generated_text, token_ids = load_residual_stream(\"res-stream/corrupted-prompt-1-test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
