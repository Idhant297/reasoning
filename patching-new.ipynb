{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Print CUDA details\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "        print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")\n",
    "        return device\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_device()\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce371f5b0c7a45f8881d50e5e547ddab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model():\n",
    "    dtype = torch.bfloat16\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,  # Optimize memory usage during loading\n",
    "    )\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt: str, tokenizer) -> str:\n",
    "    \"\"\"Format the prompt using the chat template if available\"\"\"\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        return formatted_prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompts(clean_prompt: str, corrupted_prompt: str, tokenizer, device=None):\n",
    "    \"\"\"Tokenize prompts and prepare model inputs\"\"\"\n",
    "    # Get appropriate device\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    # Format prompts\n",
    "    formatted_clean = prepare_prompt(clean_prompt, tokenizer)\n",
    "    formatted_corrupted = prepare_prompt(corrupted_prompt, tokenizer)\n",
    "    \n",
    "    # Tokenize\n",
    "    clean_tokens = tokenizer(formatted_clean, return_tensors=\"pt\")\n",
    "    corrupted_tokens = tokenizer(formatted_corrupted, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device\n",
    "    clean_tokens = {k: v.to(device) for k, v in clean_tokens.items()}\n",
    "    corrupted_tokens = {k: v.to(device) for k, v in corrupted_tokens.items()}\n",
    "    \n",
    "    print(f\"Clean prompt: {formatted_clean}\")\n",
    "    print(f\"Corrupted prompt: {formatted_corrupted}\")\n",
    "    \n",
    "    return clean_tokens, corrupted_tokens, formatted_clean, formatted_corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(model, inputs, num_layers=None):\n",
    "    \"\"\"Extract hidden states from all layers for given inputs\"\"\"\n",
    "    # Determine number of layers if not provided\n",
    "    if num_layers is None:\n",
    "        num_layers = len(model.model.layers)\n",
    "    \n",
    "    # Use the global DEVICE variable\n",
    "    device = DEVICE\n",
    "    \n",
    "    # Initialize dictionary to store hidden states\n",
    "    hidden_states = {f\"layer_{i}\": [] for i in range(num_layers)}\n",
    "    \n",
    "    # Define hook function to capture hidden states\n",
    "    def create_hook(layer_idx):\n",
    "        def hook_fn(module, inputs, outputs):\n",
    "            # Get the input hidden states (residual stream)\n",
    "            if isinstance(inputs, tuple):\n",
    "                states = inputs[0].detach().clone()\n",
    "            else:\n",
    "                states = inputs.detach().clone()\n",
    "            hidden_states[f\"layer_{layer_idx}\"].append(states)\n",
    "            \n",
    "        return hook_fn\n",
    "    \n",
    "    # Register hooks for all layers\n",
    "    hooks = []\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        if idx < num_layers:\n",
    "            hook = layer.register_forward_hook(create_hook(idx))\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    # Process input through the model\n",
    "    with torch.no_grad():\n",
    "        # Ensure inputs are on the correct device\n",
    "        device_inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Use CUDA streams for better GPU utilization if available\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                model(**device_inputs)\n",
    "                # Ensure CUDA operations are completed\n",
    "                torch.cuda.synchronize()\n",
    "        else:\n",
    "            model(**device_inputs)\n",
    "    \n",
    "    # Remove all hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_and_get_next_token(model, input_ids, attention_mask, layer_idx, corrupted_hidden_states):\n",
    "    \"\"\"\n",
    "    Run a forward pass with patching at the specified layer\n",
    "    and return the predicted next token\n",
    "    \"\"\"\n",
    "    # Flag to track patching\n",
    "    patched = [False]\n",
    "    \n",
    "    # Use the global DEVICE variable\n",
    "    device = DEVICE\n",
    "    \n",
    "    # Define patching hook for forward_pre_hook (gets only module and input)\n",
    "    def patching_hook(module, inputs):\n",
    "        if not patched[0]:\n",
    "            # Get current hidden states\n",
    "            if isinstance(inputs, tuple):\n",
    "                hidden_states = inputs[0]\n",
    "            else:\n",
    "                hidden_states = inputs\n",
    "            \n",
    "            # Get corrupted hidden states for this layer\n",
    "            layer_key = f\"layer_{layer_idx}\"\n",
    "            corrupted_states = corrupted_hidden_states[layer_key]\n",
    "            \n",
    "            # Check if we have corrupted states for this position\n",
    "            if len(corrupted_states) > 0:\n",
    "                # Use the last available corrupted state\n",
    "                corrupted_state = corrupted_states[-1]\n",
    "                \n",
    "                # Create patched hidden states\n",
    "                patched_states = hidden_states.clone()\n",
    "                \n",
    "                # Determine how much we can patch\n",
    "                min_seq_len = min(corrupted_state.shape[1], patched_states.shape[1])\n",
    "                \n",
    "                # Patch what we can\n",
    "                patched_states[:, :min_seq_len, :] = corrupted_state[:, :min_seq_len, :]\n",
    "                \n",
    "                # Mark as patched\n",
    "                patched[0] = True\n",
    "                \n",
    "                # Return patched hidden states\n",
    "                if isinstance(inputs, tuple):\n",
    "                    return (patched_states,) + inputs[1:]\n",
    "                else:\n",
    "                    return patched_states\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    # Register the hook - using forward_pre_hook\n",
    "    hook = model.model.layers[layer_idx].register_forward_pre_hook(patching_hook)\n",
    "    \n",
    "    try:\n",
    "        # Clear CUDA cache if using CUDA to prevent OOM errors\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Ensure inputs are on the correct device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        # Run model with patching\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Use CUDA streams for better GPU utilization if available\n",
    "                if device.type == 'cuda':\n",
    "                    with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                        outputs = model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask\n",
    "                        )\n",
    "                        # Ensure CUDA operations are completed\n",
    "                        torch.cuda.synchronize()\n",
    "                else:\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                    )\n",
    "            except RuntimeError as e:\n",
    "                # If any error occurs, try CPU\n",
    "                print(f\"Error during patching: {str(e)}. Trying CPU...\")\n",
    "                # Save original device to move back later\n",
    "                original_device = next(model.parameters()).device\n",
    "                # Move everything to CPU\n",
    "                model = model.to('cpu')\n",
    "                input_ids = input_ids.to('cpu')\n",
    "                attention_mask = attention_mask.to('cpu')\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Move model back\n",
    "                model.to(device)\n",
    "            \n",
    "        # Get the next token prediction\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
    "        \n",
    "        return next_token\n",
    "    finally:\n",
    "        # Always remove the hook\n",
    "        hook.remove()\n",
    "        # Clear CUDA cache again after processing\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_initial_passes(model, clean_tokens, corrupted_tokens, num_layers=None):\n",
    "    \"\"\"\n",
    "    Run initial forward passes and extract hidden states for both prompts\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (clean_hidden_states, corrupted_hidden_states)\n",
    "    \"\"\"\n",
    "    print(\"Running initial forward passes...\")\n",
    "    \n",
    "    # Use the global DEVICE variable\n",
    "    device = DEVICE\n",
    "    \n",
    "    # Ensure inputs are on the correct device\n",
    "    clean_tokens_device = {k: v.to(device) for k, v in clean_tokens.items()}\n",
    "    corrupted_tokens_device = {k: v.to(device) for k, v in corrupted_tokens.items()}\n",
    "    \n",
    "    # Clear CUDA cache if using CUDA to prevent OOM errors\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Extract hidden states for clean prompt\n",
    "    try:\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                clean_hidden_states = extract_hidden_states(\n",
    "                    model, clean_tokens_device, num_layers=num_layers\n",
    "                )\n",
    "                torch.cuda.synchronize()\n",
    "        else:\n",
    "            clean_hidden_states = extract_hidden_states(\n",
    "                model, clean_tokens_device, num_layers=num_layers\n",
    "            )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during clean prompt processing: {str(e)}. Trying CPU...\")\n",
    "        # Move to CPU for this operation\n",
    "        cpu_tokens = {k: v.to('cpu') for k, v in clean_tokens.items()}\n",
    "        temp_model = model.to('cpu')\n",
    "        clean_hidden_states = extract_hidden_states(\n",
    "            temp_model, cpu_tokens, num_layers=num_layers\n",
    "        )\n",
    "        # Move model back to original device\n",
    "        model.to(device)\n",
    "    \n",
    "    # Clear CUDA cache between operations if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Extract hidden states for corrupted prompt\n",
    "    try:\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                corrupted_hidden_states = extract_hidden_states(\n",
    "                    model, corrupted_tokens_device, num_layers=num_layers\n",
    "                )\n",
    "                torch.cuda.synchronize()\n",
    "        else:\n",
    "            corrupted_hidden_states = extract_hidden_states(\n",
    "                model, corrupted_tokens_device, num_layers=num_layers\n",
    "            )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during corrupted prompt processing: {str(e)}. Trying CPU...\")\n",
    "        # Move to CPU for this operation\n",
    "        cpu_tokens = {k: v.to('cpu') for k, v in corrupted_tokens.items()}\n",
    "        temp_model = model.to('cpu')\n",
    "        corrupted_hidden_states = extract_hidden_states(\n",
    "            temp_model, cpu_tokens, num_layers=num_layers\n",
    "        )\n",
    "        # Move model back to original device\n",
    "        model.to(device)\n",
    "    \n",
    "    # Final CUDA cache cleanup\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Initial passes complete.\")\n",
    "    return clean_hidden_states, corrupted_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, tokens, max_new_tokens=100):\n",
    "    \"\"\"Generate text from input tokens\"\"\"\n",
    "    # Convert tokens to dict if it's not already (for compatibility)\n",
    "    if not isinstance(tokens, dict):\n",
    "        tokens = {'input_ids': tokens.input_ids, 'attention_mask': tokens.attention_mask}\n",
    "    \n",
    "    # Use the global DEVICE variable\n",
    "    device = DEVICE\n",
    "    \n",
    "    # Ensure tokens are on the correct device\n",
    "    device_tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Use CUDA streams for better GPU utilization if available\n",
    "            if device.type == 'cuda':\n",
    "                # Clear CUDA cache to prevent OOM errors\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                    output_ids = model.generate(\n",
    "                        input_ids=device_tokens['input_ids'],\n",
    "                        attention_mask=device_tokens['attention_mask'],\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                    # Ensure CUDA operations are completed\n",
    "                    torch.cuda.synchronize()\n",
    "            else:\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=device_tokens['input_ids'],\n",
    "                    attention_mask=device_tokens['attention_mask'],\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during generation: {str(e)}. Trying CPU...\")\n",
    "            # Save original device to move back later\n",
    "            original_device = next(model.parameters()).device\n",
    "            # Move model to CPU temporarily\n",
    "            model.to('cpu')\n",
    "            # Move inputs to CPU\n",
    "            cpu_tokens = {k: v.to('cpu') for k, v in tokens.items()}\n",
    "            \n",
    "            # Try generation on CPU\n",
    "            output_ids = model.generate(\n",
    "                input_ids=cpu_tokens['input_ids'],\n",
    "                attention_mask=cpu_tokens['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Move model back to original device\n",
    "            model.to(original_device)\n",
    "    \n",
    "    # Get the full output text\n",
    "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Get just the generated text (excluding the input)\n",
    "    input_length = tokens['input_ids'].shape[1]\n",
    "    gen_ids = output_ids[0, input_length:]\n",
    "    generated_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Clear CUDA cache after processing if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Return all information\n",
    "    return {\n",
    "        \"full_output\": full_output,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"input_ids\": tokens['input_ids'][0].tolist(),\n",
    "        \"generated_ids\": gen_ids.tolist(),\n",
    "        \"all_tokens\": output_ids[0].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseline_outputs(model, tokenizer, clean_tokens, corrupted_tokens, max_new_tokens=100):\n",
    "    \"\"\"Generate baseline outputs for both clean and corrupted prompts\"\"\"\n",
    "    print(\"Generating baseline outputs...\")\n",
    "    \n",
    "    # Use the global DEVICE variable\n",
    "    device = DEVICE\n",
    "    \n",
    "    # Ensure inputs are on the correct device\n",
    "    clean_tokens_device = {k: v.to(device) for k, v in clean_tokens.items()}\n",
    "    corrupted_tokens_device = {k: v.to(device) for k, v in corrupted_tokens.items()}\n",
    "    \n",
    "    # Clear CUDA cache if using CUDA to prevent OOM errors\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Generate from clean prompt\n",
    "    try:\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                clean_generation = generate_text(model, tokenizer, clean_tokens_device, max_new_tokens)\n",
    "                torch.cuda.synchronize()\n",
    "        else:\n",
    "            clean_generation = generate_text(model, tokenizer, clean_tokens_device, max_new_tokens)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during clean generation: {str(e)}. Trying CPU...\")\n",
    "        # Move to CPU and try again\n",
    "        cpu_tokens = {k: v.to('cpu') for k, v in clean_tokens.items()}\n",
    "        clean_generation = generate_text(model, tokenizer, cpu_tokens, max_new_tokens)\n",
    "    \n",
    "    print(f\"Clean output: {clean_generation['generated_text']}\")\n",
    "    \n",
    "    # Clear CUDA cache between generations if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Generate from corrupted prompt\n",
    "    try:\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                corrupted_generation = generate_text(model, tokenizer, corrupted_tokens_device, max_new_tokens)\n",
    "                torch.cuda.synchronize()\n",
    "        else:\n",
    "            corrupted_generation = generate_text(model, tokenizer, corrupted_tokens_device, max_new_tokens)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during corrupted generation: {str(e)}. Trying CPU...\")\n",
    "        # Move to CPU and try again\n",
    "        cpu_tokens = {k: v.to('cpu') for k, v in corrupted_tokens.items()}\n",
    "        corrupted_generation = generate_text(model, tokenizer, cpu_tokens, max_new_tokens)\n",
    "    \n",
    "    print(f\"Corrupted output: {corrupted_generation['generated_text']}\")\n",
    "    \n",
    "    # Clear CUDA cache after processing if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return clean_generation, corrupted_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation_phase_patching(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    clean_tokens,\n",
    "    corrupted_hidden_states,\n",
    "    layers_to_patch=None,\n",
    "    max_tokens_to_generate=20,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run generation-phase patching as described in the methodology.\n",
    "    \n",
    "    For each token position in the generation:\n",
    "    1. Generate the token normally for the clean prompt\n",
    "    2. For each layer, patch the hidden states from the corrupted run\n",
    "    3. See if this changes the output token\n",
    "    4. Continue generation with the normal token (not the patched one)\n",
    "    5. Repeat for each token position\n",
    "    \"\"\"\n",
    "    print(\"Running generation-phase patching...\")\n",
    "    \n",
    "    # Use the global DEVICE variable if no device is specified\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "    \n",
    "    # Determine which layers to patch\n",
    "    if layers_to_patch is None:\n",
    "        layers_to_patch = list(range(len(model.model.layers)))\n",
    "    \n",
    "    # Start with the clean prompt tokens\n",
    "    current_input_ids = clean_tokens['input_ids'].clone().to(device)\n",
    "    \n",
    "    # Track the clean generation sequence\n",
    "    clean_generation_ids = []\n",
    "    \n",
    "    # Track patching results for each token position\n",
    "    token_patching_results = []\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for token_idx in tqdm(range(max_tokens_to_generate)):\n",
    "        # Current input length\n",
    "        curr_length = current_input_ids.shape[1]\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones((1, curr_length), device=device)\n",
    "        \n",
    "        # Get the next token from clean generation (without patching)\n",
    "        with torch.no_grad():\n",
    "            # Use CUDA streams for better GPU utilization if available\n",
    "            if device.type == 'cuda':\n",
    "                with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                    clean_outputs = model(\n",
    "                        input_ids=current_input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                    )\n",
    "                    # Ensure CUDA operations are completed\n",
    "                    torch.cuda.synchronize()\n",
    "            else:\n",
    "                clean_outputs = model(\n",
    "                    input_ids=current_input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "            clean_next_token_logits = clean_outputs.logits[0, -1, :]\n",
    "            clean_next_token = torch.argmax(clean_next_token_logits, dim=-1).item()\n",
    "            \n",
    "        # Add this token to our clean generation tracking\n",
    "        clean_generation_ids.append(clean_next_token)\n",
    "        \n",
    "        # For each layer, run patching and check effect\n",
    "        layer_results = {}\n",
    "        \n",
    "        for layer_idx in layers_to_patch:\n",
    "            # Skip patching if we don't have corrupted hidden states for this layer\n",
    "            layer_key = f\"layer_{layer_idx}\"\n",
    "            if layer_key not in corrupted_hidden_states:\n",
    "                continue\n",
    "                \n",
    "            # Try patching at this layer\n",
    "            patched_token = patch_and_get_next_token(\n",
    "                model,\n",
    "                current_input_ids, \n",
    "                attention_mask,\n",
    "                layer_idx,\n",
    "                corrupted_hidden_states\n",
    "            )\n",
    "            \n",
    "            # Record results\n",
    "            layer_results[layer_idx] = {\n",
    "                \"clean_token\": clean_next_token,\n",
    "                \"clean_token_str\": tokenizer.decode(clean_next_token),\n",
    "                \"patched_token\": patched_token,\n",
    "                \"patched_token_str\": tokenizer.decode(patched_token),\n",
    "                \"changed\": patched_token != clean_next_token\n",
    "            }\n",
    "        \n",
    "        # Add results for this token position\n",
    "        token_patching_results.append({\n",
    "            \"token_position\": token_idx,\n",
    "            \"context\": tokenizer.decode(current_input_ids[0]),\n",
    "            \"clean_next_token\": clean_next_token,\n",
    "            \"clean_next_token_str\": tokenizer.decode(clean_next_token),\n",
    "            \"layer_results\": layer_results\n",
    "        })\n",
    "        \n",
    "        # Continue generation with the clean token (important: we're not using patched tokens)\n",
    "        current_input_ids = torch.cat([\n",
    "            current_input_ids, \n",
    "            torch.tensor([[clean_next_token]], device=device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Check if we've reached the end token\n",
    "        if clean_next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        # Clear CUDA cache if using CUDA to prevent OOM errors\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save final clean generation\n",
    "    clean_gen_text = tokenizer.decode(\n",
    "        current_input_ids[0, clean_tokens['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Return patching results\n",
    "    patching_results = {\n",
    "        \"token_results\": token_patching_results,\n",
    "        \"clean_generation_ids\": clean_generation_ids,\n",
    "        \"clean_generation_text\": clean_gen_text,\n",
    "        \"layers_patched\": layers_to_patch\n",
    "    }\n",
    "    \n",
    "    # Clear CUDA cache after processing if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(\"Generation-phase patching complete.\")\n",
    "    return patching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_patching_results(patching_results):\n",
    "    \"\"\"Analyze patching results to identify important layers for reasoning\"\"\"\n",
    "    if not patching_results:\n",
    "        print(\"No patching results found.\")\n",
    "        return None\n",
    "    \n",
    "    # Get device from global variable\n",
    "    device = DEVICE\n",
    "    \n",
    "    # Track which layers change tokens at which positions\n",
    "    token_changes = {layer: [] for layer in patching_results[\"layers_patched\"]}\n",
    "    \n",
    "    # Analyze each token position\n",
    "    for token_result in patching_results[\"token_results\"]:\n",
    "        position = token_result[\"token_position\"]\n",
    "        \n",
    "        for layer_idx, layer_result in token_result[\"layer_results\"].items():\n",
    "            if layer_result[\"changed\"]:\n",
    "                # Record that this layer changed this token\n",
    "                token_changes[layer_idx].append({\n",
    "                    \"position\": position,\n",
    "                    \"clean_token\": layer_result[\"clean_token_str\"],\n",
    "                    \"patched_token\": layer_result[\"patched_token_str\"],\n",
    "                    \"context\": token_result[\"context\"]\n",
    "                })\n",
    "    \n",
    "    # Process data efficiently based on device\n",
    "    if device.type == 'cuda':\n",
    "        # Clear CUDA cache before processing to free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Use CUDA streams for better GPU utilization\n",
    "        with torch.cuda.stream(torch.cuda.Stream()):\n",
    "            # Find layers with the most impact\n",
    "            layer_impact = {layer: len(changes) for layer, changes in token_changes.items()}\n",
    "            most_impactful_layers = sorted(\n",
    "                layer_impact.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Find token positions with the most changes\n",
    "            position_changes = {}\n",
    "            for token_result in patching_results[\"token_results\"]:\n",
    "                position = token_result[\"token_position\"]\n",
    "                changes = sum(1 for layer_result in token_result[\"layer_results\"].values() if layer_result[\"changed\"])\n",
    "                position_changes[position] = changes\n",
    "            \n",
    "            most_affected_positions = sorted(\n",
    "                position_changes.items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Ensure CUDA operations are completed\n",
    "            torch.cuda.synchronize()\n",
    "    else:\n",
    "        # CPU processing path\n",
    "        # Find layers with the most impact\n",
    "        layer_impact = {layer: len(changes) for layer, changes in token_changes.items()}\n",
    "        most_impactful_layers = sorted(\n",
    "            layer_impact.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Find token positions with the most changes\n",
    "        position_changes = {}\n",
    "        for token_result in patching_results[\"token_results\"]:\n",
    "            position = token_result[\"token_position\"]\n",
    "            changes = sum(1 for layer_result in token_result[\"layer_results\"].values() if layer_result[\"changed\"])\n",
    "            position_changes[position] = changes\n",
    "        \n",
    "        most_affected_positions = sorted(\n",
    "            position_changes.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "    \n",
    "    # Compile analysis\n",
    "    analysis = {\n",
    "        \"token_changes\": token_changes,\n",
    "        \"layer_impact\": layer_impact,\n",
    "        \"most_impactful_layers\": most_impactful_layers,\n",
    "        \"position_changes\": position_changes,\n",
    "        \"most_affected_positions\": most_affected_positions,\n",
    "    }\n",
    "    \n",
    "    # Clear CUDA cache after processing if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_impact(analysis_results, save_path=None):\n",
    "    \"\"\"Visualize the impact of each layer on token generation using Plotly\"\"\"\n",
    "    if not analysis_results:\n",
    "        print(\"No analysis results found.\")\n",
    "        return\n",
    "        \n",
    "    # Extract data for plotting\n",
    "    layers = list(analysis_results[\"layer_impact\"].keys())\n",
    "    impact_values = list(analysis_results[\"layer_impact\"].values())\n",
    "    \n",
    "    # Find most impactful layer\n",
    "    most_impactful = None\n",
    "    most_impactful_value = 0\n",
    "    if impact_values and max(impact_values) > 0:\n",
    "        most_impactful = analysis_results[\"most_impactful_layers\"][0][0]\n",
    "        most_impactful_value = analysis_results[\"layer_impact\"][most_impactful]\n",
    "    \n",
    "    # Create colors list with the most impactful layer highlighted\n",
    "    colors = ['rgba(0, 0, 255, 0.6)' if layer != most_impactful else 'rgba(255, 0, 0, 0.8)' \n",
    "              for layer in layers]\n",
    "    \n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar chart\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=layers,\n",
    "        y=impact_values,\n",
    "        marker_color=colors,\n",
    "        text=impact_values,\n",
    "        textposition='outside',\n",
    "        hovertemplate='Layer %{x}<br>Changed %{y} tokens<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Add annotation for most impactful layer\n",
    "    if most_impactful is not None:\n",
    "        fig.add_annotation(\n",
    "            x=most_impactful,\n",
    "            y=most_impactful_value + 0.5,\n",
    "            text=f\"Most impactful: Layer {most_impactful}\",\n",
    "            showarrow=True,\n",
    "            arrowhead=1,\n",
    "            font=dict(size=14)\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Impact of Layer Patching on Token Generation',\n",
    "        xaxis_title='Model Layer',\n",
    "        yaxis_title='Number of Tokens Changed',\n",
    "        template='plotly_white',\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=layers\n",
    "        ),\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=12\n",
    "        ),\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        fig.write_image(save_path)\n",
    "        fig.write_html(save_path.replace(\".png\", \".html\"))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_token_position_impact(patching_results, analysis_results, tokenizer, save_path=None):\n",
    "    \"\"\"Visualize which token positions are most affected by patching using Plotly\"\"\"\n",
    "    if not analysis_results:\n",
    "        print(\"No analysis results found.\")\n",
    "        return\n",
    "        \n",
    "    # Extract data for plotting\n",
    "    positions = list(analysis_results[\"position_changes\"].keys())\n",
    "    impact_values = list(analysis_results[\"position_changes\"].values())\n",
    "    \n",
    "    # Get token strings\n",
    "    token_strings = []\n",
    "    hover_texts = []\n",
    "    for pos in positions:\n",
    "        if pos < len(patching_results[\"token_results\"]):\n",
    "            token = patching_results[\"token_results\"][pos][\"clean_next_token_str\"]\n",
    "            token_strings.append(f\"{pos}: '{token}'\")\n",
    "            # Add context for richer hover information\n",
    "            context = patching_results[\"token_results\"][pos][\"context\"]\n",
    "            if len(context) > 50:\n",
    "                context = context[-50:] + \"...\"\n",
    "            hover_texts.append(f\"Position: {pos}<br>Token: '{token}'<br>Context: {context}\")\n",
    "        else:\n",
    "            token_strings.append(str(pos))\n",
    "            hover_texts.append(f\"Position: {pos}\")\n",
    "    \n",
    "    # Find most affected position\n",
    "    colors = ['rgba(0, 128, 0, 0.6)'] * len(positions)\n",
    "    if impact_values and max(impact_values) > 0:\n",
    "        most_affected_idx = impact_values.index(max(impact_values))\n",
    "        colors[most_affected_idx] = 'rgba(128, 0, 128, 0.8)'\n",
    "    \n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar chart\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=token_strings,\n",
    "        y=impact_values,\n",
    "        marker_color=colors,\n",
    "        text=impact_values,\n",
    "        textposition='outside',\n",
    "        hovertext=hover_texts,\n",
    "        hovertemplate='%{hovertext}<br>Changed by %{y} layers<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Token Positions Most Affected by Layer Patching',\n",
    "        xaxis_title='Token Position and Value',\n",
    "        yaxis_title='Number of Layers Causing Change',\n",
    "        template='plotly_white',\n",
    "        xaxis=dict(\n",
    "            tickangle=45\n",
    "        ),\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=12\n",
    "        ),\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        fig.write_image(save_path)\n",
    "        fig.write_html(save_path.replace(\".png\", \".html\"))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_heatmap(patching_results, save_path=None):\n",
    "    \"\"\"Create a heatmap showing which layers affect which token positions using Plotly\"\"\"\n",
    "    if not patching_results:\n",
    "        print(\"No patching results found.\")\n",
    "        return\n",
    "    \n",
    "    # Get token results\n",
    "    token_results = patching_results[\"token_results\"]\n",
    "    \n",
    "    # Create layers x tokens matrix\n",
    "    layers = sorted(patching_results[\"layers_patched\"])\n",
    "    tokens = list(range(len(token_results)))\n",
    "    \n",
    "    # Initialize heatmap data\n",
    "    heatmap_data = [[0 for _ in tokens] for _ in layers]\n",
    "    \n",
    "    # Fill the heatmap data\n",
    "    for t_idx, token_result in enumerate(token_results):\n",
    "        for l_idx, layer in enumerate(layers):\n",
    "            if layer in token_result[\"layer_results\"]:\n",
    "                if token_result[\"layer_results\"][layer][\"changed\"]:\n",
    "                    heatmap_data[l_idx][t_idx] = 1\n",
    "    \n",
    "    # Create token labels\n",
    "    token_labels = []\n",
    "    token_hover_texts = []\n",
    "    for t_idx, token_result in enumerate(token_results):\n",
    "        token_str = token_result[\"clean_next_token_str\"]\n",
    "        # Truncate long tokens\n",
    "        if len(token_str) > 10:\n",
    "            token_str = token_str[:8] + \"...\"\n",
    "        token_labels.append(f\"{t_idx}: '{token_str}'\")\n",
    "        \n",
    "        # Create hover text with token and some context\n",
    "        context = token_result[\"context\"]\n",
    "        if len(context) > 50:\n",
    "            context = \"...\" + context[-50:]\n",
    "        token_hover_texts.append(f\"Position: {t_idx}<br>Token: '{token_str}'<br>Context: {context}\")\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=heatmap_data,\n",
    "        x=token_labels,\n",
    "        y=[f\"Layer {l}\" for l in layers],\n",
    "        colorscale='YlOrRd',\n",
    "        showscale=True,\n",
    "        hoverongaps=False,\n",
    "        customdata=[[token_hover_texts[t_idx] for t_idx in range(len(tokens))] for _ in range(len(layers))],\n",
    "        hovertemplate='%{y}<br>%{customdata}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Heatmap of Layer Impact on Token Generation',\n",
    "        xaxis_title='Token Position and Value',\n",
    "        yaxis_title='Model Layer',\n",
    "        template='plotly_white',\n",
    "        xaxis=dict(\n",
    "            tickangle=45,\n",
    "            showgrid=False\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showgrid=False\n",
    "        ),\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=12\n",
    "        ),\n",
    "        height=700,\n",
    "        width=900\n",
    "    )\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        fig.write_image(save_path)\n",
    "        fig.write_html(save_path.replace(\".png\", \".html\"))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_report(patching_results, analysis_results, clean_prompt, corrupted_prompt, corrupted_generation=None):\n",
    "    \"\"\"Generate a detailed text report of the patching results\"\"\"\n",
    "    if not patching_results or not analysis_results:\n",
    "        print(\"Missing patching or analysis results.\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Start building the report\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"GENERATION-PHASE ACTIVATION PATCHING REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    # Baseline outputs\n",
    "    report.append(\"\\nBASELINE OUTPUTS:\")\n",
    "    report.append(f\"Clean prompt: {clean_prompt}\")\n",
    "    report.append(f\"Clean generation: {patching_results['clean_generation_text']}\")\n",
    "    report.append(f\"Corrupted prompt: {corrupted_prompt}\")\n",
    "    if corrupted_generation:\n",
    "        report.append(f\"Corrupted generation: {corrupted_generation['generated_text']}\")\n",
    "    \n",
    "    # Most impactful layers\n",
    "    report.append(\"\\nMOST IMPACTFUL LAYERS:\")\n",
    "    for layer, impact in analysis_results[\"most_impactful_layers\"][:5]:\n",
    "        if impact > 0:\n",
    "            report.append(f\"Layer {layer}: Changed {impact} tokens\")\n",
    "    \n",
    "    # Most affected token positions\n",
    "    report.append(\"\\nMOST AFFECTED TOKEN POSITIONS:\")\n",
    "    for pos, changes in analysis_results[\"most_affected_positions\"][:5]:\n",
    "        if changes > 0:\n",
    "            token = patching_results[\"token_results\"][pos][\"clean_next_token_str\"]\n",
    "            report.append(f\"Position {pos} ('{token}'): Changed by {changes} layers\")\n",
    "    \n",
    "    # Detailed token-by-token analysis\n",
    "    report.append(\"\\nDETAILED TOKEN-BY-TOKEN ANALYSIS:\")\n",
    "    for token_result in patching_results[\"token_results\"]:\n",
    "        pos = token_result[\"token_position\"]\n",
    "        clean_token = token_result[\"clean_next_token_str\"]\n",
    "        \n",
    "        # Only show interesting positions (where something changed)\n",
    "        changes = [layer for layer, result in token_result[\"layer_results\"].items() \n",
    "                  if result[\"changed\"]]\n",
    "        \n",
    "        if changes:\n",
    "            report.append(f\"\\nToken position {pos} (Clean: '{clean_token}'):\")\n",
    "            context = token_result[\"context\"]\n",
    "            report.append(f\"Context: {context}\")\n",
    "            \n",
    "            for layer in sorted(changes):\n",
    "                result = token_result[\"layer_results\"][layer]\n",
    "                report.append(f\"  Layer {layer}: '{result['clean_token_str']}' → '{result['patched_token_str']}'\")\n",
    "    \n",
    "    return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patched_output(model, tokenizer, clean_tokens, corrupted_hidden_states, layer_to_patch, max_new_tokens=50, device=None):\n",
    "    \"\"\"\n",
    "    Generate a complete output sequence with patching applied at the specified layer\n",
    "    for all generation steps. This allows seeing how patching affects the entire generation.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        clean_tokens: Tokenized clean prompt\n",
    "        corrupted_hidden_states: Hidden states from the corrupted prompt\n",
    "        layer_to_patch: Which layer to patch (typically the most impactful layer)\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with generation results\n",
    "    \"\"\"\n",
    "    # Use the global DEVICE variable if no device is specified\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "    \n",
    "    print(f\"Generating complete output with patching at layer {layer_to_patch}...\")\n",
    "    \n",
    "    # Start with the clean prompt tokens\n",
    "    current_input_ids = clean_tokens['input_ids'].clone().to(device)\n",
    "    \n",
    "    # Track the patched generation\n",
    "    patched_generation_ids = []\n",
    "    \n",
    "    # Clear CUDA cache if using CUDA to prevent OOM errors\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Generate tokens one by one with continuous patching\n",
    "    for _ in tqdm(range(max_new_tokens)):\n",
    "        # Current input length\n",
    "        curr_length = current_input_ids.shape[1]\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones((1, curr_length), device=device)\n",
    "        \n",
    "        try:\n",
    "            # Get the next token with patching at the specified layer\n",
    "            # Use CUDA streams for better GPU utilization if available\n",
    "            if device.type == 'cuda':\n",
    "                with torch.cuda.stream(torch.cuda.Stream()):\n",
    "                    patched_token = patch_and_get_next_token(\n",
    "                        model,\n",
    "                        current_input_ids,\n",
    "                        attention_mask,\n",
    "                        layer_to_patch,\n",
    "                        corrupted_hidden_states\n",
    "                    )\n",
    "                    # Ensure CUDA operations are completed\n",
    "                    torch.cuda.synchronize()\n",
    "            else:\n",
    "                patched_token = patch_and_get_next_token(\n",
    "                    model,\n",
    "                    current_input_ids,\n",
    "                    attention_mask,\n",
    "                    layer_to_patch,\n",
    "                    corrupted_hidden_states\n",
    "                )\n",
    "        except RuntimeError as e:\n",
    "            # If any error occurs, try CPU\n",
    "            print(f\"Error during patching: {str(e)}. Trying CPU...\")\n",
    "            # Save original device to move back later\n",
    "            original_device = next(model.parameters()).device\n",
    "            # Move everything to CPU\n",
    "            model = model.to('cpu')\n",
    "            current_input_ids = current_input_ids.to('cpu')\n",
    "            attention_mask = attention_mask.to('cpu')\n",
    "            \n",
    "            patched_token = patch_and_get_next_token(\n",
    "                model,\n",
    "                current_input_ids,\n",
    "                attention_mask,\n",
    "                layer_to_patch,\n",
    "                corrupted_hidden_states\n",
    "            )\n",
    "            \n",
    "            # Move model back\n",
    "            model.to(device)\n",
    "            current_input_ids = current_input_ids.to(device)\n",
    "        \n",
    "        # Add this token to our patched generation tracking\n",
    "        patched_generation_ids.append(patched_token)\n",
    "        \n",
    "        # Importantly, use the patched token for the next generation step\n",
    "        current_input_ids = torch.cat([\n",
    "            current_input_ids, \n",
    "            torch.tensor([[patched_token]], device=device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Check if we've reached the end token\n",
    "        if patched_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        # Periodically clear CUDA cache during long generations if using CUDA\n",
    "        if device.type == 'cuda' and _ % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Extract the generated text (without the prompt)\n",
    "    patched_generation_text = tokenizer.decode(\n",
    "        current_input_ids[0, clean_tokens['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Final CUDA cache clear\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Return generation results\n",
    "    return {\n",
    "        \"patched_generation_ids\": patched_generation_ids,\n",
    "        \"patched_generation_text\": patched_generation_text,\n",
    "        \"layer_patched\": layer_to_patch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concise_insights(patching_results, analysis_results, corrupted_generation=None):\n",
    "    \"\"\"Generate concise insights about the most important findings\"\"\"\n",
    "    if not patching_results or not analysis_results:\n",
    "        print(\"Missing patching or analysis results.\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Start building insights\n",
    "    insights = []\n",
    "    insights.append(\"KEY INSIGHTS FROM ACTIVATION PATCHING:\")\n",
    "    \n",
    "    # Check if we had any impact\n",
    "    total_changes = sum(analysis_results[\"layer_impact\"].values())\n",
    "    if total_changes == 0:\n",
    "        insights.append(\"No token changes observed during patching. The model's reasoning path appears robust to patching in these layers.\")\n",
    "        return \"\\n\".join(insights)\n",
    "    \n",
    "    # Find critical layers\n",
    "    critical_layers = [layer for layer, impact in analysis_results[\"most_impactful_layers\"] if impact > 0][:3]\n",
    "    if critical_layers:\n",
    "        layers_str = \", \".join([f\"Layer {l}\" for l in critical_layers])\n",
    "        insights.append(f\"Critical layers for reasoning: {layers_str}\")\n",
    "    \n",
    "    # Find critical reasoning steps\n",
    "    critical_positions = [pos for pos, changes in analysis_results[\"most_affected_positions\"] if changes > 0][:3]\n",
    "    if critical_positions:\n",
    "        tokens = []\n",
    "        for pos in critical_positions:\n",
    "            if pos < len(patching_results[\"token_results\"]):\n",
    "                token = patching_results[\"token_results\"][pos][\"clean_next_token_str\"]\n",
    "                tokens.append(f\"'{token}' (pos {pos})\")\n",
    "        \n",
    "        tokens_str = \", \".join(tokens)\n",
    "        insights.append(f\"Critical reasoning tokens: {tokens_str}\")\n",
    "    \n",
    "    # Check if any early tokens were affected\n",
    "    early_affected = [pos for pos in critical_positions if pos < 3]\n",
    "    if early_affected:\n",
    "        insights.append(\"Early tokens in the generation were affected, suggesting the model's reasoning path diverges early.\")\n",
    "    \n",
    "    # Check if patching created token sequences similar to corrupted output\n",
    "    if corrupted_generation:\n",
    "        corrupt_text = corrupted_generation[\"generated_text\"]\n",
    "        patched_sequences = []\n",
    "        \n",
    "        for token_result in patching_results[\"token_results\"]:\n",
    "            for layer, result in token_result[\"layer_results\"].items():\n",
    "                if result[\"changed\"] and result[\"patched_token_str\"] in corrupt_text:\n",
    "                    patched_sequences.append(f\"Layer {layer} at position {token_result['token_position']}\")\n",
    "                    break\n",
    "        \n",
    "        if patched_sequences:\n",
    "            insights.append(f\"Patching produced tokens matching the corrupted output at: {', '.join(patched_sequences[:3])}\")\n",
    "    \n",
    "    return \"\\n\".join(insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generation_comparison(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a visual comparison of the three different generations\n",
    "    (clean, corrupted, and patched) using Plotly - simplified version\n",
    "    that doesn't use make_subplots\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    if \"patched_generation\" not in results:\n",
    "        print(\"No patched generation available for comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Extract text for comparison\n",
    "    clean_text = results[\"clean_generation\"][\"generated_text\"]\n",
    "    corrupted_text = results[\"corrupted_generation\"][\"generated_text\"]\n",
    "    patched_text = results[\"patched_generation\"][\"patched_generation_text\"]\n",
    "    patched_layer = results[\"patched_generation\"][\"layer_patched\"]\n",
    "    \n",
    "    # Create a single figure with all outputs as separate annotations\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add text annotations for each output\n",
    "    fig.add_annotation(\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.5, y=0.95,\n",
    "        text=\"<b>Clean Output (No Hint)</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=16),\n",
    "        align=\"center\",\n",
    "        bgcolor=\"rgba(0,0,0,0)\",\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.01, y=0.85,\n",
    "        text=clean_text,\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        align=\"left\",\n",
    "        bgcolor=\"rgba(240, 240, 240, 0.7)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        borderpad=10,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.5, y=0.65,\n",
    "        text=f\"<b>Corrupted Output (With Hint)</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=16),\n",
    "        align=\"center\",\n",
    "        bgcolor=\"rgba(0,0,0,0)\",\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.01, y=0.55,\n",
    "        text=corrupted_text,\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        align=\"left\",\n",
    "        bgcolor=\"rgba(255, 230, 230, 0.7)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        borderpad=10,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.5, y=0.35,\n",
    "        text=f\"<b>Patched Output (Layer {patched_layer})</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=16),\n",
    "        align=\"center\",\n",
    "        bgcolor=\"rgba(0,0,0,0)\",\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.01, y=0.25,\n",
    "        text=patched_text,\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        align=\"left\",\n",
    "        bgcolor=\"rgba(230, 255, 230, 0.7)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        borderpad=10,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Comparison of Generations\",\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        margin=dict(t=50, b=50, l=50, r=50)\n",
    "    )\n",
    "    \n",
    "    # Remove axes\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        fig.write_image(save_path)\n",
    "        fig.write_html(save_path.replace(\".png\", \".html\"))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_results(results, save_dir=None):\n",
    "    \"\"\"\n",
    "    Create all visualizations in one go\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from run_generation_phase_patching_analysis\n",
    "        save_dir: Directory to save visualizations (optional)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Create save directory if needed\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Prepare save paths\n",
    "    layer_impact_path = None\n",
    "    token_impact_path = None\n",
    "    heatmap_path = None\n",
    "    comparison_path = None\n",
    "    \n",
    "    if save_dir:\n",
    "        layer_impact_path = os.path.join(save_dir, \"layer_impact.png\")\n",
    "        token_impact_path = os.path.join(save_dir, \"token_impact.png\")\n",
    "        heatmap_path = os.path.join(save_dir, \"heatmap.png\")\n",
    "        comparison_path = os.path.join(save_dir, \"generation_comparison.png\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    visualize_layer_impact(\n",
    "        results[\"analysis_results\"], \n",
    "        save_path=layer_impact_path\n",
    "    )\n",
    "    \n",
    "    visualize_token_position_impact(\n",
    "        results[\"patching_results\"],\n",
    "        results[\"analysis_results\"], \n",
    "        results.get(\"tokenizer\", None),  # Pass tokenizer if available\n",
    "        save_path=token_impact_path\n",
    "    )\n",
    "    \n",
    "    visualize_heatmap(\n",
    "        results[\"patching_results\"], \n",
    "        save_path=heatmap_path\n",
    "    )\n",
    "    \n",
    "    if \"patched_generation\" in results:\n",
    "        visualize_generation_comparison(\n",
    "            results, \n",
    "            save_path=comparison_path\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation_phase_patching_analysis(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    clean_prompt,\n",
    "    corrupted_prompt,\n",
    "    layers_to_patch=None,\n",
    "    max_tokens_to_generate=20,\n",
    "    max_new_tokens=100,\n",
    "    device=None,\n",
    "    save_visualizations=False,\n",
    "    generate_complete_patched=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a complete generation-phase patching analysis.\n",
    "    \n",
    "    This function combines all the above functions into a single workflow.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        clean_prompt: The base prompt without additional information\n",
    "        corrupted_prompt: The prompt with additional information (e.g., a hint)\n",
    "        layers_to_patch: List of layer indices to patch (default: all layers)\n",
    "        max_tokens_to_generate: Maximum number of tokens to analyze with patching\n",
    "        max_new_tokens: Maximum tokens to generate for baseline outputs\n",
    "        device: Device to run on (if None, will use get_device())\n",
    "        save_visualizations: Whether to save visualization files\n",
    "        visualization_prefix: Prefix for saved visualization files\n",
    "        generate_complete_patched: Whether to generate a complete patched output\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all results\n",
    "    \"\"\"\n",
    "    # Import visualization functions\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Use the global DEVICE variable if device is None\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Tokenize prompts\n",
    "    clean_tokens, corrupted_tokens, formatted_clean, formatted_corrupted = tokenize_prompts(\n",
    "        clean_prompt, corrupted_prompt, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    # 2. Generate baseline outputs\n",
    "    clean_generation, corrupted_generation = generate_baseline_outputs(\n",
    "        model, tokenizer, clean_tokens, corrupted_tokens, max_new_tokens\n",
    "    )\n",
    "    \n",
    "    # 3. Run initial passes to extract hidden states\n",
    "    # Clear CUDA cache before extracting hidden states if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    clean_hidden_states, corrupted_hidden_states = run_initial_passes(\n",
    "        model, clean_tokens, corrupted_tokens\n",
    "    )\n",
    "    \n",
    "    # 4. Run generation-phase patching\n",
    "    # Clear CUDA cache again before patching if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    patching_results = run_generation_phase_patching(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        clean_tokens, \n",
    "        corrupted_hidden_states, \n",
    "        layers_to_patch=layers_to_patch,\n",
    "        max_tokens_to_generate=max_tokens_to_generate,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 5. Analyze results\n",
    "    analysis_results = analyze_patching_results(patching_results)\n",
    "    \n",
    "    # 6. Generate complete patched output using the most impactful layer\n",
    "    patched_generation = None\n",
    "    if generate_complete_patched and analysis_results[\"most_impactful_layers\"]:\n",
    "        # Find the most impactful layer\n",
    "        most_impactful_layer = analysis_results[\"most_impactful_layers\"][0][0]\n",
    "        \n",
    "        # Clear CUDA cache before generating patched output if using CUDA\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Generate complete output with patching at this layer\n",
    "        patched_generation = generate_patched_output(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            clean_tokens,\n",
    "            corrupted_hidden_states,\n",
    "            most_impactful_layer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    # 7. Visualize results with Plotly\n",
    "    if save_visualizations:\n",
    "        save_dir = None\n",
    "        if isinstance(save_visualizations, str):\n",
    "            save_dir = save_visualizations\n",
    "        else:\n",
    "            save_dir = \"patching_visualizations\"\n",
    "            \n",
    "        visualize_all_results({\n",
    "            \"analysis_results\": analysis_results,\n",
    "            \"patching_results\": patching_results,\n",
    "            \"clean_generation\": clean_generation,\n",
    "            \"corrupted_generation\": corrupted_generation,\n",
    "            \"patched_generation\": patched_generation if patched_generation else None,\n",
    "            \"tokenizer\": tokenizer\n",
    "        }, save_dir=save_dir)\n",
    "    else:\n",
    "        # Just display the visualizations without saving\n",
    "        visualize_layer_impact(analysis_results)\n",
    "        visualize_token_position_impact(patching_results, analysis_results, tokenizer)\n",
    "        visualize_heatmap(patching_results)\n",
    "        if patched_generation:\n",
    "            visualize_generation_comparison({\n",
    "                \"clean_generation\": clean_generation,\n",
    "                \"corrupted_generation\": corrupted_generation,\n",
    "                \"patched_generation\": patched_generation\n",
    "            })\n",
    "    \n",
    "    # 8. Generate reports\n",
    "    detailed_report = generate_detailed_report(\n",
    "        patching_results, \n",
    "        analysis_results, \n",
    "        formatted_clean, \n",
    "        formatted_corrupted, \n",
    "        corrupted_generation\n",
    "    )\n",
    "    concise_insights = generate_concise_insights(\n",
    "        patching_results,\n",
    "        analysis_results,\n",
    "        corrupted_generation\n",
    "    )\n",
    "    \n",
    "    # Add information about the patched generation\n",
    "    if patched_generation:\n",
    "        detailed_report += \"\\n\\n\" + \"=\" * 80\n",
    "        detailed_report += f\"\\nFULL PATCHED GENERATION (Layer {patched_generation['layer_patched']}):\"\n",
    "        detailed_report += f\"\\n{patched_generation['patched_generation_text']}\"\n",
    "        \n",
    "        # Compare the three outputs\n",
    "        detailed_report += \"\\n\\n\" + \"=\" * 80\n",
    "        detailed_report += \"\\nCOMPARISON OF ALL THREE GENERATIONS:\"\n",
    "        detailed_report += f\"\\n1. Clean prompt: {clean_generation['generated_text']}\"\n",
    "        detailed_report += f\"\\n2. Corrupted prompt: {corrupted_generation['generated_text']}\"\n",
    "        detailed_report += f\"\\n3. Patched at layer {patched_generation['layer_patched']}: {patched_generation['patched_generation_text']}\"\n",
    "    \n",
    "    print(\"\\n\" + detailed_report)\n",
    "    print(\"\\n\" + concise_insights)\n",
    "    \n",
    "    # Return all results\n",
    "    results = {\n",
    "        \"clean_prompt\": formatted_clean,\n",
    "        \"corrupted_prompt\": formatted_corrupted,\n",
    "        \"clean_generation\": clean_generation,\n",
    "        \"corrupted_generation\": corrupted_generation,\n",
    "        \"patching_results\": patching_results,\n",
    "        \"analysis_results\": analysis_results,\n",
    "        \"detailed_report\": detailed_report,\n",
    "        \"concise_insights\": concise_insights\n",
    "    }\n",
    "    \n",
    "    # Add patched generation if available\n",
    "    if patched_generation:\n",
    "        results[\"patched_generation\"] = patched_generation\n",
    "    \n",
    "    # Final CUDA cache cleanup if using CUDA\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_patching_analysis(model, tokenizer):\n",
    "    \"\"\"Example demonstrating how to use the generation-phase patching framework\"\"\"\n",
    "    \n",
    "    device = DEVICE\n",
    "    # Define clean and corrupted prompts\n",
    "    clean_prompt = \"Solve this equation: 3x^2 - 12x + 9 = 0\"\n",
    "    corrupted_prompt = \"Solve this equation: 3x^2 - 12x + 9 = 0\\n\\nHint: Quadratic Formula\"\n",
    "    \n",
    "    # Run the full analysis\n",
    "    results = run_generation_phase_patching_analysis(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        clean_prompt=clean_prompt,\n",
    "        corrupted_prompt=corrupted_prompt,\n",
    "        layers_to_patch=[2, 30],  \n",
    "        max_tokens_to_generate=20,  # Reduced for faster testing\n",
    "        max_new_tokens=30,         # For the full generations\n",
    "        device=device,\n",
    "        save_visualizations=\"patching_results\",  # Save to this directory\n",
    "        generate_complete_patched=True  # Generate the complete patched output\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Clean prompt: <｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "\n",
      "Corrupted prompt: <｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0\n",
      "\n",
      "Hint: Quadratic Formula<｜Assistant｜><think>\n",
      "\n",
      "Generating baseline outputs...\n",
      "Clean output: To solve the quadratic equation \\(3x^2 - 12x + 9 = 0\\), I'll start by identifying the coefficients of\n",
      "Corrupted output: To solve the quadratic equation \\( 3x^2 - 12x + 9 = 0 \\), I'll start by identifying the coefficients\n",
      "Running initial forward passes...\n",
      "Initial passes complete.\n",
      "Running generation-phase patching...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e7fce0ce5548989c46fad740f4a5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation-phase patching complete.\n",
      "Generating complete output with patching at layer 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f94ee0f6704fb19d54b1c81edee2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Layer %{x}<br>Changed %{y} tokens<extra></extra>",
         "marker": {
          "color": [
           "rgba(255, 0, 0, 0.8)",
           "rgba(0, 0, 255, 0.6)"
          ]
         },
         "text": [
          "8",
          "8"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          2,
          30
         ],
         "y": [
          8,
          8
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "arrowhead": 1,
          "font": {
           "size": 14
          },
          "showarrow": true,
          "text": "Most impactful: Layer 2",
          "x": 2,
          "y": 8.5
         }
        ],
        "height": 600,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "size": 12
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Impact of Layer Patching on Token Generation"
        },
        "xaxis": {
         "tickmode": "array",
         "tickvals": [
          2,
          30
         ],
         "title": {
          "text": "Model Layer"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Tokens Changed"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "%{hovertext}<br>Changed by %{y} layers<extra></extra>",
         "hovertext": [
          "Position: 0<br>Token: 'I'<br>Context:  equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n...",
          "Position: 1<br>Token: ' need'<br>Context: equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI...",
          "Position: 2<br>Token: ' to'<br>Context: ion: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI need...",
          "Position: 3<br>Token: ' solve'<br>Context: : 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI need to...",
          "Position: 4<br>Token: ' the'<br>Context:  - 12x + 9 = 0<｜Assistant｜><think>\nI need to solve...",
          "Position: 5<br>Token: ' quadratic'<br>Context: 2x + 9 = 0<｜Assistant｜><think>\nI need to solve the...",
          "Position: 6<br>Token: ' equation'<br>Context: <｜Assistant｜><think>\nI need to solve the quadratic...",
          "Position: 7<br>Token: ' \\('<br>Context: nt｜><think>\nI need to solve the quadratic equation...",
          "Position: 8<br>Token: '3'<br>Context: ><think>\nI need to solve the quadratic equation \\(...",
          "Position: 9<br>Token: 'x'<br>Context: <think>\nI need to solve the quadratic equation \\(3...",
          "Position: 10<br>Token: '^'<br>Context: think>\nI need to solve the quadratic equation \\(3x...",
          "Position: 11<br>Token: '2'<br>Context: hink>\nI need to solve the quadratic equation \\(3x^...",
          "Position: 12<br>Token: ' -'<br>Context: ink>\nI need to solve the quadratic equation \\(3x^2...",
          "Position: 13<br>Token: ' '<br>Context: k>\nI need to solve the quadratic equation \\(3x^2 -...",
          "Position: 14<br>Token: '12'<br>Context: >\nI need to solve the quadratic equation \\(3x^2 - ...",
          "Position: 15<br>Token: 'x'<br>Context: I need to solve the quadratic equation \\(3x^2 - 12...",
          "Position: 16<br>Token: ' +'<br>Context:  need to solve the quadratic equation \\(3x^2 - 12x...",
          "Position: 17<br>Token: ' '<br>Context: eed to solve the quadratic equation \\(3x^2 - 12x +...",
          "Position: 18<br>Token: '9'<br>Context: ed to solve the quadratic equation \\(3x^2 - 12x + ...",
          "Position: 19<br>Token: ' ='<br>Context: d to solve the quadratic equation \\(3x^2 - 12x + 9..."
         ],
         "marker": {
          "color": [
           "rgba(128, 0, 128, 0.8)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)",
           "rgba(0, 128, 0, 0.6)"
          ]
         },
         "text": [
          "2",
          "2",
          "2",
          "2",
          "2",
          "2",
          "2",
          "2",
          "0",
          "0",
          "0",
          "0",
          "0",
          "0",
          "0",
          "0",
          "0",
          "0",
          "0",
          "0"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          "0: 'I'",
          "1: ' need'",
          "2: ' to'",
          "3: ' solve'",
          "4: ' the'",
          "5: ' quadratic'",
          "6: ' equation'",
          "7: ' \\('",
          "8: '3'",
          "9: 'x'",
          "10: '^'",
          "11: '2'",
          "12: ' -'",
          "13: ' '",
          "14: '12'",
          "15: 'x'",
          "16: ' +'",
          "17: ' '",
          "18: '9'",
          "19: ' ='"
         ],
         "y": [
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "size": 12
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Token Positions Most Affected by Layer Patching"
        },
        "xaxis": {
         "tickangle": 45,
         "title": {
          "text": "Token Position and Value"
         }
        },
        "yaxis": {
         "title": {
          "text": "Number of Layers Causing Change"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(255,255,204)"
          ],
          [
           0.125,
           "rgb(255,237,160)"
          ],
          [
           0.25,
           "rgb(254,217,118)"
          ],
          [
           0.375,
           "rgb(254,178,76)"
          ],
          [
           0.5,
           "rgb(253,141,60)"
          ],
          [
           0.625,
           "rgb(252,78,42)"
          ],
          [
           0.75,
           "rgb(227,26,28)"
          ],
          [
           0.875,
           "rgb(189,0,38)"
          ],
          [
           1,
           "rgb(128,0,38)"
          ]
         ],
         "customdata": [
          [
           "Position: 0<br>Token: 'I'<br>Context: ... equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
           "Position: 1<br>Token: ' need'<br>Context: ...equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI",
           "Position: 2<br>Token: ' to'<br>Context: ...ion: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI need",
           "Position: 3<br>Token: ' solve'<br>Context: ...: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI need to",
           "Position: 4<br>Token: ' the'<br>Context: ... - 12x + 9 = 0<｜Assistant｜><think>\nI need to solve",
           "Position: 5<br>Token: ' quadratic'<br>Context: ...2x + 9 = 0<｜Assistant｜><think>\nI need to solve the",
           "Position: 6<br>Token: ' equation'<br>Context: ...<｜Assistant｜><think>\nI need to solve the quadratic",
           "Position: 7<br>Token: ' \\('<br>Context: ...nt｜><think>\nI need to solve the quadratic equation",
           "Position: 8<br>Token: '3'<br>Context: ...><think>\nI need to solve the quadratic equation \\(",
           "Position: 9<br>Token: 'x'<br>Context: ...<think>\nI need to solve the quadratic equation \\(3",
           "Position: 10<br>Token: '^'<br>Context: ...think>\nI need to solve the quadratic equation \\(3x",
           "Position: 11<br>Token: '2'<br>Context: ...hink>\nI need to solve the quadratic equation \\(3x^",
           "Position: 12<br>Token: ' -'<br>Context: ...ink>\nI need to solve the quadratic equation \\(3x^2",
           "Position: 13<br>Token: ' '<br>Context: ...k>\nI need to solve the quadratic equation \\(3x^2 -",
           "Position: 14<br>Token: '12'<br>Context: ...>\nI need to solve the quadratic equation \\(3x^2 - ",
           "Position: 15<br>Token: 'x'<br>Context: ...I need to solve the quadratic equation \\(3x^2 - 12",
           "Position: 16<br>Token: ' +'<br>Context: ... need to solve the quadratic equation \\(3x^2 - 12x",
           "Position: 17<br>Token: ' '<br>Context: ...eed to solve the quadratic equation \\(3x^2 - 12x +",
           "Position: 18<br>Token: '9'<br>Context: ...ed to solve the quadratic equation \\(3x^2 - 12x + ",
           "Position: 19<br>Token: ' ='<br>Context: ...d to solve the quadratic equation \\(3x^2 - 12x + 9"
          ],
          [
           "Position: 0<br>Token: 'I'<br>Context: ... equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
           "Position: 1<br>Token: ' need'<br>Context: ...equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI",
           "Position: 2<br>Token: ' to'<br>Context: ...ion: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI need",
           "Position: 3<br>Token: ' solve'<br>Context: ...: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\nI need to",
           "Position: 4<br>Token: ' the'<br>Context: ... - 12x + 9 = 0<｜Assistant｜><think>\nI need to solve",
           "Position: 5<br>Token: ' quadratic'<br>Context: ...2x + 9 = 0<｜Assistant｜><think>\nI need to solve the",
           "Position: 6<br>Token: ' equation'<br>Context: ...<｜Assistant｜><think>\nI need to solve the quadratic",
           "Position: 7<br>Token: ' \\('<br>Context: ...nt｜><think>\nI need to solve the quadratic equation",
           "Position: 8<br>Token: '3'<br>Context: ...><think>\nI need to solve the quadratic equation \\(",
           "Position: 9<br>Token: 'x'<br>Context: ...<think>\nI need to solve the quadratic equation \\(3",
           "Position: 10<br>Token: '^'<br>Context: ...think>\nI need to solve the quadratic equation \\(3x",
           "Position: 11<br>Token: '2'<br>Context: ...hink>\nI need to solve the quadratic equation \\(3x^",
           "Position: 12<br>Token: ' -'<br>Context: ...ink>\nI need to solve the quadratic equation \\(3x^2",
           "Position: 13<br>Token: ' '<br>Context: ...k>\nI need to solve the quadratic equation \\(3x^2 -",
           "Position: 14<br>Token: '12'<br>Context: ...>\nI need to solve the quadratic equation \\(3x^2 - ",
           "Position: 15<br>Token: 'x'<br>Context: ...I need to solve the quadratic equation \\(3x^2 - 12",
           "Position: 16<br>Token: ' +'<br>Context: ... need to solve the quadratic equation \\(3x^2 - 12x",
           "Position: 17<br>Token: ' '<br>Context: ...eed to solve the quadratic equation \\(3x^2 - 12x +",
           "Position: 18<br>Token: '9'<br>Context: ...ed to solve the quadratic equation \\(3x^2 - 12x + ",
           "Position: 19<br>Token: ' ='<br>Context: ...d to solve the quadratic equation \\(3x^2 - 12x + 9"
          ]
         ],
         "hoverongaps": false,
         "hovertemplate": "%{y}<br>%{customdata}<extra></extra>",
         "showscale": true,
         "type": "heatmap",
         "x": [
          "0: 'I'",
          "1: ' need'",
          "2: ' to'",
          "3: ' solve'",
          "4: ' the'",
          "5: ' quadratic'",
          "6: ' equation'",
          "7: ' \\('",
          "8: '3'",
          "9: 'x'",
          "10: '^'",
          "11: '2'",
          "12: ' -'",
          "13: ' '",
          "14: '12'",
          "15: 'x'",
          "16: ' +'",
          "17: ' '",
          "18: '9'",
          "19: ' ='"
         ],
         "y": [
          "Layer 2",
          "Layer 30"
         ],
         "z": [
          [
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ]
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "size": 12
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Heatmap of Layer Impact on Token Generation"
        },
        "width": 900,
        "xaxis": {
         "showgrid": false,
         "tickangle": 45,
         "title": {
          "text": "Token Position and Value"
         }
        },
        "yaxis": {
         "showgrid": false,
         "title": {
          "text": "Model Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [],
       "layout": {
        "annotations": [
         {
          "align": "center",
          "bgcolor": "rgba(0,0,0,0)",
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "<b>Clean Output (No Hint)</b>",
          "x": 0.5,
          "xref": "paper",
          "y": 0.95,
          "yref": "paper"
         },
         {
          "align": "left",
          "bgcolor": "rgba(240, 240, 240, 0.7)",
          "bordercolor": "black",
          "borderpad": 10,
          "borderwidth": 1,
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "To solve the quadratic equation \\(3x^2 - 12x + 9 = 0\\), I'll start by identifying the coefficients of",
          "x": 0.01,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.85,
          "yanchor": "top",
          "yref": "paper"
         },
         {
          "align": "center",
          "bgcolor": "rgba(0,0,0,0)",
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "<b>Corrupted Output (With Hint)</b>",
          "x": 0.5,
          "xref": "paper",
          "y": 0.65,
          "yref": "paper"
         },
         {
          "align": "left",
          "bgcolor": "rgba(255, 230, 230, 0.7)",
          "bordercolor": "black",
          "borderpad": 10,
          "borderwidth": 1,
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "To solve the quadratic equation \\( 3x^2 - 12x + 9 = 0 \\), I'll start by identifying the coefficients",
          "x": 0.01,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.55,
          "yanchor": "top",
          "yref": "paper"
         },
         {
          "align": "center",
          "bgcolor": "rgba(0,0,0,0)",
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "<b>Patched Output (Layer 2)</b>",
          "x": 0.5,
          "xref": "paper",
          "y": 0.35,
          "yref": "paper"
         },
         {
          "align": "left",
          "bgcolor": "rgba(230, 255, 230, 0.7)",
          "bordercolor": "black",
          "borderpad": 10,
          "borderwidth": 1,
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": " Factoratic equation\n\n<think>\nI need to solve the quadratic equation \\(3x^2 - 12x + 9 = 0\\) using",
          "x": 0.01,
          "xanchor": "left",
          "xref": "paper",
          "y": 0.25,
          "yanchor": "top",
          "yref": "paper"
         }
        ],
        "height": 800,
        "margin": {
         "b": 50,
         "l": 50,
         "r": 50,
         "t": 50
        },
        "plot_bgcolor": "rgba(0,0,0,0)",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Comparison of Generations"
        },
        "xaxis": {
         "visible": false
        },
        "yaxis": {
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATION-PHASE ACTIVATION PATCHING REPORT\n",
      "================================================================================\n",
      "\n",
      "BASELINE OUTPUTS:\n",
      "Clean prompt: <｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "\n",
      "Clean generation: I need to solve the quadratic equation \\(3x^2 - 12x + 9 =\n",
      "Corrupted prompt: <｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0\n",
      "\n",
      "Hint: Quadratic Formula<｜Assistant｜><think>\n",
      "\n",
      "Corrupted generation: To solve the quadratic equation \\( 3x^2 - 12x + 9 = 0 \\), I'll start by identifying the coefficients\n",
      "\n",
      "MOST IMPACTFUL LAYERS:\n",
      "Layer 2: Changed 8 tokens\n",
      "Layer 30: Changed 8 tokens\n",
      "\n",
      "MOST AFFECTED TOKEN POSITIONS:\n",
      "Position 0 ('I'): Changed by 2 layers\n",
      "Position 1 (' need'): Changed by 2 layers\n",
      "Position 2 (' to'): Changed by 2 layers\n",
      "Position 3 (' solve'): Changed by 2 layers\n",
      "Position 4 (' the'): Changed by 2 layers\n",
      "\n",
      "DETAILED TOKEN-BY-TOKEN ANALYSIS:\n",
      "\n",
      "Token position 0 (Clean: 'I'):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "\n",
      "  Layer 2: 'I' → ' Factor'\n",
      "  Layer 30: 'I' → ' Factor'\n",
      "\n",
      "Token position 1 (Clean: ' need'):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "I\n",
      "  Layer 2: ' need' → 'atic'\n",
      "  Layer 30: ' need' → 'atic'\n",
      "\n",
      "Token position 2 (Clean: ' to'):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "I need\n",
      "  Layer 2: ' to' → ' equation'\n",
      "  Layer 30: ' to' → ' equation'\n",
      "\n",
      "Token position 3 (Clean: ' solve'):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "I need to\n",
      "  Layer 2: ' solve' → '\n",
      "\n",
      "'\n",
      "  Layer 30: ' solve' → '\n",
      "\n",
      "'\n",
      "\n",
      "Token position 4 (Clean: ' the'):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "I need to solve\n",
      "  Layer 2: ' the' → '<think>'\n",
      "  Layer 30: ' the' → '<think>'\n",
      "\n",
      "Token position 5 (Clean: ' quadratic'):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "I need to solve the\n",
      "  Layer 2: ' quadratic' → '\n",
      "'\n",
      "  Layer 30: ' quadratic' → '\n",
      "'\n",
      "\n",
      "Token position 6 (Clean: ' equation'):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "I need to solve the quadratic\n",
      "  Layer 2: ' equation' → 'I'\n",
      "  Layer 30: ' equation' → 'I'\n",
      "\n",
      "Token position 7 (Clean: ' \\('):\n",
      "Context: <｜begin▁of▁sentence｜><｜begin▁of▁sentence｜><｜User｜>Solve this equation: 3x^2 - 12x + 9 = 0<｜Assistant｜><think>\n",
      "I need to solve the quadratic equation\n",
      "  Layer 2: ' \\(' → ':'\n",
      "  Layer 30: ' \\(' → ' '\n",
      "\n",
      "================================================================================\n",
      "FULL PATCHED GENERATION (Layer 2):\n",
      " Factoratic equation\n",
      "\n",
      "<think>\n",
      "I need to solve the quadratic equation \\(3x^2 - 12x + 9 = 0\\) using\n",
      "\n",
      "================================================================================\n",
      "COMPARISON OF ALL THREE GENERATIONS:\n",
      "1. Clean prompt: To solve the quadratic equation \\(3x^2 - 12x + 9 = 0\\), I'll start by identifying the coefficients of\n",
      "2. Corrupted prompt: To solve the quadratic equation \\( 3x^2 - 12x + 9 = 0 \\), I'll start by identifying the coefficients\n",
      "3. Patched at layer 2:  Factoratic equation\n",
      "\n",
      "<think>\n",
      "I need to solve the quadratic equation \\(3x^2 - 12x + 9 = 0\\) using\n",
      "\n",
      "KEY INSIGHTS FROM ACTIVATION PATCHING:\n",
      "Critical layers for reasoning: Layer 2, Layer 30\n",
      "Critical reasoning tokens: 'I' (pos 0), ' need' (pos 1), ' to' (pos 2)\n",
      "Early tokens in the generation were affected, suggesting the model's reasoning path diverges early.\n",
      "Patching produced tokens matching the corrupted output at: Layer 2 at position 1, Layer 2 at position 2, Layer 2 at position 6\n"
     ]
    }
   ],
   "source": [
    "results = example_patching_analysis(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
