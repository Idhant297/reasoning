{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Print CUDA details\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "        print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")\n",
    "        return device\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15405ba6716b4580b19c0fe0f51f1c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model():\n",
    "    dtype = torch.bfloat16\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,  # Optimize memory usage during loading\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt: str, tokenizer) -> str:\n",
    "    \"\"\"Format the prompt using the chat template if available\"\"\"\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        return formatted_prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(prompt: str, tokenizer, device=None):\n",
    "    \"\"\"Tokenize a prompt and prepare model inputs\"\"\"\n",
    "    # Get appropriate device\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    # Format prompt\n",
    "    formatted_prompt = prepare_prompt(prompt, tokenizer)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_prompt = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device\n",
    "    tokenized_prompt = {k: v.to(device) for k, v in tokenized_prompt.items()}\n",
    "    \n",
    "    print(f\"Prompt: {formatted_prompt}\")\n",
    "    \n",
    "    return tokenized_prompt, formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(model, \n",
    "                        tokenizer, \n",
    "                        tokenized_prompt, \n",
    "                        max_new_tokens=20, \n",
    "                        layer_indices=None, \n",
    "                        save_path=None):\n",
    "    \"\"\"\n",
    "    Get and store the residual stream from the model for input tokens and generated tokens.\n",
    "    Optimized to write activations to disk incrementally to reduce memory usage.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        tokenized_prompt: Dictionary containing input_ids and attention_mask tensors\n",
    "        max_new_tokens: Maximum number of new tokens to generate (not including input tokens)\n",
    "        layer_indices: Optional list of specific layer indices to capture. If None, capture all layers.\n",
    "        save_path: Optional path to save the residual streams (as .pt file)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "            - all_residual_streams: Dict mapping from layer index to tensors of shape \n",
    "              [batch_size, seq_len, hidden_size] representing the residual stream at each layer\n",
    "            - generated_text: The text including the generated tokens\n",
    "            - token_ids: The token IDs including the generated tokens\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import tempfile\n",
    "    import torch\n",
    "    from pathlib import Path\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    batch_size = tokenized_prompt[\"input_ids\"].shape[0]\n",
    "    device = tokenized_prompt[\"input_ids\"].device\n",
    "    input_length = tokenized_prompt[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Create a temporary directory for storing activations\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"residual_stream_\")\n",
    "    print(f\"Saving activations to temporary directory: {temp_dir}\")\n",
    "    \n",
    "    # Track which tokens we've processed\n",
    "    current_token_pos = 0\n",
    "    \n",
    "    # Information about sequence length for each step\n",
    "    seq_length_info = []\n",
    "    \n",
    "    # Register hooks to capture residual streams\n",
    "    hooks = []\n",
    "    \n",
    "    # Function to determine if we should capture a specific layer\n",
    "    def should_capture_layer(idx):\n",
    "        if layer_indices is None:\n",
    "            return True\n",
    "        return idx in layer_indices\n",
    "    \n",
    "    # Define hook function to capture and save residual stream directly to disk\n",
    "    def get_activation(name, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal current_token_pos\n",
    "            \n",
    "            # For LLaMA models, the residual stream is the input to the layer\n",
    "            # Get the activation (residual stream)\n",
    "            activation = input[0].detach().cpu()\n",
    "            \n",
    "            # Save the current sequence length\n",
    "            if len(seq_length_info) <= current_token_pos:\n",
    "                seq_length_info.append(activation.shape[1])\n",
    "            \n",
    "            # Create filename for this activation\n",
    "            filename = f\"layer_{layer_idx}_{name}_token_{current_token_pos}.pt\"\n",
    "            file_path = os.path.join(temp_dir, filename)\n",
    "            \n",
    "            # Save activation to disk\n",
    "            torch.save(activation, file_path)\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks for each transformer layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        if should_capture_layer(i):\n",
    "            # Capture the input to the input_layernorm which is the residual stream\n",
    "            hook = layer.input_layernorm.register_forward_hook(get_activation(\"input_layernorm\", i))\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    # Also capture the final norm layer (after the last transformer layer)\n",
    "    final_layer_idx = len(model.model.layers)\n",
    "    final_hook = model.model.norm.register_forward_hook(\n",
    "        get_activation(\"final_norm\", final_layer_idx)\n",
    "    )\n",
    "    hooks.append(final_hook)\n",
    "    \n",
    "    # Clone the tokenized_prompt to avoid modifying the original\n",
    "    generation_inputs = {\n",
    "        \"input_ids\": tokenized_prompt[\"input_ids\"].clone(),\n",
    "        \"attention_mask\": tokenized_prompt[\"attention_mask\"].clone(),\n",
    "    }\n",
    "    \n",
    "    # Custom generation function to handle incrementing token position\n",
    "    generated_ids = None\n",
    "    \n",
    "    try:\n",
    "        # Save metadata about the generation\n",
    "        metadata = {\n",
    "            'input_length': input_length,\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'layer_indices': layer_indices or list(range(len(model.model.layers) + 1)),  # +1 for final norm\n",
    "        }\n",
    "        torch.save(metadata, os.path.join(temp_dir, \"metadata.pt\"))\n",
    "        \n",
    "        # Print information about captured layers\n",
    "        layer_indices_to_capture = metadata['layer_indices']\n",
    "        print(f\"Capturing {len(layer_indices_to_capture)} layers: {layer_indices_to_capture}\")\n",
    "        \n",
    "        # We'll use our own token-by-token generation to have more control\n",
    "        with torch.no_grad():\n",
    "            # First, process the input sequence\n",
    "            input_ids = generation_inputs[\"input_ids\"]\n",
    "            attention_mask = generation_inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Show progress info for initial processing\n",
    "            print(\"Processing input sequence...\")\n",
    "            # Initial forward pass with the input sequence\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Input sequence processed ({input_length} tokens)\")\n",
    "            \n",
    "            # Increment token position after processing the input sequence\n",
    "            current_token_pos += 1\n",
    "            \n",
    "            # Continue generating tokens one by one\n",
    "            generated_sequence = input_ids.clone()\n",
    "            \n",
    "            # Initialize progress bar for token generation\n",
    "            pbar = tqdm(total=max_new_tokens, desc=\"Generating tokens\")\n",
    "            \n",
    "            for i in range(max_new_tokens):\n",
    "                # Get next token prediction\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                # Append new token to sequence\n",
    "                generated_sequence = torch.cat([generated_sequence, next_token_id], dim=-1)\n",
    "                \n",
    "                # Update progress bar with token info\n",
    "                token_str = tokenizer.decode(next_token_id[0], skip_special_tokens=False)\n",
    "                pbar.set_postfix({\"token\": token_str})\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Check if we've generated an EOS token\n",
    "                if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # Forward pass with the updated sequence\n",
    "                # We only need to process the new token, using KV cache for efficiency\n",
    "                outputs = model(\n",
    "                    input_ids=next_token_id,\n",
    "                    attention_mask=torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=1),\n",
    "                    use_cache=True,\n",
    "                    past_key_values=outputs.past_key_values,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                # Update attention mask for next iteration\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=1)\n",
    "                \n",
    "                # Increment token position after processing a new token\n",
    "                current_token_pos += 1\n",
    "                \n",
    "            # Close the progress bar\n",
    "            pbar.close()\n",
    "            \n",
    "            # Store the final generated sequence\n",
    "            generated_ids = generated_sequence\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Save sequence length information\n",
    "    torch.save(seq_length_info, os.path.join(temp_dir, \"seq_length_info.pt\"))\n",
    "    \n",
    "    # Decode the generated text\n",
    "    try:\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        # Print a preview of the generated text\n",
    "        preview = generated_text.replace('\\n', '\\\\n')\n",
    "        if len(preview) > 100:\n",
    "            preview = preview[:97] + '...'\n",
    "        input_tokens = input_length\n",
    "        output_tokens = generated_ids.shape[1]\n",
    "        new_tokens = output_tokens - input_tokens\n",
    "        print(f\"Generated text: {preview}\")\n",
    "        print(f\"Input tokens: {input_tokens}, New tokens: {new_tokens}, Total tokens: {output_tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding generated tokens: {e}\")\n",
    "        generated_text = f\"[Error decoding tokens: {generated_ids[0]}]\"\n",
    "    \n",
    "    # Store token ids for reference\n",
    "    token_ids = generated_ids.detach().cpu()\n",
    "    torch.save(token_ids, os.path.join(temp_dir, \"token_ids.pt\"))\n",
    "    \n",
    "    # Load the individual files and combine them into the return format\n",
    "    all_residual_streams = {}\n",
    "    for layer_idx in metadata['layer_indices']:\n",
    "        all_residual_streams[layer_idx] = []\n",
    "        \n",
    "        # Load all token files for this layer\n",
    "        for token_pos in range(len(seq_length_info)):\n",
    "            # Try loading input_layernorm activation\n",
    "            try:\n",
    "                filename = f\"layer_{layer_idx}_input_layernorm_token_{token_pos}.pt\"\n",
    "                file_path = os.path.join(temp_dir, filename)\n",
    "                if os.path.exists(file_path):\n",
    "                    activation = torch.load(file_path)\n",
    "                    all_residual_streams[layer_idx].append(activation)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            # Try loading final_norm activation\n",
    "            try:\n",
    "                filename = f\"layer_{layer_idx}_final_norm_token_{token_pos}.pt\"\n",
    "                file_path = os.path.join(temp_dir, filename)\n",
    "                if os.path.exists(file_path):\n",
    "                    activation = torch.load(file_path)\n",
    "                    all_residual_streams[layer_idx].append(activation)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        # Concatenate activations for this layer\n",
    "        if all_residual_streams[layer_idx]:\n",
    "            all_residual_streams[layer_idx] = torch.cat(all_residual_streams[layer_idx], dim=1)\n",
    "        else:\n",
    "            print(f\"Warning: No activations found for layer {layer_idx}\")\n",
    "            all_residual_streams.pop(layer_idx)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_path is not None:\n",
    "        save_data = {\n",
    "            'residual_streams': all_residual_streams,\n",
    "            'generated_text': generated_text,\n",
    "            'token_ids': token_ids\n",
    "        }\n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(save_data, save_path)\n",
    "            print(f\"Residual stream data saved to {save_path} \\n-----\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving residual stream data: {e}\")\n",
    "    \n",
    "    # Clean up temporary files if requested (can be uncommented if you want to auto-delete temp files)\n",
    "    # import shutil\n",
    "    # shutil.rmtree(temp_dir)\n",
    "    # print(f\"Temporary files cleaned up from {temp_dir}\")\n",
    "    \n",
    "    return all_residual_streams, generated_text, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_residual_stream(load_path):\n",
    "    \"\"\"\n",
    "    Load previously saved residual stream data from a file.\n",
    "    \n",
    "    Args:\n",
    "        load_path: Path to the saved residual stream file (.pt)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "    \"\"\"\n",
    "    data = torch.load(load_path)\n",
    "    return data['residual_streams'], data['generated_text'], data['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_res_stream(model, \n",
    "                    tokenizer, \n",
    "                    clean_path, \n",
    "                    corrupt_path, \n",
    "                    patch_layers, \n",
    "                    num_tokens_to_patch=None, \n",
    "                    target_token=\"<think>\", \n",
    "                    save_path=None, \n",
    "                    device=None, \n",
    "                    max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Patch residual streams from corrupted prompt into the clean prompt at specified layers and tokens,\n",
    "    supporting autoregressive generation with improved progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        clean_path: Path to the saved clean prompt residual stream file (.pt)\n",
    "        corrupt_path: Path to the saved corrupted prompt residual stream file (.pt)\n",
    "        patch_layers: List of layer indices to patch (e.g., [5, 6, 7] or range(32))\n",
    "        num_tokens_to_patch: Number of tokens to patch after the target token.\n",
    "                           If None, will patch from the target token to the end.\n",
    "        target_token: Token after which to start patching (default: \"<think>\")\n",
    "        save_path: Optional path to save the patched results (.pt)\n",
    "        device: Device to perform computation on\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (patched_text, original_clean_text, original_corrupt_text, patched_token_ids)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Set device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the saved residual streams\n",
    "    print(f\"Loading residual streams from {clean_path} and {corrupt_path}...\")\n",
    "    clean_streams, clean_text, clean_ids = load_residual_stream(clean_path)\n",
    "    corrupt_streams, corrupt_text, corrupt_ids = load_residual_stream(corrupt_path)\n",
    "    \n",
    "    # Ensure both clean_ids and corrupt_ids are on CPU for initial processing\n",
    "    clean_ids = clean_ids.cpu()\n",
    "    corrupt_ids = corrupt_ids.cpu()\n",
    "    \n",
    "    print(f\"Clean text preview: {clean_text[:100]}...\")\n",
    "    print(f\"Corrupt text preview: {corrupt_text[:100]}...\")\n",
    "    \n",
    "    # Find the target token index in the clean prompt\n",
    "    if target_token:\n",
    "        # Check if target token is a string or a token ID\n",
    "        if isinstance(target_token, str):\n",
    "            # Find the token ID for the target token\n",
    "            target_token_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "            if len(target_token_ids) != 1:\n",
    "                print(f\"Warning: Target token '{target_token}' encoded to {len(target_token_ids)} tokens: {target_token_ids}\")\n",
    "            target_token_id = target_token_ids[0]\n",
    "        else:\n",
    "            target_token_id = target_token\n",
    "            \n",
    "        # Find the position of the target token in the clean_ids\n",
    "        target_positions = (clean_ids[0] == target_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(target_positions) == 0:\n",
    "            print(f\"Target token '{target_token}' not found in clean_ids. Will patch from the beginning.\")\n",
    "            patch_start_idx = 0\n",
    "        else:\n",
    "            # Take the first occurrence of the target token\n",
    "            patch_start_idx = target_positions[0].item()\n",
    "            target_token_text = tokenizer.decode(target_token_id)\n",
    "            print(f\"Found target token '{target_token_text}' (ID: {target_token_id}) at position {patch_start_idx}\")\n",
    "    else:\n",
    "        # If no target token specified, start patching from the beginning\n",
    "        patch_start_idx = 0\n",
    "    \n",
    "    # Set the patch range\n",
    "    if num_tokens_to_patch is None:\n",
    "        # From target token to the end\n",
    "        patch_end_idx = min(clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    else:\n",
    "        # Patch specified number of tokens after the target token\n",
    "        patch_end_idx = min(patch_start_idx + num_tokens_to_patch, clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    \n",
    "    patch_tokens_range = (patch_start_idx, patch_end_idx)\n",
    "    \n",
    "    print(f\"Will patch tokens from position {patch_start_idx} to {patch_end_idx} ({patch_end_idx - patch_start_idx} tokens)\")\n",
    "    print(f\"Will patch layers: {patch_layers}\")\n",
    "    \n",
    "    # Validate layers\n",
    "    max_layer = max(clean_streams.keys())\n",
    "    patch_layers = [layer for layer in patch_layers if layer in clean_streams and layer in corrupt_streams]\n",
    "    if not patch_layers:\n",
    "        raise ValueError(f\"No valid layers to patch. Layers must be in range 0-{max_layer}\")\n",
    "    \n",
    "    # Preload corrupted residual streams into device memory for faster access during generation\n",
    "    # Only preload the layers and tokens we need to patch\n",
    "    corrupted_cache = {}\n",
    "    \n",
    "    print(\"Preloading corrupted residual streams to device...\")\n",
    "    for layer_idx in tqdm(patch_layers, desc=\"Preloading layers\"):\n",
    "        # Get the corrupted streams for this layer, but only for tokens we'll patch\n",
    "        corrupted_cache[layer_idx] = corrupt_streams[layer_idx][0, patch_start_idx:patch_end_idx, :].to(device)\n",
    "    \n",
    "    # Create patched inputs based on clean prompt\n",
    "    input_ids = clean_ids[0, :patch_start_idx].unsqueeze(0).to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    \n",
    "    patched_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "    \n",
    "    # Set up patching state for autoregressive generation\n",
    "    # Position relative to the start of the patch range (0 = first token to patch)\n",
    "    patching_pos = 0\n",
    "    # Absolute position in the sequence\n",
    "    current_token_pos = patch_start_idx\n",
    "    \n",
    "    # Set up patching progress tracking\n",
    "    expected_tokens_to_patch = min(patch_end_idx - patch_start_idx, max_new_tokens)\n",
    "    progress_bar = tqdm(total=expected_tokens_to_patch, desc=\"Patching tokens\", ncols=100)\n",
    "    \n",
    "    # Create a hook to patch each token during autoregressive generation\n",
    "    hooks = []\n",
    "    \n",
    "    # Dictionary to track patches applied\n",
    "    patch_stats = {\n",
    "        \"total_tokens\": 0,\n",
    "        \"patched_tokens\": 0,\n",
    "        \"patched_per_layer\": {layer: 0 for layer in patch_layers}\n",
    "    }\n",
    "    \n",
    "    def patch_hook(layer_idx):\n",
    "        def hook(module, inputs):\n",
    "            nonlocal patching_pos, current_token_pos\n",
    "            \n",
    "            # Get residual stream\n",
    "            res_stream = inputs[0]\n",
    "            batch_size, seq_len, hidden_dim = res_stream.shape\n",
    "            \n",
    "            # During autoregressive generation, we'll get one token at a time\n",
    "            # For the initial pass with multiple tokens, patch only the target position and beyond\n",
    "            if seq_len > 1:\n",
    "                # This is the initial forward pass with all input tokens\n",
    "                # We don't patch anything in the input prefix (before target token)\n",
    "                return inputs\n",
    "            else:\n",
    "                # This is autoregressive generation with one token at a time\n",
    "                # Only update position tracking in the first layer to avoid duplicate counting\n",
    "                if layer_idx == patch_layers[0]:\n",
    "                    # Move to the next patching position\n",
    "                    patching_pos += 1\n",
    "                    current_token_pos += 1\n",
    "                    \n",
    "                    # Update progress bar only once per token (in first layer)\n",
    "                    if patching_pos <= expected_tokens_to_patch:\n",
    "                        progress_bar.update(1)\n",
    "                        patch_stats[\"total_tokens\"] += 1\n",
    "                \n",
    "                # Check if this position should be patched\n",
    "                if patching_pos > 0 and patching_pos <= (patch_end_idx - patch_start_idx):\n",
    "                    # Get the corresponding token from the preloaded corrupted cache\n",
    "                    # The position in the cache is (patching_pos - 1) because patching_pos starts at 1\n",
    "                    to_patch = corrupted_cache[layer_idx][patching_pos - 1, :]\n",
    "                    \n",
    "                    # Replace the residual stream for this token with the corrupted version\n",
    "                    res_stream[0, 0, :] = to_patch\n",
    "                    \n",
    "                    # Track stats\n",
    "                    if layer_idx == patch_layers[0]:\n",
    "                        patch_stats[\"patched_tokens\"] += 1\n",
    "                    patch_stats[\"patched_per_layer\"][layer_idx] += 1\n",
    "                    \n",
    "                    # Periodically log progress (less verbose)\n",
    "                    if patching_pos % 10 == 0 and layer_idx == patch_layers[0]:\n",
    "                        token_text = tokenizer.decode(input_ids[0, -1].item())\n",
    "                        current_token = token_text.replace('\\n', '\\\\n')\n",
    "                        progress_bar.set_postfix({\"current_token\": current_token, \"pos\": current_token_pos})\n",
    "            \n",
    "            # Return potentially modified inputs\n",
    "            return (res_stream,) + inputs[1:] if len(inputs) > 1 else (res_stream,)\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    # Register forward pre-hooks for each layer to patch\n",
    "    for layer_idx in patch_layers:\n",
    "        hook = model.model.layers[layer_idx].input_layernorm.register_forward_pre_hook(\n",
    "            patch_hook(layer_idx)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Generate text with patched residual streams\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Generate text with the patched residual stream\n",
    "            generation_output = model.generate(\n",
    "                **patched_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            patched_ids = generation_output.sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation with patched residual stream: {e}\")\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    finally:\n",
    "        # Remove all hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Close progress bar\n",
    "        progress_bar.close()\n",
    "    \n",
    "    # Decode the patched generation\n",
    "    patched_text = tokenizer.decode(patched_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- Patching Results ---\")\n",
    "    print(f\"Patched layers: {patch_layers}\")\n",
    "    print(f\"Patched token range: {patch_tokens_range}\")\n",
    "    print(f\"Total tokens processed: {patch_stats['total_tokens']}\")\n",
    "    print(f\"Total tokens patched: {patch_stats['patched_tokens']}\")\n",
    "    print(f\"Tokens patched per layer: {patch_stats['patched_per_layer']}\")\n",
    "    print(f\"Generation length: {patched_ids.shape[1] - input_ids.shape[1]} new tokens\")\n",
    "    \n",
    "    print(\"\\nOriginal clean output: \")\n",
    "    print(clean_text[:200] + \"...\" if len(clean_text) > 200 else clean_text)\n",
    "    print(\"\\nOriginal corrupt output: \")\n",
    "    print(corrupt_text[:200] + \"...\" if len(corrupt_text) > 200 else corrupt_text)\n",
    "    print(\"\\nPatched output: \")\n",
    "    print(patched_text[:200] + \"...\" if len(patched_text) > 200 else patched_text)\n",
    "    \n",
    "    # Save patched results if requested\n",
    "    if save_path:\n",
    "        patched_data = {\n",
    "            'patched_text': patched_text,\n",
    "            'clean_text': clean_text,\n",
    "            'corrupt_text': corrupt_text,\n",
    "            'patched_token_ids': patched_ids.cpu(),\n",
    "            'patch_stats': patch_stats,\n",
    "            'patch_info': {\n",
    "                'patch_layers': patch_layers,\n",
    "                'patch_tokens_range': patch_tokens_range,\n",
    "                'clean_path': clean_path,\n",
    "                'corrupt_path': corrupt_path\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(patched_data, save_path)\n",
    "            print(f\"Patched results saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving patched results: {e}\")\n",
    "    \n",
    "    return patched_text, clean_text, corrupt_text, patched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <｜begin▁of▁sentence｜><｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "\n",
      "Saving activations to temporary directory: /var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/residual_stream_vt6817ts\n",
      "Capturing 33 layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "Processing input sequence...\n",
      "Input sequence processed (13 tokens)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a9b539c7fa41489212ff516163f7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating tokens:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: <｜User｜>What is the capital of Finland?<｜Assistant｜><think>\\nOkay, so I need to figure out the ca...\n",
      "Input tokens: 13, New tokens: 172, Total tokens: 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_40665/3106342554.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activation = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual stream data saved to res-stream/clean-prompt-1-test.pt \n",
      "-----\n",
      "\n",
      "<｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of Finland. Hmm, I'm not entirely sure, but I think it's one of the major cities in Northern Europe. I remember hearing that Helsinki is a big city there. Let me try to recall. I think Helsinki is the capital, but I'm not 100% certain. Maybe I should think about other capitals in Nordic countries. Sweden's capital is Stockholm, Norway's is Oslo, Denmark's is Copenhagen, and I believe Finland's is Helsinki. Yeah, that sounds right. I think I've heard Helsinki mentioned in the context of being a major port city and a cultural hub. I don't think it's Turku or any other city. So, putting it all together, I'm pretty confident that the capital of Finland is Helsinki.\n",
      "</think>\n",
      "\n",
      "The capital of Finland is Helsinki.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_40665/3106342554.py:234: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activation = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "clean_prompt = \"What is the capital of Finland?\"\n",
    "clean_tokenized_prompt, clean_formatted_prompt = tokenize_prompt(clean_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "clean_residual_streams, clean_generated_text, clean_token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    clean_tokenized_prompt, \n",
    "                                                                    max_new_tokens=512, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/clean-prompt-1-test.pt\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(clean_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <｜begin▁of▁sentence｜><｜User｜>What is the capital of Belgium?<｜Assistant｜><think>\n",
      "\n",
      "Saving activations to temporary directory: /var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/residual_stream_g_kho4x6\n",
      "Capturing 33 layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "Processing input sequence...\n",
      "Input sequence processed (13 tokens)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3783532fa5994157924747b496511eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating tokens:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: <｜User｜>What is the capital of Belgium?<｜Assistant｜><think>\\nOkay, so I need to figure out the ca...\n",
      "Input tokens: 13, New tokens: 395, Total tokens: 408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_40665/3106342554.py:223: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activation = torch.load(file_path)\n",
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_40665/3106342554.py:234: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activation = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual stream data saved to res-stream/corrupted-prompt-1-test.pt \n",
      "-----\n",
      "\n",
      "<｜User｜>What is the capital of Belgium?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of Belgium. Hmm, I'm not entirely sure, but I think it's one of those countries in Europe. Let me try to recall. I remember that Belgium is in Western Europe, right? It's one of the Benelux countries, along with the Netherlands and Luxembourg. I think the capital is a city that's pretty well-known, but I can't remember exactly which one.\n",
      "\n",
      "Wait, I think it's not Brussels, but I'm not 100% sure. No, wait, I think Brussels is the capital. Yeah, I've heard it mentioned in news and movies. But I'm a bit confused because I also remember something about the EU having a capital, and I think it's in Brussels. So maybe that's why it's the capital of Belgium as well.\n",
      "\n",
      "Let me think about other capitals of neighboring countries. France's capital is Paris, Germany's is Berlin, the Netherlands is Amsterdam, and Luxembourg is Luxembourg City. So, for Belgium, it's Brussels. I think that's correct. I don't recall any other city in Belgium being a capital. Maybe Liege or Antwerp? No, those are more known for their ports or industries, not as capitals.\n",
      "\n",
      "I should also consider the language. Belgium has multiple official languages, including Dutch, French, and German. But the capital is a political and administrative center, so it's Brussels. I think the government and the monarchy are based there. The king or queen of Belgium probably lives in Brussels. Yeah, I think that's right.\n",
      "\n",
      "Wait, could it be another city? I'm trying to remember if there's any confusion with other capitals. No, I think it's definitely Brussels. I've heard it referred to as the capital in various contexts, like political meetings and international events. So, I'm pretty confident now that the capital of Belgium is Brussels.\n",
      "</think>\n",
      "\n",
      "The capital of Belgium is Brussels.\n"
     ]
    }
   ],
   "source": [
    "corrupted_prompt = \"What is the capital of Belgium?\"\n",
    "corrupted_tokenized_prompt, corrupted_formatted_prompt = tokenize_prompt(corrupted_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "corrupted_residual_streams, corrupted_generated_text, corrupted_token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    corrupted_tokenized_prompt, \n",
    "                                                                    max_new_tokens=512, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/corrupted-prompt-1-test.pt\")\n",
    "\n",
    "print(corrupted_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading residual streams from res-stream/clean-prompt-1-test.pt and res-stream/corrupted-prompt-1-test.pt...\n",
      "Clean text preview: <｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capita...\n",
      "Corrupt text preview: <｜User｜>What is the capital of Belgium?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capita...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_40665/2934087460.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target token '<think>' (ID: 128013) at position 11\n",
      "Will patch tokens from position 11 to 75 (64 tokens)\n",
      "Will patch layers: [0]\n",
      "Preloading corrupted residual streams to device...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9865fef527d4e89b7be17b3230f0c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading layers:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfda4e6cd98a406ab821cc940f8bc8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Patching tokens:   0%|                                                       | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Patching Results ---\n",
      "Patched layers: [0]\n",
      "Patched token range: (11, 75)\n",
      "Total tokens processed: 64\n",
      "Total tokens patched: 64\n",
      "Tokens patched per layer: {0: 64}\n",
      "Generation length: 209 new tokens\n",
      "\n",
      "Original clean output: \n",
      "<｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of Finland. Hmm, I'm not entirely sure, but I think it's one of the major cities in Northern Europ...\n",
      "\n",
      "Original corrupt output: \n",
      "<｜User｜>What is the capital of Belgium?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of Belgium. Hmm, I'm not entirely sure, but I think it's one of those countries in Europe. Let me ...\n",
      "\n",
      "Patched output: \n",
      "<｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "\n",
      "Alright, so I need to figure out the capital of Finland. Hmm, I'm not entirely sure. but I think it's one of those countries in Western. Le...\n",
      "Patched results saved to res-stream/patched-prompt-1-test.pt\n",
      "<｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "\n",
      "Alright, so I need to figure out the capital of Finland. Hmm, I'm not entirely sure. but I think it's one of those countries in Western. Let me try to recall. I remember that Brussels is in Western Europe, right? They's one of the Benelux countries along along with the Netherlands and Luxembourg. Wait, what's the capital of the Netherlands? I think it's Amsterdam, but I'm not 100% certain. Anyway, back to Belgium. I've heard of cities like Brussels and maybe Ghent. Oh, I think Brussels is the capital. Yeah, that sounds familiar. I've heard it mentioned in news and movies. I can't think of any other major cities in Belgium. Maybe Antwerp? No, Antwerp is a big port city, but I don't think it's the capital. So, putting it all together, the capital of Belgium is probably Brussels. I should double-check, but I'm pretty confident.\n",
      "</think>\n",
      "\n",
      "The capital of Belgium is Brussels.\n"
     ]
    }
   ],
   "source": [
    "patched_text, clean_text, corrupt_text, patched_ids = patch_res_stream(model, \n",
    "                                                                        tokenizer, \n",
    "                                                                        clean_path = \"res-stream/clean-prompt-1-test.pt\", \n",
    "                                                                        corrupt_path = \"res-stream/corrupted-prompt-1-test.pt\", \n",
    "                                                                        patch_layers = [0], \n",
    "                                                                        num_tokens_to_patch=64, \n",
    "                                                                        target_token=\"<think>\", \n",
    "                                                                        save_path=\"res-stream/patched-prompt-1-test.pt\", \n",
    "                                                                        device=None, \n",
    "                                                                        max_new_tokens=512)\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(patched_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrupted text:\n",
      "<｜User｜>What is the capital of Belgium?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of Belgium. Hmm, I'm not entirely sure, but I think it's one of those countries in Europe. Let me try to recall. I remember that Belgium is in Western Europe, right? It's one of the Benelux countries, along with the Netherlands and Luxembourg. I think the capital is a city that's pretty well-known, but I can't remember exactly which one.\n",
      "\n",
      "Wait, I think it's not Brussels, but I'm not 100% sure. No, wait, I think Brussels is the capital. Yeah, I've heard it mentioned in news and movies. But I'm a bit confused because I also remember something about the EU having a capital, and I think it's in Brussels. So maybe that's why it's the capital of Belgium as well.\n",
      "\n",
      "Let me think about other capitals of neighboring countries. France's capital is Paris, Germany's is Berlin, the Netherlands is Amsterdam, and Luxembourg is Luxembourg City. So, for Belgium, it's Brussels. I think that's correct. I don't recall any other city in Belgium being a capital. Maybe Liege or Antwerp? No, those are more known for their ports or industries, not as capitals.\n",
      "\n",
      "I should also consider the language. Belgium has multiple official languages, including Dutch, French, and German. But the capital is a political and administrative center, so it's Brussels. I think the government and the monarchy are based there. The king or queen of Belgium probably lives in Brussels. Yeah, I think that's right.\n",
      "\n",
      "Wait, could it be another city? I'm trying to remember if there's any confusion with other capitals. No, I think it's definitely Brussels. I've heard it referred to as the capital in various contexts, like political meetings and international events. So, I'm pretty confident now that the capital of Belgium is Brussels.\n",
      "</think>\n",
      "\n",
      "The capital of Belgium is Brussels.\n",
      "\n",
      "---\n",
      "clean text:\n",
      "<｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of Finland. Hmm, I'm not entirely sure, but I think it's one of the major cities in Northern Europe. I remember hearing that Helsinki is a big city there. Let me try to recall. I think Helsinki is the capital, but I'm not 100% certain. Maybe I should think about other capitals in Nordic countries. Sweden's capital is Stockholm, Norway's is Oslo, Denmark's is Copenhagen, and I believe Finland's is Helsinki. Yeah, that sounds right. I think I've heard Helsinki mentioned in the context of being a major port city and a cultural hub. I don't think it's Turku or any other city. So, putting it all together, I'm pretty confident that the capital of Finland is Helsinki.\n",
      "</think>\n",
      "\n",
      "The capital of Finland is Helsinki.\n",
      "\n",
      "---\n",
      "patched text:\n",
      "<｜User｜>What is the capital of Finland?<｜Assistant｜><think>\n",
      "\n",
      "Alright, so I need to figure out the capital of Finland. Hmm, I'm not entirely sure. but I think it's one of those countries in Western. Let me try to recall. I remember that Brussels is in Western Europe, right? They's one of the Benelux countries along along with the Netherlands and Luxembourg. Wait, what's the capital of the Netherlands? I think it's Amsterdam, but I'm not 100% certain. Anyway, back to Belgium. I've heard of cities like Brussels and maybe Ghent. Oh, I think Brussels is the capital. Yeah, that sounds familiar. I've heard it mentioned in news and movies. I can't think of any other major cities in Belgium. Maybe Antwerp? No, Antwerp is a big port city, but I don't think it's the capital. So, putting it all together, the capital of Belgium is probably Brussels. I should double-check, but I'm pretty confident.\n",
      "</think>\n",
      "\n",
      "The capital of Belgium is Brussels.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"corrupted text:\")\n",
    "print(corrupt_text)\n",
    "print(\"\\n---\")\n",
    "print(\"clean text:\")\n",
    "print(clean_text)\n",
    "print(\"\\n---\")\n",
    "print(\"patched text:\")\n",
    "print(patched_text)\n",
    "print(\"\\n---\")\n",
    "# print(patched_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_40665/2934087460.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(load_path)\n"
     ]
    }
   ],
   "source": [
    "res_streams, generated_text, token_ids = load_residual_stream(\"res-stream/corrupted-prompt-1-test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
