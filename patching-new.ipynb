{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Print CUDA details\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "        print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")\n",
    "        return device\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f0f4de289e417e9bb1e1d4cb1b7f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model():\n",
    "    dtype = torch.bfloat16\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,  # Optimize memory usage during loading\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt: str, tokenizer) -> str:\n",
    "    \"\"\"Format the prompt using the chat template if available\"\"\"\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        return formatted_prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(prompt: str, tokenizer, device=None):\n",
    "    \"\"\"Tokenize a prompt and prepare model inputs\"\"\"\n",
    "    # Get appropriate device\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    # Format prompt\n",
    "    formatted_prompt = prepare_prompt(prompt, tokenizer)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_prompt = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device\n",
    "    tokenized_prompt = {k: v.to(device) for k, v in tokenized_prompt.items()}\n",
    "    \n",
    "    print(f\"Prompt: {formatted_prompt}\")\n",
    "    \n",
    "    return tokenized_prompt, formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(model, \n",
    "                        tokenizer, \n",
    "                        tokenized_prompt, \n",
    "                        max_new_tokens=20, \n",
    "                        layer_indices=None, \n",
    "                        save_path=None):\n",
    "    \"\"\"\n",
    "    Get and store the residual stream from the model for input tokens and generated tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        tokenized_prompt: Dictionary containing input_ids and attention_mask tensors\n",
    "        max_new_tokens: Maximum number of new tokens to generate (not including input tokens)\n",
    "        layer_indices: Optional list of specific layer indices to capture. If None, capture all layers.\n",
    "        save_path: Optional path to save the residual streams (as .pt file)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "            - all_residual_streams: Dict mapping from layer index to tensors of shape \n",
    "              [batch_size, seq_len, hidden_size] representing the residual stream at each layer\n",
    "            - generated_text: The text including the generated tokens\n",
    "            - token_ids: The token IDs including the generated tokens\n",
    "    \"\"\"\n",
    "    batch_size = tokenized_prompt[\"input_ids\"].shape[0]\n",
    "    device = tokenized_prompt[\"input_ids\"].device\n",
    "    input_length = tokenized_prompt[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Register hooks to capture residual streams\n",
    "    all_residual_streams = {}\n",
    "    hooks = []\n",
    "    \n",
    "    # Function to determine if we should capture a specific layer\n",
    "    def should_capture_layer(idx):\n",
    "        if layer_indices is None:\n",
    "            return True\n",
    "        return idx in layer_indices\n",
    "    \n",
    "    # Define hook function to capture residual stream\n",
    "    def get_activation(name, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            # For LLaMA models, the residual stream is the input to the layer's input_layernorm\n",
    "            if name == \"input_layernorm\":\n",
    "                # Store the residual stream (input[0])\n",
    "                if layer_idx not in all_residual_streams:\n",
    "                    all_residual_streams[layer_idx] = []\n",
    "                all_residual_streams[layer_idx].append(input[0].detach().cpu())\n",
    "            elif name == \"final_norm\":\n",
    "                # Final norm layer\n",
    "                if layer_idx not in all_residual_streams:\n",
    "                    all_residual_streams[layer_idx] = []\n",
    "                all_residual_streams[layer_idx].append(input[0].detach().cpu())\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks for each transformer layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        if should_capture_layer(i):\n",
    "            # Capture the input to the input_layernorm which is the residual stream\n",
    "            hook = layer.input_layernorm.register_forward_hook(get_activation(\"input_layernorm\", i))\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    # Also capture the final norm layer (after the last transformer layer)\n",
    "    final_hook = model.model.norm.register_forward_hook(\n",
    "        get_activation(\"final_norm\", len(model.model.layers))\n",
    "    )\n",
    "    hooks.append(final_hook)\n",
    "    \n",
    "    # Clone the tokenized_prompt to avoid modifying the original\n",
    "    generation_inputs = {\n",
    "        \"input_ids\": tokenized_prompt[\"input_ids\"].clone(),\n",
    "        \"attention_mask\": tokenized_prompt[\"attention_mask\"].clone(),\n",
    "    }\n",
    "    \n",
    "    # Generate text using the model's built-in generate method to handle stopping properly\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Use the model's generate method, which handles EOS tokens and stopping criteria\n",
    "            generation_output = model.generate(\n",
    "                **generation_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                output_hidden_states=False,  # We capture hidden states via hooks\n",
    "                # Set pad_token_id to EOS to avoid potential issues\n",
    "                pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 2\n",
    "            )\n",
    "            \n",
    "            # Get the generated sequence\n",
    "            generated_ids = generation_output.sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for layer_idx in all_residual_streams:\n",
    "        # Concatenate all the tensors for this layer along the sequence dimension\n",
    "        all_residual_streams[layer_idx] = torch.cat(all_residual_streams[layer_idx], dim=1)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    try:\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        # Print a preview of the generated text\n",
    "        preview = generated_text.replace('\\n', '\\\\n')\n",
    "        if len(preview) > 100:\n",
    "            preview = preview[:97] + '...'\n",
    "        input_tokens = input_length\n",
    "        output_tokens = generated_ids.shape[1]\n",
    "        new_tokens = output_tokens - input_tokens\n",
    "        print(f\"Generated text: {preview}\")\n",
    "        print(f\"Input tokens: {input_tokens}, New tokens: {new_tokens}, Total tokens: {output_tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding generated tokens: {e}\")\n",
    "        generated_text = f\"[Error decoding tokens: {generated_ids[0]}]\"\n",
    "    \n",
    "    # Store token ids for reference\n",
    "    token_ids = generated_ids.detach().cpu()\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_path is not None:\n",
    "        save_data = {\n",
    "            'residual_streams': all_residual_streams,\n",
    "            'generated_text': generated_text,\n",
    "            'token_ids': token_ids\n",
    "        }\n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            import os\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(save_data, save_path)\n",
    "            print(f\"Residual stream data saved to {save_path} \\n-----\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving residual stream data: {e}\")\n",
    "    \n",
    "    return all_residual_streams, generated_text, token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <｜begin▁of▁sentence｜><｜User｜>what is the capital of france?<｜Assistant｜><think>\n",
      "\n",
      "Generated text: <｜User｜>what is the capital of france?<｜Assistant｜><think>\\nOkay, so I need to figure out the cap...\n",
      "Input tokens: 13, New tokens: 157, Total tokens: 170\n",
      "Residual stream data saved to res-stream/test.pt: \n",
      "-----\n",
      "\n",
      "<｜User｜>what is the capital of france?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of France. Hmm, I remember from school that France is a country in Europe, but I'm not exactly sure about its capital. Let me think. I've heard of Paris being a major city there, right? It's often mentioned in history and culture. I think Paris is the capital, but wait, I should make sure. Maybe there's another city that's also significant? I don't recall another one. I think the government and important institutions are located in Paris. Oh yeah, the Eiffel Tower and the Louvre Museum are there, which are big landmarks. So, putting it all together, I'm pretty confident that Paris is the capital of France.\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prmpt = \"what is the capital of france?\"\n",
    "tokenized_prompt, formatted_prompt = tokenize_prompt(prmpt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "all_residual_streams, generated_text, token_ids = get_residual_stream(model, tokenizer, tokenized_prompt, max_new_tokens=1024, layer_indices=None, save_path=\"res-stream/test.pt\")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 7.8201e-05,  1.1635e-04, -2.3651e-04,  ...,  9.9182e-04,\n",
      "          -1.9670e-06, -1.2302e-04],\n",
      "         [ 7.8201e-05,  1.1635e-04, -2.3651e-04,  ...,  9.9182e-04,\n",
      "          -1.9670e-06, -1.2302e-04],\n",
      "         [-7.0572e-04, -9.4223e-04,  3.6430e-04,  ..., -1.9989e-03,\n",
      "           1.2817e-03, -3.9291e-04],\n",
      "         ...,\n",
      "         [-1.7319e-03,  1.3428e-03, -6.4468e-04,  ...,  1.1292e-02,\n",
      "           5.1880e-03,  1.1719e-02],\n",
      "         [-2.3804e-02,  1.0071e-02, -2.3651e-03,  ...,  6.2866e-03,\n",
      "          -3.9864e-04,  7.3853e-03],\n",
      "         [-1.4954e-03, -1.4191e-03,  5.8746e-04,  ...,  1.9684e-03,\n",
      "          -1.0824e-04,  1.9455e-04]]], dtype=torch.bfloat16)\n",
      "<｜User｜>what is the capital of france?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of France. Hmm, I remember from school that France is a country in Europe, but I'm not exactly sure about its capital. Let me think. I've heard of Paris being a major city there, right? It's often mentioned in history and culture. I think Paris is the capital, but wait, I should make sure. Maybe there's another city that's also significant? I don't recall another one. I think the government and important institutions are located in Paris. Oh yeah, the Eiffel Tower and the Louvre Museum are there, which are big landmarks. So, putting it all together, I'm pretty confident that Paris is the capital of France.\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n",
      "tensor([[128000, 128000, 128011,  12840,    374,    279,   6864,    315,  48687,\n",
      "             30, 128012, 128013,    198,  33413,     11,    779,    358,   1205,\n",
      "            311,   7216,    704,    279,   6864,    315,   9822,     13,  89290,\n",
      "             11,    358,   6227,    505,   2978,    430,   9822,    374,    264,\n",
      "           3224,    304,   4606,     11,    719,    358,   2846,    539,   7041,\n",
      "           2771,    922,   1202,   6864,     13,   6914,    757,   1781,     13,\n",
      "            358,   3077,   6755,    315,  12366,   1694,    264,   3682,   3363,\n",
      "           1070,     11,   1314,     30,   1102,    596,   3629,   9932,    304,\n",
      "           3925,    323,   7829,     13,    358,   1781,  12366,    374,    279,\n",
      "           6864,     11,    719,   3868,     11,    358,   1288,   1304,   2771,\n",
      "             13,  10926,   1070,    596,   2500,   3363,    430,    596,   1101,\n",
      "           5199,     30,    358,   1541,    956,  19635,   2500,    832,     13,\n",
      "            358,   1781,    279,   3109,    323,   3062,  14673,    527,   7559,\n",
      "            304,  12366,     13,   8840,  22371,     11,    279,    469,   3168,\n",
      "            301,  22703,    323,    279,   9928,  49606,  16730,    527,   1070,\n",
      "             11,    902,    527,   2466,  61024,     13,   2100,     11,  10917,\n",
      "            433,    682,   3871,     11,    358,   2846,   5128,  16913,    430,\n",
      "          12366,    374,    279,   6864,    315,   9822,    627, 128014,    271,\n",
      "            791,   6864,    315,   9822,    374,  12366,     13, 128001]])\n"
     ]
    }
   ],
   "source": [
    "print(all_residual_streams[0])\n",
    "print(generated_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_residual_stream(load_path):\n",
    "    \"\"\"\n",
    "    Load previously saved residual stream data from a file.\n",
    "    \n",
    "    Args:\n",
    "        load_path: Path to the saved residual stream file (.pt)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "    \"\"\"\n",
    "    data = torch.load(load_path)\n",
    "    return data['residual_streams'], data['generated_text'], data['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 7.8201e-05,  1.1635e-04, -2.3651e-04,  ...,  9.9182e-04,\n",
      "          -1.9670e-06, -1.2302e-04],\n",
      "         [ 7.8201e-05,  1.1635e-04, -2.3651e-04,  ...,  9.9182e-04,\n",
      "          -1.9670e-06, -1.2302e-04],\n",
      "         [-7.0572e-04, -9.4223e-04,  3.6430e-04,  ..., -1.9989e-03,\n",
      "           1.2817e-03, -3.9291e-04],\n",
      "         ...,\n",
      "         [-1.7319e-03,  1.3428e-03, -6.4468e-04,  ...,  1.1292e-02,\n",
      "           5.1880e-03,  1.1719e-02],\n",
      "         [-2.3804e-02,  1.0071e-02, -2.3651e-03,  ...,  6.2866e-03,\n",
      "          -3.9864e-04,  7.3853e-03],\n",
      "         [-1.4954e-03, -1.4191e-03,  5.8746e-04,  ...,  1.9684e-03,\n",
      "          -1.0824e-04,  1.9455e-04]]], dtype=torch.bfloat16)\n",
      "\n",
      "\n",
      "<｜User｜>what is the capital of france?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of France. Hmm, I remember from school that France is a country in Europe, but I'm not exactly sure about its capital. Let me think. I've heard of Paris being a major city there, right? It's often mentioned in history and culture. I think Paris is the capital, but wait, I should make sure. Maybe there's another city that's also significant? I don't recall another one. I think the government and important institutions are located in Paris. Oh yeah, the Eiffel Tower and the Louvre Museum are there, which are big landmarks. So, putting it all together, I'm pretty confident that Paris is the capital of France.\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "\n",
      "tensor([[128000, 128000, 128011,  12840,    374,    279,   6864,    315,  48687,\n",
      "             30, 128012, 128013,    198,  33413,     11,    779,    358,   1205,\n",
      "            311,   7216,    704,    279,   6864,    315,   9822,     13,  89290,\n",
      "             11,    358,   6227,    505,   2978,    430,   9822,    374,    264,\n",
      "           3224,    304,   4606,     11,    719,    358,   2846,    539,   7041,\n",
      "           2771,    922,   1202,   6864,     13,   6914,    757,   1781,     13,\n",
      "            358,   3077,   6755,    315,  12366,   1694,    264,   3682,   3363,\n",
      "           1070,     11,   1314,     30,   1102,    596,   3629,   9932,    304,\n",
      "           3925,    323,   7829,     13,    358,   1781,  12366,    374,    279,\n",
      "           6864,     11,    719,   3868,     11,    358,   1288,   1304,   2771,\n",
      "             13,  10926,   1070,    596,   2500,   3363,    430,    596,   1101,\n",
      "           5199,     30,    358,   1541,    956,  19635,   2500,    832,     13,\n",
      "            358,   1781,    279,   3109,    323,   3062,  14673,    527,   7559,\n",
      "            304,  12366,     13,   8840,  22371,     11,    279,    469,   3168,\n",
      "            301,  22703,    323,    279,   9928,  49606,  16730,    527,   1070,\n",
      "             11,    902,    527,   2466,  61024,     13,   2100,     11,  10917,\n",
      "            433,    682,   3871,     11,    358,   2846,   5128,  16913,    430,\n",
      "          12366,    374,    279,   6864,    315,   9822,    627, 128014,    271,\n",
      "            791,   6864,    315,   9822,    374,  12366,     13, 128001]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_58295/2934087460.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(load_path)\n"
     ]
    }
   ],
   "source": [
    "res_stream, generated_text, token_ids = load_residual_stream(\"res-stream/test.pt\")\n",
    "print(res_stream[0])\n",
    "print(\"\\n\")\n",
    "print(generated_text)\n",
    "print(\"\\n\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_res_stream(model, \n",
    "                    tokenizer, \n",
    "                    clean_path, \n",
    "                    corrupt_path, \n",
    "                    patch_layers, \n",
    "                    num_tokens_to_patch=None, \n",
    "                    target_token=\"<think>\", \n",
    "                    save_path=None, \n",
    "                    device=None, \n",
    "                    max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Patch residual streams from corrupted prompt into the clean prompt at specified layers and tokens,\n",
    "    supporting autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        clean_path: Path to the saved clean prompt residual stream file (.pt)\n",
    "        corrupt_path: Path to the saved corrupted prompt residual stream file (.pt)\n",
    "        patch_layers: List of layer indices to patch (e.g., [5, 6, 7] or range(32))\n",
    "        num_tokens_to_patch: Number of tokens to patch after the target token.\n",
    "                           If None, will patch from the target token to the end.\n",
    "        target_token: Token after which to start patching (default: \"<think>\")\n",
    "        save_path: Optional path to save the patched results (.pt)\n",
    "        device: Device to perform computation on\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (patched_text, original_clean_text, original_corrupt_text, patched_token_ids)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    # Set device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the saved residual streams\n",
    "    print(f\"Loading residual streams from {clean_path} and {corrupt_path}...\")\n",
    "    clean_streams, clean_text, clean_ids = load_residual_stream(clean_path)\n",
    "    corrupt_streams, corrupt_text, corrupt_ids = load_residual_stream(corrupt_path)\n",
    "    \n",
    "    print(f\"Clean text preview: {clean_text[:100]}...\")\n",
    "    print(f\"Corrupt text preview: {corrupt_text[:100]}...\")\n",
    "    \n",
    "    # Find the target token index in the clean prompt\n",
    "    if target_token:\n",
    "        # Check if target token is a string or a token ID\n",
    "        if isinstance(target_token, str):\n",
    "            # Find the token ID for the target token\n",
    "            target_token_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "            if len(target_token_ids) != 1:\n",
    "                print(f\"Warning: Target token '{target_token}' encoded to {len(target_token_ids)} tokens: {target_token_ids}\")\n",
    "            target_token_id = target_token_ids[0]\n",
    "        else:\n",
    "            target_token_id = target_token\n",
    "            \n",
    "        # Find the position of the target token in the clean_ids\n",
    "        target_positions = (clean_ids[0] == target_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(target_positions) == 0:\n",
    "            print(f\"Target token '{target_token}' not found in clean_ids. Will patch from the beginning.\")\n",
    "            patch_start_idx = 0\n",
    "        else:\n",
    "            # Take the first occurrence of the target token\n",
    "            patch_start_idx = target_positions[0].item()\n",
    "            print(f\"Found target token at position {patch_start_idx}\")\n",
    "    else:\n",
    "        # If no target token specified, start patching from the beginning\n",
    "        patch_start_idx = 0\n",
    "    \n",
    "    # Set the patch range\n",
    "    patch_start_idx = patch_start_idx if target_token else 0\n",
    "    \n",
    "    if num_tokens_to_patch is None:\n",
    "        # From target token to the end\n",
    "        patch_end_idx = min(clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    else:\n",
    "        # Patch specified number of tokens after the target token\n",
    "        patch_end_idx = min(patch_start_idx + num_tokens_to_patch, clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    \n",
    "    patch_tokens_range = (patch_start_idx, patch_end_idx)\n",
    "    \n",
    "    print(f\"Will patch tokens from position {patch_start_idx} to {patch_end_idx} ({patch_end_idx - patch_start_idx} tokens)\")\n",
    "    print(f\"Will patch layers: {patch_layers}\")\n",
    "    \n",
    "    # Validate layers\n",
    "    max_layer = max(clean_streams.keys())\n",
    "    patch_layers = [layer for layer in patch_layers if layer in clean_streams and layer in corrupt_streams]\n",
    "    if not patch_layers:\n",
    "        raise ValueError(f\"No valid layers to patch. Layers must be in range 0-{max_layer}\")\n",
    "    \n",
    "    # Create patched inputs based on clean prompt\n",
    "    input_ids = clean_ids[0, :patch_start_idx].unsqueeze(0).to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    \n",
    "    patched_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "    \n",
    "    # Set up patching state for autoregressive generation\n",
    "    current_token_pos = patch_start_idx - 1  # Position of the last token in inputs (-1 because we increment before patching)\n",
    "    max_corrupted_len = corrupt_ids.shape[1]\n",
    "    \n",
    "    # Set up patching progress tracking\n",
    "    from tqdm.auto import tqdm\n",
    "    expected_tokens_to_patch = min(patch_end_idx - patch_start_idx, max_new_tokens)\n",
    "    progress_bar = tqdm(total=expected_tokens_to_patch, desc=\"Patching tokens\", ncols=100)\n",
    "    \n",
    "    # Create a hook to patch each token during autoregressive generation\n",
    "    hooks = []\n",
    "    \n",
    "    def patch_hook(layer_idx):\n",
    "        patched_tokens_counter = 0  # Initialize counter inside hook closure\n",
    "        \n",
    "        def hook(module, inputs):\n",
    "            nonlocal current_token_pos, patched_tokens_counter\n",
    "            \n",
    "            # Get residual stream\n",
    "            res_stream = inputs[0]\n",
    "            batch_size, seq_len, hidden_dim = res_stream.shape\n",
    "            \n",
    "            # During autoregressive generation, we'll get one token at a time\n",
    "            # For the initial pass with multiple tokens, patch only the target position and beyond\n",
    "            if seq_len > 1:\n",
    "                # This is the initial forward pass with all input tokens\n",
    "                # We don't patch anything in the input prefix (before target token)\n",
    "                return inputs\n",
    "            else:\n",
    "                # This is autoregressive generation with one token at a time\n",
    "                # Increment position counter - this tells us which token we're generating\n",
    "                current_token_pos += 1\n",
    "                \n",
    "                # Check if this position should be patched\n",
    "                if patch_start_idx <= current_token_pos < patch_end_idx and current_token_pos < max_corrupted_len:\n",
    "                    # Get the corresponding position in the corrupted stream\n",
    "                    to_patch = corrupt_streams[layer_idx][0, current_token_pos, :].to(device)\n",
    "                    \n",
    "                    # Replace the residual stream for this token with the corrupted version\n",
    "                    res_stream[0, 0, :] = to_patch\n",
    "                    \n",
    "                    # Update progress tracking\n",
    "                    patched_tokens_counter += 1\n",
    "                    progress_bar.update(1)\n",
    "                    \n",
    "                    if current_token_pos % 10 == 0:  # Less frequent logging\n",
    "                        print(f\"Patched token at position {current_token_pos} in layer {layer_idx}\")\n",
    "            \n",
    "            # Return potentially modified inputs\n",
    "            return (res_stream,) + inputs[1:] if len(inputs) > 1 else (res_stream,)\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    # Register forward pre-hooks for each layer to patch\n",
    "    for layer_idx in patch_layers:\n",
    "        hook = model.model.layers[layer_idx].input_layernorm.register_forward_pre_hook(\n",
    "            patch_hook(layer_idx)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Generate text with patched residual streams\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Generate text with the patched residual stream\n",
    "            generation_output = model.generate(\n",
    "                **patched_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            patched_ids = generation_output.sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation with patched residual stream: {e}\")\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    finally:\n",
    "        # Remove all hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Close progress bar\n",
    "        progress_bar.close()\n",
    "    \n",
    "    # Decode the patched generation\n",
    "    patched_text = tokenizer.decode(patched_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- Patching Results ---\")\n",
    "    print(f\"Patched layers: {patch_layers}\")\n",
    "    print(f\"Patched token range: {patch_tokens_range}\")\n",
    "    print(f\"Generation length: {patched_ids.shape[1] - input_ids.shape[1]} new tokens\")\n",
    "    print(\"\\nOriginal clean output: \")\n",
    "    print(clean_text[:200] + \"...\" if len(clean_text) > 200 else clean_text)\n",
    "    print(\"\\nOriginal corrupt output: \")\n",
    "    print(corrupt_text[:200] + \"...\" if len(corrupt_text) > 200 else corrupt_text)\n",
    "    print(\"\\nPatched output: \")\n",
    "    print(patched_text[:200] + \"...\" if len(patched_text) > 200 else patched_text)\n",
    "    \n",
    "    # Save patched results if requested\n",
    "    if save_path:\n",
    "        patched_data = {\n",
    "            'patched_text': patched_text,\n",
    "            'clean_text': clean_text,\n",
    "            'corrupt_text': corrupt_text,\n",
    "            'patched_token_ids': patched_ids.cpu(),\n",
    "            'patch_info': {\n",
    "                'patch_layers': patch_layers,\n",
    "                'patch_tokens_range': patch_tokens_range,\n",
    "                'clean_path': clean_path,\n",
    "                'corrupt_path': corrupt_path\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(patched_data, save_path)\n",
    "            print(f\"Patched results saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving patched results: {e}\")\n",
    "    \n",
    "    return patched_text, clean_text, corrupt_text, patched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <｜begin▁of▁sentence｜><｜User｜>What is the capital of France?<｜Assistant｜><think>\n",
      "\n",
      "Generated text: <｜User｜>What is the capital of France?<｜Assistant｜><think>\\nOkay, so I need to figure out the cap...\n",
      "Input tokens: 13, New tokens: 159, Total tokens: 172\n",
      "Residual stream data saved to res-stream/clean-prompt-test.pt \n",
      "-----\n",
      "\n",
      "<｜User｜>What is the capital of France?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of France. Hmm, I remember from school that France is a country in Europe, right? I think their capital is a pretty big city. I've heard Paris mentioned a lot when people talk about France. Wait, is it called Paris? Yeah, that sounds familiar. I think Paris is known for things like the Eiffel Tower and the Louvre Museum. I've seen pictures of those. Maybe I should double-check to make sure I'm not confusing it with another country, but I'm pretty sure it's Paris. I don't recall any other major cities in France that are as famous as Paris. So, I'm going to go with Paris as the capital of France.\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "clean_prompt = \"What is the capital of France?\"\n",
    "tokenized_prompt, formatted_prompt = tokenize_prompt(clean_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "all_residual_streams, generated_text, token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    tokenized_prompt, \n",
    "                                                                    max_new_tokens=512, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/clean-prompt-test.pt\")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <｜begin▁of▁sentence｜><｜User｜>What is the capital of USA?<｜Assistant｜><think>\n",
      "\n",
      "Generated text: <｜User｜>What is the capital of USA?<｜Assistant｜><think>\\nOkay, so I need to figure out the capita...\n",
      "Input tokens: 13, New tokens: 506, Total tokens: 519\n",
      "Residual stream data saved to res-stream/corrupted-prompt-test.pt \n",
      "-----\n",
      "\n",
      "<｜User｜>What is the capital of USA?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of the USA. Hmm, I remember hearing it's Washington, but I'm not entirely sure. Let me think through this step by step.\n",
      "\n",
      "First, I know that the USA is a country made up of 50 states. Each state has its own capital, but the capital of the country itself is a different place. I think it's somewhere in the north-east part of the country. Maybe New York? Wait, no, I think New York is a state, and its capital is Albany. But I'm pretty sure the federal capital isn't there.\n",
      "\n",
      "I've heard the term \"District of Columbia\" before. Maybe that's where the capital is. DC, as people sometimes call it, stands out because it's a federal district, not part of any state. I think it's in the Mid-Atlantic region. Washington D.C. sounds familiar because of the Washington Monument and all the museums there.\n",
      "\n",
      "Let me try to recall any historical context. I remember learning in school that Washington, D.C., was established as the capital by an act of Congress. It was chosen because of its strategic location along the Potomac River, which made it easier to defend and connect to both the Atlantic coast and the Midwest. That makes sense because the Potomac is a major waterway.\n",
      "\n",
      "I also remember that the White House is in Washington, D.C., which is the president's official residence. The Capitol building there is where Congress meets. So, all the important government stuff happens there. That reinforces the idea that Washington, D.C. is the capital.\n",
      "\n",
      "Wait, could it be another city? Maybe not. I don't recall any other city being referred to as the capital. There's also the concept of the \"District of Columbia,\" which is the federal district where the capital is located. So, putting it all together, it's Washington in the District of Columbia.\n",
      "\n",
      "I think I'm confident now. The capital of the USA is Washington, D.C.\n",
      "</think>\n",
      "\n",
      "The capital of the United States is Washington, D.C., officially known as the District of Columbia. It is located in the northeast region of the country, distinguished by landmarks like the White House and the Capitol building. Established by an act of Congress, Washington, D.C., was chosen for its strategic location along the Potomac River, facilitating defense and connections to the Atlantic coast and Midwest. Thus, Washington, D.C., is the federal capital where the government operates.\n"
     ]
    }
   ],
   "source": [
    "corrupted_prompt = \"What is the capital of USA?\"\n",
    "tokenized_prompt, formatted_prompt = tokenize_prompt(corrupted_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "all_residual_streams, generated_text, token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    tokenized_prompt, \n",
    "                                                                    max_new_tokens=512, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/corrupted-prompt-test.pt\")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading residual streams from res-stream/clean-prompt-test.pt and res-stream/corrupted-prompt-test.pt...\n",
      "Clean text preview: <｜User｜>What is the capital of France?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital...\n",
      "Corrupt text preview: <｜User｜>What is the capital of USA?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of...\n",
      "Found target token at position 11\n",
      "Will patch tokens from position 11 to 75 (64 tokens)\n",
      "Will patch layers: [30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/sgrd2bsj1msgt6bs3kfgj9hw0000gn/T/ipykernel_60284/2934087460.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(load_path)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d484c7a47c84b47b2ae95b97b68ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Patching tokens:   0%|                                                       | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched token at position 20 in layer 30\n",
      "Patched token at position 30 in layer 30\n",
      "Patched token at position 40 in layer 30\n",
      "Patched token at position 50 in layer 30\n",
      "Patched token at position 60 in layer 30\n",
      "Patched token at position 70 in layer 30\n",
      "\n",
      "--- Patching Results ---\n",
      "Patched layers: [30]\n",
      "Patched token range: (11, 75)\n",
      "Generation length: 512 new tokens\n",
      "\n",
      "Original clean output: \n",
      "<｜User｜>What is the capital of France?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of France. Hmm, I remember from school that France is a country in Europe, right? I think their cap...\n",
      "\n",
      "Original corrupt output: \n",
      "<｜User｜>What is the capital of USA?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of the USA. Hmm, I remember hearing it's Washington, but I'm not entirely sure. Let me think through t...\n",
      "\n",
      "Patched output: \n",
      "<｜User｜>What is the capital of France?<｜Assistant｜><think>\n",
      "\n",
      "Okay, so I need to figure out the capital of the USA. Hmm, I think that it's Washington, but I'm not  sure. Let me think through this step b...\n",
      "Patched results saved to res-stream/patched-prompt-test.pt\n"
     ]
    }
   ],
   "source": [
    "patched_text, clean_text, corrupt_text, patched_ids = patch_res_stream(model, \n",
    "                                                                        tokenizer, \n",
    "                                                                        clean_path = \"res-stream/clean-prompt-test.pt\", \n",
    "                                                                        corrupt_path = \"res-stream/corrupted-prompt-test.pt\", \n",
    "                                                                        patch_layers = [30], \n",
    "                                                                        num_tokens_to_patch=64, \n",
    "                                                                        target_token=\"<think>\", \n",
    "                                                                        save_path=\"res-stream/patched-prompt-test.pt\", \n",
    "                                                                        device=None, \n",
    "                                                                        max_new_tokens=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrupted text:\n",
      "<｜User｜>What is the capital of USA?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of the USA. Hmm, I remember hearing it's Washington, but I'm not entirely sure. Let me think through this step by step.\n",
      "\n",
      "First, I know that the USA is a country made up of 50 states. Each state has its own capital, but the capital of the country itself is a different place. I think it's somewhere in the north-east part of the country. Maybe New York? Wait, no, I think New York is a state, and its capital is Albany. But I'm pretty sure the federal capital isn't there.\n",
      "\n",
      "I've heard the term \"District of Columbia\" before. Maybe that's where the capital is. DC, as people sometimes call it, stands out because it's a federal district, not part of any state. I think it's in the Mid-Atlantic region. Washington D.C. sounds familiar because of the Washington Monument and all the museums there.\n",
      "\n",
      "Let me try to recall any historical context. I remember learning in school that Washington, D.C., was established as the capital by an act of Congress. It was chosen because of its strategic location along the Potomac River, which made it easier to defend and connect to both the Atlantic coast and the Midwest. That makes sense because the Potomac is a major waterway.\n",
      "\n",
      "I also remember that the White House is in Washington, D.C., which is the president's official residence. The Capitol building there is where Congress meets. So, all the important government stuff happens there. That reinforces the idea that Washington, D.C. is the capital.\n",
      "\n",
      "Wait, could it be another city? Maybe not. I don't recall any other city being referred to as the capital. There's also the concept of the \"District of Columbia,\" which is the federal district where the capital is located. So, putting it all together, it's Washington in the District of Columbia.\n",
      "\n",
      "I think I'm confident now. The capital of the USA is Washington, D.C.\n",
      "</think>\n",
      "\n",
      "The capital of the United States is Washington, D.C., officially known as the District of Columbia. It is located in the northeast region of the country, distinguished by landmarks like the White House and the Capitol building. Established by an act of Congress, Washington, D.C., was chosen for its strategic location along the Potomac River, facilitating defense and connections to the Atlantic coast and Midwest. Thus, Washington, D.C., is the federal capital where the government operates.\n",
      "\n",
      "---\n",
      "clean text:\n",
      "<｜User｜>What is the capital of France?<｜Assistant｜><think>\n",
      "Okay, so I need to figure out the capital of France. Hmm, I remember from school that France is a country in Europe, right? I think their capital is a pretty big city. I've heard Paris mentioned a lot when people talk about France. Wait, is it called Paris? Yeah, that sounds familiar. I think Paris is known for things like the Eiffel Tower and the Louvre Museum. I've seen pictures of those. Maybe I should double-check to make sure I'm not confusing it with another country, but I'm pretty sure it's Paris. I don't recall any other major cities in France that are as famous as Paris. So, I'm going to go with Paris as the capital of France.\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "---\n",
      "patched text:\n",
      "<｜User｜>What is the capital of France?<｜Assistant｜><think>\n",
      "\n",
      "Okay, so I need to figure out the capital of the USA. Hmm, I think that it's Washington, but I'm not  sure. Let me think through this step by step.\n",
      "\n",
      "First, I know the the USA is a country made up of 50 states, Each state has its own capital, but the capital of the country is different. I remember hearing about Washington D.C. when I was younger. I think that stands for District of Columbia. So, Washington D.C. is a district, not a state, but it's the capital. \n",
      "\n",
      "Wait, but I'm a bit confused because I also remember hearing about other capitals like New York and California having their own capitals, like Albany and Sacramento. But no, those are the capitals of their respective states, not the country. So, the capital of the USA should be the capital of the federal government, which is Washington D.C. \n",
      "\n",
      "I also recall that the White House is in Washington D.C., which is the president's residence. That makes sense because the president is the head of the country, so their home office is in the capital. So, if I'm looking for the capital, it's where the government is centered, which is Washington D.C.\n",
      "\n",
      "I think I'm pretty confident now that the capital of the USA is Washington D.C. But just to make sure, maybe I can think about famous landmarks or events that happen there. The Lincoln Memorial, the Pentagon, and the Smithsonian Museums are all in Washington D.C., which are significant American landmarks. Also, many important events, like the March on Washington, took place there. So, that reinforces the idea that it's the capital.\n",
      "\n",
      "I don't think it's another city like Los Angeles or New York City because those are more known for other things, like Hollywood or Wall Street. Those cities are big and have their own governments, but the federal government's capital is a separate area, the District of Columbia. So, I'm pretty sure the capital is Washington D.C.\n",
      "\n",
      "Another way to check might be to think about where the president gives speeches or makes important decisions. The State of the Union address is delivered in Washington D.C., which is the capital. So, that's another indication that it's the country's capital.\n",
      "\n",
      "Wait, I also remember that the term \"District of Columbia\" is often abbreviated as D.C., so Washington D.C. is the capital city. That makes sense because D.C. is a district, not a state, but it's the\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"corrupted text:\")\n",
    "print(corrupt_text)\n",
    "print(\"\\n---\")\n",
    "print(\"clean text:\")\n",
    "print(clean_text)\n",
    "print(\"\\n---\")\n",
    "print(\"patched text:\")\n",
    "print(patched_text)\n",
    "print(\"\\n---\")\n",
    "# print(patched_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
