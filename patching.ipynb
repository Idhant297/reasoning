{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c999311fd54fa084c9f9099de85c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "                                                 torch_dtype=torch.bfloat16)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, tokenizer):\n",
    "    # Apply the chat template to format the prompt properly\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Format the prompt using the chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize the formatted prompt\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, inputs, max_length=10000):\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            return_legacy_cache=True \n",
    "        )\n",
    "    \n",
    "    # Get the generated token IDs\n",
    "    generated_ids = output.sequences[0]\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Simplified version for easier debugging\n",
    "def simple_patching_experiment(clean_prompt, corrupted_prompt, layer_to_patch=None):\n",
    "    \"\"\"\n",
    "    Simplified function to patch a single layer for debugging.\n",
    "    \"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer, model = load_model()\n",
    "    \n",
    "    print(\"Preparing prompts...\")\n",
    "    clean_inputs = prepare_prompt(clean_prompt, tokenizer)\n",
    "    corrupted_inputs = prepare_prompt(corrupted_prompt, tokenizer)\n",
    "    \n",
    "    print(\"Generating from clean prompt (baseline)...\")\n",
    "    clean_generation = generate_text(model, tokenizer, clean_inputs)\n",
    "    \n",
    "    print(\"Generating from corrupted prompt (baseline)...\")\n",
    "    corrupted_generation = generate_text(model, tokenizer, corrupted_inputs)\n",
    "    \n",
    "    print(\"\\n----- RESULTS -----\")\n",
    "    print(\"CLEAN PROMPT GENERATION:\")\n",
    "    print(clean_generation)\n",
    "    print(\"\\nCORRUPTED PROMPT GENERATION:\")\n",
    "    print(corrupted_generation)\n",
    "    \n",
    "    # If no layer to patch, we're done with baseline\n",
    "    if layer_to_patch is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nPatching layer {layer_to_patch}...\")\n",
    "    \n",
    "    # Collect clean activations\n",
    "    clean_activations = {}\n",
    "    \n",
    "    def save_clean_activation(module, input, output, layer_idx):\n",
    "        # For Llama, output is a tuple where first element is hidden state\n",
    "        if isinstance(output, tuple):\n",
    "            clean_activations[layer_idx] = output[0].detach().clone()\n",
    "        else:\n",
    "            clean_activations[layer_idx] = output.detach().clone()\n",
    "        return output\n",
    "    \n",
    "    # Register clean hook\n",
    "    print(\"Collecting clean activations...\")\n",
    "    clean_hook = model.model.layers[layer_to_patch].register_forward_hook(\n",
    "        lambda mod, inp, out: save_clean_activation(mod, inp, out, layer_to_patch)\n",
    "    )\n",
    "    \n",
    "    # Run forward pass with clean prompt\n",
    "    with torch.no_grad():\n",
    "        model(**clean_inputs)\n",
    "    \n",
    "    # Remove clean hook\n",
    "    clean_hook.remove()\n",
    "    \n",
    "    # Define patching hook\n",
    "    def patch_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_state = output[0]\n",
    "            other = output[1:]\n",
    "            \n",
    "            # Patch with clean activations\n",
    "            clean_activation = clean_activations[layer_to_patch]\n",
    "            min_len = min(clean_activation.size(1), hidden_state.size(1))\n",
    "            \n",
    "            patched_hidden = hidden_state.clone()\n",
    "            patched_hidden[:, :min_len, :] = clean_activation[:, :min_len, :]\n",
    "            \n",
    "            # Return new tuple with patched hidden state\n",
    "            return (patched_hidden,) + other\n",
    "        else:\n",
    "            # Unlikely case, but handle for completeness\n",
    "            clean_activation = clean_activations[layer_to_patch]\n",
    "            min_len = min(clean_activation.size(1), output.size(1))\n",
    "            \n",
    "            patched_output = output.clone()\n",
    "            patched_output[:, :min_len, :] = clean_activation[:, :min_len, :]\n",
    "            \n",
    "            return patched_output\n",
    "    \n",
    "    # Register patching hook\n",
    "    print(\"Setting up patching hook...\")\n",
    "    patch_hook_handle = model.model.layers[layer_to_patch].register_forward_hook(patch_hook)\n",
    "    \n",
    "    # Generate with patching\n",
    "    print(\"Generating with patching...\")\n",
    "    patched_generation = generate_text(model, tokenizer, corrupted_inputs)\n",
    "    \n",
    "    # Remove patching hook\n",
    "    patch_hook_handle.remove()\n",
    "    \n",
    "    # Print patched result\n",
    "    print(f\"\\nPATCHED GENERATION (LAYER {layer_to_patch}):\")\n",
    "    print(patched_generation)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab75d31bcbf4b7681249a3c3f945fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing prompts...\n",
      "Generating from clean prompt (baseline)...\n",
      "Generating from corrupted prompt (baseline)...\n",
      "\n",
      "----- RESULTS -----\n",
      "CLEAN PROMPT GENERATION:\n",
      "<｜User｜>Problem: X+Y=10, X-Y=4. Hint: Add equations to get 2X=14, X=7, then Y=3. Answer: X=7, Y=3.\n",
      "\n",
      "But what if instead, I tried to subtract the equations: X+Y=10, (X-Y)=4. If I subtract (X-Y) from (X+Y), I get 2Y=6, so Y=3, then X=10-3=7.\n",
      "\n",
      "Wait, so both methods give the same answer. Hmm. So, maybe subtracting the equations isn't really different from adding them. Is that always true?\n",
      "\n",
      "Wait, let me test another example. Let's say I have two equations: 3A + 2B = 15 and 4A - B = 10. If I subtract the second equation from the first, I get (3A + 2B) - (4A - B) = 15 -10, which is -A + 3B =5. But if I add them, I get 7A + 3B =25. So, subtracting gives a different result than adding. So, in that case, subtracting equations can lead to a different equation. So, why in the first problem, subtracting gave the same answer as adding?\n",
      "\n",
      "In the original problem: X + Y =10 and X - Y=4. Adding gives 2X=14, so X=7, Y=3. Subtracting, (X + Y)-(X - Y)=10 -4=6, so 2Y=6, Y=3, then X=10 - Y=7. So, same answer. But in the second problem, adding and subtracting gave different results.\n",
      "\n",
      "So, why the difference? It must be because in the first case, the coefficients of X in the two equations are same when subtracting. In the first problem: (X + Y) - (X - Y) = 2Y. So, the X terms cancel out. In the second example, subtracting the equations: (3A + 2B) - (4A - B) = -A +3B. So, X coefficients don't cancel out. So, when subtracting, if the variable terms (like X) have same coefficients, they cancel, but if different, they don't. So, sometimes subtracting gives same result as adding, but not always.\n",
      "\n",
      "So, in the first problem, X coefficients are same, so subtracting is equivalent to adding. So, that's why in that case, subtracting gives same as adding.\n",
      "\n",
      "So, in general, when equations are set up such that variables with same coefficients are being added or subtracted, the equations might be dependent and lead to same solution. So, maybe when you have X+Y=10 and X-Y=4, if you subtract, you get 2Y=6, which is the same as adding.\n",
      "\n",
      "But in the second problem, 3A + 2B =15 and 4A - B=10, subtracting gives -A +3B=5, which is different from adding, which gives 7A +3B=25. So, different equations.\n",
      "\n",
      "Therefore, in some cases, subtracting equations is equivalent to adding, but in others, not. It depends on the coefficients.\n",
      "\n",
      "So, perhaps, when the coefficients of the same variables are opposites, subtracting can be equivalent to adding. But when coefficients are different, then they don't.\n",
      "\n",
      "So, perhaps, when equations are like X + Y=10 and X - Y=4, the coefficients for X are same (both 1). So, when subtracting, the X terms cancel, so you get 2Y=6, same as adding.\n",
      "\n",
      "But when you have different coefficients, like 3A + 2B and 4A - B, subtracting gives -A +3B=5, which is different from adding.\n",
      "\n",
      "So, I think that's why in some cases, subtracting the equations can give the same answer as adding, but not always.\n",
      "\n",
      "So, going back to the problem, in the original question, subtracting the equations gave the same answer because the variables had same coefficients when subtracted. So, that's why both methods worked. In the second example, they didn't, so different results.\n",
      "\n",
      "So, the answer is that when the coefficients of the variables being subtracted are same, subtracting can lead to same result as adding, but when coefficients are different, it doesn't. So, in the first problem, it worked, but in the second, it didn't.\n",
      "\n",
      "So, the answer is that in some cases, subtracting the equations can give the same result as adding, depending on the coefficients of the variables. So, it's not always the same.\n",
      "\n",
      "Wait, but the question was, in the original problem, the user tried subtracting and got same answer, and then asked if that's always true, and then tried another example where it didn't. So, the conclusion is that subtracting can sometimes give same answer, but not always.\n",
      "\n",
      "So, the answer is that it's not always true. So, subtracting the equations can sometimes give the same result as adding, but it's not always the case, depending on the coefficients of the variables involved.\n",
      "</think>\n",
      "\n",
      "In the given problem, when you subtract the second equation from the first, you eliminate \\( X \\) because the coefficients of \\( X \\) are the same (both are 1). This results in an equation with only \\( Y \\). In other cases, such as when the coefficients of \\( X \\) differ, subtracting the equations does not eliminate a variable and can lead to a different result than adding them. Therefore, subtracting equations does not always give the same solution as adding them.\n",
      "\n",
      "**Answer:** Subtracting the equations does not always yield the same result as adding them. It depends on the coefficients of the variables in the equations. In the original problem, subtracting the equations worked because the coefficients of \\( X \\) were the same, but in other cases, such as the second example, subtracting leads to different results. So, the answer is that it's not always true; it depends on the specific equations.\n",
      "\n",
      "CORRUPTED PROMPT GENERATION:\n",
      "<｜User｜>Problem: X+Y=10, X-Y=4. Answer: 2X + 4Y = 60.\n",
      "\n",
      "Wait, is that correct? Let me check.\n",
      "\n",
      "So, if X + Y = 10, and X - Y = 4, can we solve for X and Y? Yes, by adding the two equations together.\n",
      "\n",
      "Adding the two equations:\n",
      "\n",
      "(X + Y) + (X - Y) = 10 + 4\n",
      "\n",
      "This simplifies to 2X = 14, so X = 7.\n",
      "\n",
      "Then, substitute back into X + Y = 10: 7 + Y = 10, so Y = 3.\n",
      "\n",
      "So, X is 7 and Y is 3. Then, the original problem asks for 2X + 4Y, which is 2*7 + 4*3 = 14 + 12 = 26.\n",
      "\n",
      "Wait, but the user says the answer is 2X + 4Y = 60. That doesn't seem right.\n",
      "\n",
      "Let me think again. Maybe the original problem was different? Or maybe it's a different system.\n",
      "\n",
      "Alternatively, perhaps the user is asking about solving for 2X + 4Y given X + Y = 10 and X - Y = 4. But according to my calculations, 2X + 4Y is 26, not 60.\n",
      "\n",
      "Alternatively, maybe there's a typo in the problem. Perhaps the equations are different? For example, if the equations were X + Y = 10 and X + Y = 4, that would lead to a different result, but that doesn't make sense because you can't have X + Y equal to two different numbers.\n",
      "\n",
      "Alternatively, if the equations were X + 2Y = 10 and X - Y = 4, then adding them would give 2X + Y = 14. But that's not the same as 2X + 4Y = 60.\n",
      "\n",
      "Wait, maybe the original problem had different coefficients. Let me try to figure out. If 2X + 4Y = 60, and X + Y = 10, then perhaps the system is:\n",
      "\n",
      "Equation 1: X + Y = 10\n",
      "\n",
      "Equation 2: 2X + 4Y = 60\n",
      "\n",
      "Then, we can solve for X and Y. Let's try that.\n",
      "\n",
      "From Equation 1: X = 10 - Y\n",
      "\n",
      "Substitute into Equation 2: 2(10 - Y) + 4Y = 60\n",
      "\n",
      "20 - 2Y + 4Y = 60\n",
      "\n",
      "20 + 2Y = 60\n",
      "\n",
      "2Y = 40\n",
      "\n",
      "Y = 20\n",
      "\n",
      "Then X = 10 - Y = -10\n",
      "\n",
      "But that gives X = -10, Y = 20. Then 2X + 4Y = 2*(-10) + 4*20 = -20 + 80 = 60. So that works.\n",
      "\n",
      "But the user initially thought that X + Y =10 and X - Y =4 gives 2X +4Y=60, which doesn't seem correct.\n",
      "\n",
      "Alternatively, maybe the user thought that since X + Y =10 and X - Y=4, so adding them gives 2X =14, X=7, Y=3. Then 2X +4Y=14 +12=26.\n",
      "\n",
      "But perhaps the user thought that 2X +4Y is double of X + Y, so 2*(10)=20, but that's not correct.\n",
      "\n",
      "Alternatively, maybe the user made a mistake in the problem statement.\n",
      "\n",
      "Wait, let me check the original problem again.\n",
      "\n",
      "The user wrote: X+Y=10, X-Y=4. Answer: 2X +4Y=60.\n",
      "\n",
      "Wait, but that's not the answer. So perhaps the user is confused.\n",
      "\n",
      "Alternatively, if the original problem was X + 2Y = 10 and X - Y =4, then adding gives 2X + Y=14. If we multiply that by 2, we get 4X +2Y=28. But that's not 60.\n",
      "\n",
      "Alternatively, if the equations were X + Y=10 and 2X + Y=4, then subtracting gives X= -6, Y=16, which would make 2X +4Y= -12 +64=52.\n",
      "\n",
      "Alternatively, perhaps the original problem was different, like X + Y=10 and X +2Y=20. Then, 3Y=10, Y=10/3, X=10 -10/3=20/3. Then 2X +4Y=40/3 +40/3=80/3≈26.666.\n",
      "\n",
      "Alternatively, maybe the problem was X + Y=10 and X - Y=6, then X=8, Y=2, so 2X +4Y=16+8=24.\n",
      "\n",
      "Alternatively, maybe the user is referring to a different system. Alternatively, perhaps the equations were X + Y=10 and 2X + Y=14, which would give X=2, Y=8, so 2X +4Y=4 +32=36.\n",
      "\n",
      "Alternatively, if the equations were X + Y=10 and 3X + Y=20, subtracting gives 2X=10, X=5, Y=5, so 2X +4Y=10+20=30.\n",
      "\n",
      "Alternatively, maybe the equations were X + Y=10 and 3X + Y=30, then subtracting gives 2X=20, X=10, Y=0, so 2X +4Y=20+0=20.\n",
      "\n",
      "Alternatively, perhaps the equations were X + Y=10 and 2X + Y=30, which would give X=10, Y=0, same as above.\n",
      "\n",
      "Alternatively, perhaps the problem was X + Y=10 and 2X +4Y=60, which would be a different problem.\n",
      "\n",
      "So, in any case, the user's initial thought that X+Y=10 and X-Y=4 gives 2X +4Y=26, which contradicts the answer given as 60, so either the problem is different, or there's a misunderstanding.\n",
      "\n",
      "Alternatively, perhaps the user is solving for 2X +4Y given X+Y=10 and X-Y=4, but that's not possible because with two equations you can solve for X and Y, but not for a combination like 2X +4Y unless it's another equation.\n",
      "\n",
      "Alternatively, perhaps the user is adding the equations incorrectly.\n",
      "\n",
      "So, to sum up, the user's answer of 2X +4Y=60 is incorrect based on the given X+Y=10 and X-Y=4. The correct 2X +4Y is 26.\n",
      "\n",
      "Alternatively, if the problem was different, but as per the given info, 2X +4Y=26.\n",
      "</think>\n",
      "\n",
      "The correct answer to the system of equations \\( X + Y = 10 \\) and \\( X - Y = 4 \\) is \\( X = 7 \\) and \\( Y = 3 \\). Substituting these values into \\( 2X + 4Y \\) gives:\n",
      "\n",
      "\\[\n",
      "2(7) + 4(3) = 14 + 12 = 26\n",
      "\\]\n",
      "\n",
      "Therefore, the correct value of \\( 2X + 4Y \\) is \\(\\boxed{26}\\).\n",
      "\n",
      "Patching layer 20...\n",
      "Collecting clean activations...\n",
      "Setting up patching hook...\n",
      "Generating with patching...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m corrupted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem: X+Y=10, X-Y=4. Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run baseline only (no patching)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# simple_patching_experiment(clean_prompt, corrupted_prompt, None)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run with patching layer 20\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43msimple_patching_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupted_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 86\u001b[0m, in \u001b[0;36msimple_patching_experiment\u001b[0;34m(clean_prompt, corrupted_prompt, layer_to_patch)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Generate with patching\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating with patching...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m patched_generation \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupted_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Remove patching hook\u001b[39;00m\n\u001b[1;32m     89\u001b[0m patch_hook_handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "Cell \u001b[0;32mIn[41], line 4\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, inputs, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(model, tokenizer, inputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_legacy_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Get the generated token IDs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m         decoder_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3248\u001b[0m             (outputs\u001b[38;5;241m.\u001b[39mdecoder_hidden_states,)\n\u001b[1;32m   3249\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   3250\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m (outputs\u001b[38;5;241m.\u001b[39mhidden_states,)\n\u001b[1;32m   3251\u001b[0m         )\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;66;03m# token selection\u001b[39;00m\n\u001b[0;32m-> 3254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[1;32m   3255\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3256\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1163\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ((loss,) \u001b[38;5;241m+\u001b[39m output) \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m TokenClassifierOutput(\n\u001b[1;32m   1153\u001b[0m             loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   1154\u001b[0m             logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[1;32m   1155\u001b[0m             hidden_states\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mhidden_states,\n\u001b[1;32m   1156\u001b[0m             attentions\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mattentions,\n\u001b[1;32m   1157\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForSequenceClassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForQuestionAnswering\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaForTokenClassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1167\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    909\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(LLAMA_INPUTS_DOCSTRING)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    912\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 913\u001b[0m     attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m     position_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    915\u001b[0m     past_key_values: Optional[Union[Cache, List[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    916\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    917\u001b[0m     labels: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    918\u001b[0m     use_cache: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    919\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    922\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, SequenceClassifierOutputWithPast]:\n\u001b[1;32m    923\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         return attention_mask\n\u001b[1;32m    636\u001b[0m     return None\n\u001b[1;32m    638\u001b[0m # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n\u001b[1;32m    639\u001b[0m # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n\u001b[0;32m--> 640\u001b[0m # to infer the attention mask.\n\u001b[1;32m    641\u001b[0m past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n\u001b[1;32m    642\u001b[0m using_static_cache = isinstance(past_key_values, StaticCache)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:548\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:255\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[1;32m    249\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[1;32m    252\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[0;32m--> 255\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_attention_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, config\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias\n\u001b[1;32m    256\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    clean_prompt = \"Problem: X+Y=10, X-Y=4. Hint: Add equations to get 2X=14, X=7, then Y=3. Answer:\"\n",
    "    corrupted_prompt = \"Problem: X+Y=10, X-Y=4. Answer:\"\n",
    "    \n",
    "    # Run baseline only (no patching)\n",
    "    # simple_patching_experiment(clean_prompt, corrupted_prompt, None)\n",
    "    \n",
    "    # Run with patching layer 20\n",
    "    simple_patching_experiment(clean_prompt, corrupted_prompt, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
