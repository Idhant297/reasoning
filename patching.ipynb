{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Print CUDA details\n",
    "        print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
    "        print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved()/1024**2:.2f}MB\")\n",
    "        return device\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    dtype = torch.bfloat16\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,  # Optimize memory usage during loading\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt: str, tokenizer) -> str:\n",
    "    \"\"\"Format the prompt using the chat template if available\"\"\"\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        return formatted_prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(prompt: str, tokenizer, device=None):\n",
    "    \"\"\"Tokenize a prompt and prepare model inputs\"\"\"\n",
    "    # Get appropriate device\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    # Format prompt\n",
    "    formatted_prompt = prepare_prompt(prompt, tokenizer)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_prompt = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device\n",
    "    tokenized_prompt = {k: v.to(device) for k, v in tokenized_prompt.items()}\n",
    "    \n",
    "    print(f\"Prompt: {formatted_prompt}\")\n",
    "    \n",
    "    return tokenized_prompt, formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_stream(model, \n",
    "                        tokenizer, \n",
    "                        tokenized_prompt, \n",
    "                        max_new_tokens=20, \n",
    "                        layer_indices=None, \n",
    "                        save_path=None):\n",
    "    \"\"\"\n",
    "    Get and store the residual stream from the model for input tokens and generated tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        tokenized_prompt: Dictionary containing input_ids and attention_mask tensors\n",
    "        max_new_tokens: Maximum number of new tokens to generate (not including input tokens)\n",
    "        layer_indices: Optional list of specific layer indices to capture. If None, capture all layers.\n",
    "        save_path: Optional path to save the residual streams (as .pt file)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "            - all_residual_streams: Dict mapping from layer index to tensors of shape \n",
    "              [batch_size, seq_len, hidden_size] representing the residual stream at each layer\n",
    "            - generated_text: The text including the generated tokens\n",
    "            - token_ids: The token IDs including the generated tokens\n",
    "    \"\"\"\n",
    "    batch_size = tokenized_prompt[\"input_ids\"].shape[0]\n",
    "    device = tokenized_prompt[\"input_ids\"].device\n",
    "    input_length = tokenized_prompt[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Register hooks to capture residual streams\n",
    "    all_residual_streams = {}\n",
    "    hooks = []\n",
    "    \n",
    "    # Function to determine if we should capture a specific layer\n",
    "    def should_capture_layer(idx):\n",
    "        if layer_indices is None:\n",
    "            return True\n",
    "        return idx in layer_indices\n",
    "    \n",
    "    # Define hook function to capture residual stream\n",
    "    def get_activation(name, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            # For LLaMA models, the residual stream is the input to the layer's input_layernorm\n",
    "            if name == \"input_layernorm\":\n",
    "                # Store the residual stream (input[0])\n",
    "                if layer_idx not in all_residual_streams:\n",
    "                    all_residual_streams[layer_idx] = []\n",
    "                all_residual_streams[layer_idx].append(input[0].detach().cpu())\n",
    "            elif name == \"final_norm\":\n",
    "                # Final norm layer\n",
    "                if layer_idx not in all_residual_streams:\n",
    "                    all_residual_streams[layer_idx] = []\n",
    "                all_residual_streams[layer_idx].append(input[0].detach().cpu())\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks for each transformer layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        if should_capture_layer(i):\n",
    "            # Capture the input to the input_layernorm which is the residual stream\n",
    "            hook = layer.input_layernorm.register_forward_hook(get_activation(\"input_layernorm\", i))\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    # Also capture the final norm layer (after the last transformer layer)\n",
    "    final_hook = model.model.norm.register_forward_hook(\n",
    "        get_activation(\"final_norm\", len(model.model.layers))\n",
    "    )\n",
    "    hooks.append(final_hook)\n",
    "    \n",
    "    # Clone the tokenized_prompt to avoid modifying the original\n",
    "    generation_inputs = {\n",
    "        \"input_ids\": tokenized_prompt[\"input_ids\"].clone(),\n",
    "        \"attention_mask\": tokenized_prompt[\"attention_mask\"].clone(),\n",
    "    }\n",
    "    \n",
    "    # Generate text using the model's built-in generate method to handle stopping properly\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Use the model's generate method, which handles EOS tokens and stopping criteria\n",
    "            generation_output = model.generate(\n",
    "                **generation_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                output_hidden_states=False,  # We capture hidden states via hooks\n",
    "                # Set pad_token_id to EOS to avoid potential issues\n",
    "                pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 2\n",
    "            )\n",
    "            \n",
    "            # Get the generated sequence\n",
    "            generated_ids = generation_output.sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for layer_idx in all_residual_streams:\n",
    "        # Concatenate all the tensors for this layer along the sequence dimension\n",
    "        all_residual_streams[layer_idx] = torch.cat(all_residual_streams[layer_idx], dim=1)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    try:\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        # Print a preview of the generated text\n",
    "        preview = generated_text.replace('\\n', '\\\\n')\n",
    "        if len(preview) > 100:\n",
    "            preview = preview[:97] + '...'\n",
    "        input_tokens = input_length\n",
    "        output_tokens = generated_ids.shape[1]\n",
    "        new_tokens = output_tokens - input_tokens\n",
    "        print(f\"Generated text: {preview}\")\n",
    "        print(f\"Input tokens: {input_tokens}, New tokens: {new_tokens}, Total tokens: {output_tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding generated tokens: {e}\")\n",
    "        generated_text = f\"[Error decoding tokens: {generated_ids[0]}]\"\n",
    "    \n",
    "    # Store token ids for reference\n",
    "    token_ids = generated_ids.detach().cpu()\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_path is not None:\n",
    "        save_data = {\n",
    "            'residual_streams': all_residual_streams,\n",
    "            'generated_text': generated_text,\n",
    "            'token_ids': token_ids\n",
    "        }\n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            import os\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(save_data, save_path)\n",
    "            print(f\"Residual stream data saved to {save_path} \\n-----\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving residual stream data: {e}\")\n",
    "    \n",
    "    return all_residual_streams, generated_text, token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_residual_stream(load_path):\n",
    "    \"\"\"\n",
    "    Load previously saved residual stream data from a file.\n",
    "    \n",
    "    Args:\n",
    "        load_path: Path to the saved residual stream file (.pt)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_residual_streams, generated_text, token_ids)\n",
    "    \"\"\"\n",
    "    data = torch.load(load_path)\n",
    "    return data['residual_streams'], data['generated_text'], data['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_res_stream(model, \n",
    "                    tokenizer, \n",
    "                    clean_path, \n",
    "                    corrupt_path, \n",
    "                    patch_layers, \n",
    "                    num_tokens_to_patch=None, \n",
    "                    target_token=\"<think>\", \n",
    "                    save_path=None, \n",
    "                    device=None, \n",
    "                    max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Patch residual streams from corrupted prompt into the clean prompt at specified layers and tokens,\n",
    "    supporting autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model (LlamaForCausalLM)\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        clean_path: Path to the saved clean prompt residual stream file (.pt)\n",
    "        corrupt_path: Path to the saved corrupted prompt residual stream file (.pt)\n",
    "        patch_layers: List of layer indices to patch (e.g., [5, 6, 7] or range(32))\n",
    "        num_tokens_to_patch: Number of tokens to patch after the target token.\n",
    "                           If None, will patch from the target token to the end.\n",
    "        target_token: Token after which to start patching (default: \"<think>\")\n",
    "        save_path: Optional path to save the patched results (.pt)\n",
    "        device: Device to perform computation on\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (patched_text, original_clean_text, original_corrupt_text, patched_token_ids)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    # Set device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the saved residual streams\n",
    "    print(f\"Loading residual streams from {clean_path} and {corrupt_path}...\")\n",
    "    clean_streams, clean_text, clean_ids = load_residual_stream(clean_path)\n",
    "    corrupt_streams, corrupt_text, corrupt_ids = load_residual_stream(corrupt_path)\n",
    "    \n",
    "    print(f\"Clean text preview: {clean_text[:100]}...\")\n",
    "    print(f\"Corrupt text preview: {corrupt_text[:100]}...\")\n",
    "    \n",
    "    # Find the target token index in the clean prompt\n",
    "    if target_token:\n",
    "        # Check if target token is a string or a token ID\n",
    "        if isinstance(target_token, str):\n",
    "            # Find the token ID for the target token\n",
    "            target_token_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "            if len(target_token_ids) != 1:\n",
    "                print(f\"Warning: Target token '{target_token}' encoded to {len(target_token_ids)} tokens: {target_token_ids}\")\n",
    "            target_token_id = target_token_ids[0]\n",
    "        else:\n",
    "            target_token_id = target_token\n",
    "            \n",
    "        # Find the position of the target token in the clean_ids\n",
    "        target_positions = (clean_ids[0] == target_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(target_positions) == 0:\n",
    "            print(f\"Target token '{target_token}' not found in clean_ids. Will patch from the beginning.\")\n",
    "            patch_start_idx = 0\n",
    "        else:\n",
    "            # Take the first occurrence of the target token\n",
    "            patch_start_idx = target_positions[0].item()\n",
    "            print(f\"Found target token at position {patch_start_idx}\")\n",
    "    else:\n",
    "        # If no target token specified, start patching from the beginning\n",
    "        patch_start_idx = 0\n",
    "    \n",
    "    # Set the patch range\n",
    "    patch_start_idx = patch_start_idx if target_token else 0\n",
    "    \n",
    "    if num_tokens_to_patch is None:\n",
    "        # From target token to the end\n",
    "        patch_end_idx = min(clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    else:\n",
    "        # Patch specified number of tokens after the target token\n",
    "        patch_end_idx = min(patch_start_idx + num_tokens_to_patch, clean_ids.shape[1], corrupt_ids.shape[1])\n",
    "    \n",
    "    patch_tokens_range = (patch_start_idx, patch_end_idx)\n",
    "    \n",
    "    print(f\"Will patch tokens from position {patch_start_idx} to {patch_end_idx} ({patch_end_idx - patch_start_idx} tokens)\")\n",
    "    print(f\"Will patch layers: {patch_layers}\")\n",
    "    \n",
    "    # Validate layers\n",
    "    max_layer = max(clean_streams.keys())\n",
    "    patch_layers = [layer for layer in patch_layers if layer in clean_streams and layer in corrupt_streams]\n",
    "    if not patch_layers:\n",
    "        raise ValueError(f\"No valid layers to patch. Layers must be in range 0-{max_layer}\")\n",
    "    \n",
    "    # Create patched inputs based on clean prompt\n",
    "    input_ids = clean_ids[0, :patch_start_idx].unsqueeze(0).to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    \n",
    "    patched_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "    \n",
    "    # Set up patching state for autoregressive generation\n",
    "    current_token_pos = patch_start_idx - 1  # Position of the last token in inputs (-1 because we increment before patching)\n",
    "    max_corrupted_len = corrupt_ids.shape[1]\n",
    "    \n",
    "    # Set up patching progress tracking\n",
    "    from tqdm.auto import tqdm\n",
    "    expected_tokens_to_patch = min(patch_end_idx - patch_start_idx, max_new_tokens)\n",
    "    progress_bar = tqdm(total=expected_tokens_to_patch, desc=\"Patching tokens\", ncols=100)\n",
    "    \n",
    "    # Create a hook to patch each token during autoregressive generation\n",
    "    hooks = []\n",
    "    \n",
    "    def patch_hook(layer_idx):\n",
    "        patched_tokens_counter = 0  # Initialize counter inside hook closure\n",
    "        \n",
    "        def hook(module, inputs):\n",
    "            nonlocal current_token_pos, patched_tokens_counter\n",
    "            \n",
    "            # Get residual stream\n",
    "            res_stream = inputs[0]\n",
    "            batch_size, seq_len, hidden_dim = res_stream.shape\n",
    "            \n",
    "            # During autoregressive generation, we'll get one token at a time\n",
    "            # For the initial pass with multiple tokens, patch only the target position and beyond\n",
    "            if seq_len > 1:\n",
    "                # This is the initial forward pass with all input tokens\n",
    "                # We don't patch anything in the input prefix (before target token)\n",
    "                return inputs\n",
    "            else:\n",
    "                # This is autoregressive generation with one token at a time\n",
    "                # Increment position counter - this tells us which token we're generating\n",
    "                current_token_pos += 1\n",
    "                \n",
    "                # Check if this position should be patched\n",
    "                if patch_start_idx <= current_token_pos < patch_end_idx and current_token_pos < max_corrupted_len:\n",
    "                    # Get the corresponding position in the corrupted stream\n",
    "                    to_patch = corrupt_streams[layer_idx][0, current_token_pos, :].to(device)\n",
    "                    \n",
    "                    # Replace the residual stream for this token with the corrupted version\n",
    "                    res_stream[0, 0, :] = to_patch\n",
    "                    \n",
    "                    # Update progress tracking\n",
    "                    patched_tokens_counter += 1\n",
    "                    progress_bar.update(1)\n",
    "                    \n",
    "                    if current_token_pos % 10 == 0:  # Less frequent logging\n",
    "                        print(f\"Patched token at position {current_token_pos} in layer {layer_idx}\")\n",
    "            \n",
    "            # Return potentially modified inputs\n",
    "            return (res_stream,) + inputs[1:] if len(inputs) > 1 else (res_stream,)\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    # Register forward pre-hooks for each layer to patch\n",
    "    for layer_idx in patch_layers:\n",
    "        hook = model.model.layers[layer_idx].input_layernorm.register_forward_pre_hook(\n",
    "            patch_hook(layer_idx)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Generate text with patched residual streams\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Generate text with the patched residual stream\n",
    "            generation_output = model.generate(\n",
    "                **patched_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            patched_ids = generation_output.sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation with patched residual stream: {e}\")\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        raise\n",
    "    finally:\n",
    "        # Remove all hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Close progress bar\n",
    "        progress_bar.close()\n",
    "    \n",
    "    # Decode the patched generation\n",
    "    patched_text = tokenizer.decode(patched_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- Patching Results ---\")\n",
    "    print(f\"Patched layers: {patch_layers}\")\n",
    "    print(f\"Patched token range: {patch_tokens_range}\")\n",
    "    print(f\"Generation length: {patched_ids.shape[1] - input_ids.shape[1]} new tokens\")\n",
    "    print(\"\\nOriginal clean output: \")\n",
    "    print(clean_text[:200] + \"...\" if len(clean_text) > 200 else clean_text)\n",
    "    print(\"\\nOriginal corrupt output: \")\n",
    "    print(corrupt_text[:200] + \"...\" if len(corrupt_text) > 200 else corrupt_text)\n",
    "    print(\"\\nPatched output: \")\n",
    "    print(patched_text[:200] + \"...\" if len(patched_text) > 200 else patched_text)\n",
    "    \n",
    "    # Save patched results if requested\n",
    "    if save_path:\n",
    "        patched_data = {\n",
    "            'patched_text': patched_text,\n",
    "            'clean_text': clean_text,\n",
    "            'corrupt_text': corrupt_text,\n",
    "            'patched_token_ids': patched_ids.cpu(),\n",
    "            'patch_info': {\n",
    "                'patch_layers': patch_layers,\n",
    "                'patch_tokens_range': patch_tokens_range,\n",
    "                'clean_path': clean_path,\n",
    "                'corrupt_path': corrupt_path\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Add file extension if not provided\n",
    "            if not save_path.endswith('.pt'):\n",
    "                save_path = f\"{save_path}.pt\"\n",
    "            \n",
    "            # Make sure directory exists\n",
    "            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
    "            \n",
    "            torch.save(patched_data, save_path)\n",
    "            print(f\"Patched results saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving patched results: {e}\")\n",
    "    \n",
    "    return patched_text, clean_text, corrupt_text, patched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_prompt = \"David found four sticks of different lengths that can be used to form three non-congruent convex cyclic quadrilaterals, $A,\\text{ }B,\\text{ }C$ , which can each be inscribed in a circle with radius $1$ . Let $\\varphi_A$ denote the measure of the acute angle made by the diagonals of quadrilateral $A$ , and define $\\varphi_B$ and $\\varphi_C$ similarly. Suppose that $\\sin\\varphi_A=\\frac{2}{3}$ , $\\sin\\varphi_B=\\frac{3}{5}$ , and $\\sin\\varphi_C=\\frac{6}{7}$ . All three quadrilaterals have the same area $K$ , which can be written in the form $\\dfrac{m}{n}$ , where $m$ and $n$ are relatively prime positive integers. Find $m+n$ .\"\n",
    "clean_tokenized_prompt, clean_formatted_prompt = tokenize_prompt(clean_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "clean_residual_streams, clean_generated_text, clean_token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    clean_tokenized_prompt, \n",
    "                                                                    max_new_tokens=32768, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/clean-prompt-2.pt\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(clean_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_prompt = \"For integers $a,b,c$ and $d,$ let $f(x)=x^2+ax+b$ and $g(x)=x^2+cx+d.$ Find the number of ordered triples $(a,b,c)$ of integers with absolute values not exceeding $10$ for which there is an integer $d$ such that $g(f(2))=g(f(4))=0.$ . Hint: The key insight is to recognize what it means for $g(f(2)) = g(f(4)) = 0$. This tells us that both $f(2)$ and $f(4)$ are roots of the polynomial $g(x)$. There are two distinct cases to consider: 1. What happens if $f(2) = f(4)$? When would this occur? What constraint does this place on $a$ 2. What happens if $f(2) \\neq f(4)$? In this case, since $g(x)$ is a quadratic polynomial and we know both $f(2)$ and $f(4)$ are roots, we can write: $g(x) = (x - f(2))(x - f(4))$. When you expand this, you'll get expressions for $c$ and $d$ in terms of $a$ and $b$. Focus on the constraint for $c$ (since $d$ can be any integer), and determine which values of $(a,b)$ will give you values of $c$ with $|c| \\leq 10$. Try separating the cases and counting how many valid triples $(a,b,c)$ exist in each scenario.\"\n",
    "corrupted_tokenized_prompt, corrupted_formatted_prompt = tokenize_prompt(corrupted_prompt, tokenizer, device=DEVICE)\n",
    "\n",
    "\n",
    "corrupted_residual_streams, corrupted_generated_text, corrupted_token_ids = get_residual_stream(model, \n",
    "                                                                    tokenizer, \n",
    "                                                                    corrupted_tokenized_prompt, \n",
    "                                                                    max_new_tokens=16384, \n",
    "                                                                    layer_indices=None, \n",
    "                                                                    save_path=\"res-stream/corrupted-prompt-1.pt\")\n",
    "\n",
    "print(corrupted_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_text, clean_text, corrupt_text, patched_ids = patch_res_stream(model, \n",
    "                                                                        tokenizer, \n",
    "                                                                        clean_path = \"res-stream/clean-prompt-1.pt\", \n",
    "                                                                        corrupt_path = \"res-stream/corrupted-prompt-1.pt\", \n",
    "                                                                        patch_layers = [0], \n",
    "                                                                        num_tokens_to_patch=64, \n",
    "                                                                        target_token=\"<think>\", \n",
    "                                                                        save_path=\"res-stream/patched-prompt-1.pt\", \n",
    "                                                                        device=None, \n",
    "                                                                        max_new_tokens=1024)\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(patched_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"corrupted text:\")\n",
    "print(corrupt_text)\n",
    "print(\"\\n---\")\n",
    "print(\"clean text:\")\n",
    "print(clean_text)\n",
    "print(\"\\n---\")\n",
    "print(\"patched text:\")\n",
    "print(patched_text)\n",
    "print(\"\\n---\")\n",
    "# print(patched_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_streams, generated_text, token_ids = load_residual_stream(\"res-stream/corrupted-prompt-1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
